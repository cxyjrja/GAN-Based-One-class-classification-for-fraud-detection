{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN\n",
    "\n",
    "The real samples has three types of features: \n",
    "- non sequential features which has a shape of (14, )\n",
    "- sequential features which has a shape of (60,17)\n",
    "- image features(MTF along the features) which has a shape of (31,31,1). \n",
    "\n",
    "There are three corresponding input layers in the discriminators which were then followed by Dense layers, LSTM layers or CNN layers and merged together. After that there are several dense layers for classification. \n",
    "\n",
    "\n",
    "Correspondingly, we have three generators to generate fake samples from noise. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from tensorflow.keras.optimizers import Adam,RMSprop,SGD\n",
    "from keras.layers import Input, Embedding, multiply, BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Reshape, Dense, Dropout, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Dense, Dropout, Flatten,concatenate,LSTM,Input,Bidirectional\n",
    "from keras.layers import Conv2D, MaxPooling2D,AveragePooling2D,Input, Dense, Reshape, Flatten, Embedding, Dropout,Conv2DTranspose\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras import metrics\n",
    "\n",
    "\n",
    "with open('label.json') as f:\n",
    "    labels=json.load(f)\n",
    "with open('non_sequential_features.json') as f:\n",
    "    non_sequential_features=json.load(f)\n",
    "with open('padded_sequential_features_3.json') as f:\n",
    "    sequential_features=json.load(f)\n",
    "with open('featurematrix.json') as f:\n",
    "    arr_=json.load(f)\n",
    "arr_=np.array(arr_)\n",
    "\n",
    "# from dict to numpy array\n",
    "feature1=np.array([sequential_features[key] for key in sequential_features.keys()])\n",
    "feature2=np.array([non_sequential_features[key] for key in non_sequential_features.keys()])\n",
    "label=np.array([labels[key] for key in labels.keys()])\n",
    "\n",
    "#reshape features\n",
    "arr_=arr_.reshape(-1,31,31,1)\n",
    "feature1=feature1[:,:,1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- We will only use benign samples to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training set include only benign users\n",
    "arr_benign=arr_[label==0]\n",
    "arr_fraud=arr_[label==1]\n",
    "\n",
    "feature1_benign=feature1[label==0]\n",
    "feature1_fraud=feature1[label==1]\n",
    "\n",
    "feature2_benign=feature2[label==0]\n",
    "feature2_fraud=feature2[label==1]\n",
    "\n",
    "label_benign=label[label==0]  \n",
    "label_fraud=label[label==1]  \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_arr, X_test_arr,X_train_f1, X_test_f1,X_train_f2, X_test_f2,y_train,y_test= train_test_split(arr_benign,\n",
    "                                                                                                     feature1_benign,\n",
    "                                                                                                     feature2_benign,\n",
    "                                                                                                     label_benign,\n",
    "                                                                                          test_size=0.40, random_state=42)\n",
    "# testing set includes both benign and fraud \n",
    "X_test_arr=np.concatenate([X_test_arr, arr_fraud])\n",
    "X_test_f1=np.concatenate([X_test_f1, feature1_fraud])\n",
    "X_test_f2=np.concatenate([X_test_f2, feature2_fraud])\n",
    "y_test=np.concatenate([y_test, label_fraud])\n",
    "\n",
    "\n",
    "#shuffle testing set\n",
    "randomize = np.arange(len(X_test_arr))\n",
    "np.random.shuffle(randomize)\n",
    "X_test_arr = X_test_arr[randomize]\n",
    "X_test_f1 = X_test_f1[randomize]\n",
    "X_test_f2 = X_test_f2[randomize]\n",
    "y_test = y_test[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((83878, 31, 31, 1), (83878, 60, 17), (83878, 14), (83878,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_arr.shape,X_train_f1.shape,X_train_f2.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set for model selection, testing set for reporting \n",
    "X_valid_arr,X_test_arr,X_valid_f1,X_test_f1,X_valid_f2,X_test_f2,y_valid,y_test=train_test_split(X_test_arr,\n",
    "                                                                                                 X_test_f1,\n",
    "                                                                                                 X_test_f2,\n",
    "                                                                                                 y_test,\n",
    "                                                                                                 test_size=0.50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33022, 31, 31, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build multi-inputs GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.feature_matrix_shape=(31,31,1)\n",
    "        self.mtf_shape=(60,60,1)\n",
    "        self.lstm_features_cnt=17\n",
    "        self.non_seq_shape=(14,)\n",
    "        optimizer_SGD= SGD()\n",
    "        optimizer_adam= Adam()\n",
    "\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',optimizer=optimizer_SGD, metrics=['accuracy'])\n",
    "        \n",
    "        # Build the generator\n",
    "        self.generator1 = self.build_generator1()\n",
    "        noise_non_sequential=Input(shape=(14,),name=\"dense_input\")\n",
    "        non_sequential_fake=self.generator1(noise_non_sequential)\n",
    "        \n",
    "        self.generator2 = self.build_generator2()\n",
    "        noise_fm=Input(shape=(32,32,1),name=\"cnn_input\")\n",
    "        fm_fake=self.generator2(noise_fm)\n",
    "        \n",
    "        self.generator3 = self.build_generator3()\n",
    "        noise_rnn=Input(shape=(60,17),name=\"rnn_input\")\n",
    "        rnn_fake=self.generator3(noise_rnn)\n",
    "\n",
    "        # combined generator and discriminator for generator training\n",
    "        self.discriminator.trainable = False  #discriminator is not trainable when training generator\n",
    "        validity = self.discriminator([non_sequential_fake,fm_fake,rnn_fake])\n",
    "        self.combined = Model([noise_non_sequential,noise_fm,noise_rnn], validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer_adam)\n",
    "        \n",
    "            \n",
    "\n",
    "    def build_discriminator(self):\n",
    "        ## input from feature matrix\n",
    "        input_fm=Input(shape=self.feature_matrix_shape,name=\"cnn_input\")\n",
    "        fm=BatchNormalization()(input_fm)\n",
    "        cnn1=Conv2D(64, (3, 3), padding=\"same\")(fm)\n",
    "        cnn1=LeakyReLU(alpha=0.2)(cnn1)\n",
    "        bn1=BatchNormalization()(cnn1)\n",
    "        bn1=Dropout(0.3)(bn1)\n",
    "        pool1=AveragePooling2D(pool_size=(2,2),strides=2)(bn1)\n",
    "        cnn2=Conv2D(32, (3, 3), padding=\"same\")(pool1)\n",
    "        cnn2=LeakyReLU(alpha=0.2)(cnn2)\n",
    "        bn2=BatchNormalization()(cnn2)\n",
    "        pool2=AveragePooling2D(pool_size=(2,2),strides=2)(bn2)\n",
    "        pool2=Dropout(0.3)(pool2)\n",
    "        cnn3=Conv2D(1, (3, 3), padding=\"same\")(pool2)\n",
    "        cnn3=LeakyReLU(alpha=0.2)(cnn3)\n",
    "        fm_output=Flatten()(cnn3)\n",
    "\n",
    "\n",
    "        #input from sequential features\n",
    "        input_rnn=Input(shape=(60,17),name=\"rnn_input\")\n",
    "        lstm1=Bidirectional(keras.layers.LSTM(64, activation='tanh', return_sequences=True))(input_rnn)\n",
    "        #lstm1=SeqSelfAttention(attention_activation='sigmoid')(lstm1)\n",
    "        lstm1=Dropout(0.3)(lstm1)\n",
    "        rnn_output=Bidirectional(keras.layers.LSTM(32, activation='tanh', return_sequences=False))(lstm1)\n",
    "        \n",
    "\n",
    "        #input from non-sequential features\n",
    "        input_non_sequential=Input(shape=self.non_seq_shape,name=\"dense_input\")\n",
    "        dense1=Dense(32)(input_non_sequential)\n",
    "        dense1=LeakyReLU(alpha=0.2)(dense1)\n",
    "        bn3=BatchNormalization()(dense1)\n",
    "        drop2=Dropout(0.3)(bn3)\n",
    "        dense2_output=Dense(32)(drop2)\n",
    "        dense2_output=LeakyReLU(alpha=0.2)(dense2_output)\n",
    "\n",
    "        \n",
    "        #combine three types of input\n",
    "        merged = concatenate([dense2_output,fm_output, rnn_output])\n",
    "        dense3=Dense(128)(merged)\n",
    "        dense3=LeakyReLU(alpha=0.2)(dense3)\n",
    "        bn4=BatchNormalization()(dense3)\n",
    "        bn4=Dropout(0.3)(bn4)\n",
    "        dense4=Dense(64)(bn4)\n",
    "        dense4=LeakyReLU(alpha=0.2)(dense4)\n",
    "        bn5=BatchNormalization()(dense4)\n",
    "        out = Dense(1, activation='sigmoid', name='output_layer')(bn5)\n",
    "\n",
    "        model = Model(inputs=[input_non_sequential,input_fm,input_rnn], outputs=[out])\n",
    "        #model.summary()\n",
    "        return model\n",
    "    \n",
    "    def build_generator1(self):\n",
    "        # generator to generate fake data like non-sequential features \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(32))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(64))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(14,activation='tanh'))\n",
    "\n",
    "        #model.summary()\n",
    "\n",
    "        noise = Input(shape=(14,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "    \n",
    "    def build_generator2(self):\n",
    "         # generator to generate fake data like feature matrix\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2DTranspose(8,strides=(2, 2), kernel_size=(3,3)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(keras.layers.BatchNormalization(momentum=0.8))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(filters=4,kernel_size=4,strides=2,padding='valid'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(keras.layers.BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(filters=1,kernel_size=4,strides=1,padding='same',activation='tanh'))\n",
    "        noise = Input(shape=(32,32,1))\n",
    "        img = model(noise)\n",
    "        #model.summary()\n",
    "        return Model(noise, img)\n",
    "    \n",
    "    def build_generator3(self):\n",
    "        # generator to generate fake data like sequential features\n",
    "        model = Sequential()\n",
    "        model.add(keras.layers.LSTM(32, activation=\"tanh\", return_sequences=True))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(keras.layers.LSTM(64, activation=\"tanh\", return_sequences=True))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(keras.layers.LSTM(128, activation=\"tanh\", return_sequences=True))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(keras.layers.LSTM(17, activation=\"tanh\",return_sequences=True))\n",
    "        noise = Input(shape=(60,17))\n",
    "        img = model(noise)\n",
    "       # model.summary()\n",
    "        return Model(noise, img)\n",
    "    \n",
    "\n",
    "    def train(self,X_train_arr,X_train_f1,X_train_f2,y_train,X_test_arr,X_test_f1,X_test_f2,y_test,\n",
    "              epochs=200, batch_size=128):\n",
    "        auc_list=[]\n",
    "        ks_list=[]\n",
    "        auc_progress = []\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        noise_until = epochs\n",
    "\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Select a random half batch of real benign users data\n",
    "            idx = np.random.randint(0, y_train.shape[0], half_batch)\n",
    "            arr=X_train_arr[idx]\n",
    "            sequential=X_train_f1[idx]\n",
    "            non_sequential=X_train_f2[idx]\n",
    "\n",
    "           # Sample noise and generate a half batch of new fake data\n",
    "            noise1 = np.random.normal(0, 1, (half_batch, 14))\n",
    "            noise2 = np.random.normal(0, 1, (half_batch, 32,32,1))\n",
    "            noise3 = np.random.normal(0, 1, (half_batch, 60,17))\n",
    "                \n",
    "            non_sequential_fake=self.generator1.predict(noise1)\n",
    "            fm_fake=self.generator2.predict(noise2)\n",
    "            rnn_fake=self.generator3.predict(noise3)\n",
    "\n",
    "            valid = np.ones((half_batch, 1))\n",
    "            fake = np.zeros((half_batch, 1))\n",
    "\n",
    "            # Train the discriminator\n",
    "            \n",
    "            d_loss_real = self.discriminator.train_on_batch([non_sequential,arr,sequential], valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch([non_sequential_fake,fm_fake,rnn_fake], fake)\n",
    "\n",
    " \n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "\n",
    "            #  Train Generator\n",
    "  \n",
    "            noise1 = np.random.normal(0, 1, (batch_size, 14))\n",
    "            noise2 = np.random.normal(0, 1, (batch_size, 32,32,1))\n",
    "            noise3 = np.random.normal(0, 1, (batch_size, 60,17))\n",
    "                \n",
    "            validity = np.ones((batch_size, 1))\n",
    "\n",
    "           # if epoch<=1000 or epoch%3==0:\n",
    "            g_loss = self.combined.train_on_batch([noise1,noise2,noise3], validity)\n",
    "           \n",
    "            #g_loss = self.combined.test_on_batch([noise1,noise2,noise3], validity)\n",
    "\n",
    "            if epoch % 5 == 0:\n",
    "                print (\"%d [D loss: %f, acc: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            if epoch % 50 == 0:\n",
    "                y_pred = self.discriminator.predict([X_test_f2,X_valid_arr,X_test_f1],batch_size=batch_size)\n",
    "                (fpr, tpr, thresholds) = roc_curve(y_test,y_pred)\n",
    "                area = auc(fpr,tpr)\n",
    "                auc_list.append(area)\n",
    "\n",
    "                ks=(tpr-fpr)\n",
    "                max_ks=np.max(ks)\n",
    "                ks_list.append(max_ks)\n",
    "                print('Epoch: {}, auc: {:.5f}, ks: {}'.format(epoch,area,max_ks))\n",
    "                self.discriminator.save(\"gans_model_saved/\"+str(epoch)+\"_model_gans.h5\")\n",
    "\n",
    "\n",
    "        return auc_list,ks_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    auc_list,ks_list=gan.train(X_train_arr,X_train_f1,X_train_f2,y_train,X_valid_arr,X_valid_f1,X_valid_f2,y_valid,\n",
    "             epochs=10000, \n",
    "             batch_size=512)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
