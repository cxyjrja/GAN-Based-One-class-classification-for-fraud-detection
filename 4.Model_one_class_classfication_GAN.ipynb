{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN\n",
    "\n",
    "The real samples has three types of features: \n",
    "- non sequential features which has a shape of (14, )\n",
    "- sequential features which has a shape of (60,17)\n",
    "- image features(MTF along the features) which has a shape of (31,31,1). \n",
    "\n",
    "There are three corresponding input layers in the discriminators which were then followed by Dense layers, LSTM layers or CNN layers and merged together. After that there are several dense layers for classification. \n",
    "\n",
    "\n",
    "Correspondingly, we have three generators to generate fake samples from noise. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from keras import layers\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "from tensorflow.keras.optimizers import Adam,RMSprop,SGD\n",
    "from keras.layers import Input, Embedding, multiply, BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Reshape, Dense, Dropout, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers import Dense, Dropout, Flatten,concatenate,LSTM,Input,Bidirectional\n",
    "from keras.layers import Conv2D, MaxPooling2D,AveragePooling2D,Input, Dense, Reshape, Flatten, Embedding, Dropout,Conv2DTranspose\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras import metrics\n",
    "\n",
    "\n",
    "with open('/home/yz3698/label.json') as f:\n",
    "    labels=json.load(f)\n",
    "with open('/home/yz3698/non_sequential_features.json') as f:\n",
    "    non_sequential_features=json.load(f)\n",
    "with open('/home/yz3698/padded_sequential_features_3.json') as f:\n",
    "    sequential_features=json.load(f)\n",
    "with open('featurematrix.json') as f:\n",
    "    arr_=json.load(f)\n",
    "arr_=np.array(arr_)\n",
    "\n",
    "# from dict to numpy array\n",
    "feature1=np.array([sequential_features[key] for key in sequential_features.keys()])\n",
    "feature2=np.array([non_sequential_features[key] for key in non_sequential_features.keys()])\n",
    "label=np.array([labels[key] for key in labels.keys()])\n",
    "\n",
    "#reshape features\n",
    "arr_=arr_.reshape(-1,31,31,1)\n",
    "feature1=feature1[:,:,1:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "- We separate data into training set, validation set, and testing set.\n",
    "- We will only use benign samples to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training set include only benign users\n",
    "arr_benign=arr_[label==0]\n",
    "arr_fraud=arr_[label==1]\n",
    "\n",
    "feature1_benign=feature1[label==0]\n",
    "feature1_fraud=feature1[label==1]\n",
    "\n",
    "feature2_benign=feature2[label==0]\n",
    "feature2_fraud=feature2[label==1]\n",
    "\n",
    "label_benign=label[label==0]  \n",
    "label_fraud=label[label==1]  \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_arr, X_test_arr,X_train_f1, X_test_f1,X_train_f2, X_test_f2,y_train,y_test= train_test_split(arr_benign,\n",
    "                                                                                                     feature1_benign,\n",
    "                                                                                                     feature2_benign,\n",
    "                                                                                                     label_benign,\n",
    "                                                                                          test_size=0.40, random_state=42)\n",
    "# testing set includes both benign and fraud \n",
    "X_test_arr=np.concatenate([X_test_arr, arr_fraud])\n",
    "X_test_f1=np.concatenate([X_test_f1, feature1_fraud])\n",
    "X_test_f2=np.concatenate([X_test_f2, feature2_fraud])\n",
    "y_test=np.concatenate([y_test, label_fraud])\n",
    "\n",
    "\n",
    "#shuffle testing set\n",
    "randomize = np.arange(len(X_test_arr))\n",
    "np.random.shuffle(randomize)\n",
    "X_test_arr = X_test_arr[randomize]\n",
    "X_test_f1 = X_test_f1[randomize]\n",
    "X_test_f2 = X_test_f2[randomize]\n",
    "y_test = y_test[randomize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((83878, 31, 31, 1), (83878, 60, 17), (83878, 14), (83878,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_arr.shape,X_train_f1.shape,X_train_f2.shape,y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation set for model selection, testing set for reporting \n",
    "X_valid_arr,X_test_arr,X_valid_f1,X_test_f1,X_valid_f2,X_test_f2,y_valid,y_test=train_test_split(X_test_arr,\n",
    "                                                                                                 X_test_f1,\n",
    "                                                                                                 X_test_f2,\n",
    "                                                                                                 y_test,\n",
    "                                                                                                 test_size=0.50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33022, 31, 31, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build multi-source GANs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN():\n",
    "    def __init__(self):\n",
    "        self.feature_matrix_shape=(31,31,1)\n",
    "        self.mtf_shape=(60,60,1)\n",
    "        self.lstm_features_cnt=17\n",
    "        self.non_seq_shape=(14,)\n",
    "        optimizer_SGD= SGD()\n",
    "        optimizer_adam= Adam()\n",
    "\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',optimizer=optimizer_SGD, metrics=['accuracy'])\n",
    "        \n",
    "        # Build the generator\n",
    "        self.generator1 = self.build_generator1()\n",
    "        noise_non_sequential=Input(shape=(14,),name=\"dense_input\")\n",
    "        non_sequential_fake=self.generator1(noise_non_sequential)\n",
    "        \n",
    "        self.generator2 = self.build_generator2()\n",
    "        noise_fm=Input(shape=(32,32,1),name=\"cnn_input\")\n",
    "        fm_fake=self.generator2(noise_fm)\n",
    "        \n",
    "        self.generator3 = self.build_generator3()\n",
    "        noise_rnn=Input(shape=(60,17),name=\"rnn_input\")\n",
    "        rnn_fake=self.generator3(noise_rnn)\n",
    "\n",
    "        # combined generator and discriminator for generator training\n",
    "        self.discriminator.trainable = False  #discriminator is not trainable when training generator\n",
    "        validity = self.discriminator([non_sequential_fake,fm_fake,rnn_fake])\n",
    "        self.combined = Model([noise_non_sequential,noise_fm,noise_rnn], validity)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer_adam)\n",
    "        \n",
    "            \n",
    "\n",
    "    def build_discriminator(self):\n",
    "        ## input from feature matrix\n",
    "        input_fm=Input(shape=self.feature_matrix_shape,name=\"cnn_input\")\n",
    "        fm=BatchNormalization()(input_fm)\n",
    "        cnn1=Conv2D(64, (3, 3), padding=\"same\")(fm)\n",
    "        cnn1=LeakyReLU(alpha=0.2)(cnn1)\n",
    "        bn1=BatchNormalization()(cnn1)\n",
    "        bn1=Dropout(0.3)(bn1)\n",
    "        pool1=AveragePooling2D(pool_size=(2,2),strides=2)(bn1)\n",
    "        cnn2=Conv2D(32, (3, 3), padding=\"same\")(pool1)\n",
    "        cnn2=LeakyReLU(alpha=0.2)(cnn2)\n",
    "        bn2=BatchNormalization()(cnn2)\n",
    "        pool2=AveragePooling2D(pool_size=(2,2),strides=2)(bn2)\n",
    "        pool2=Dropout(0.3)(pool2)\n",
    "        cnn3=Conv2D(1, (3, 3), padding=\"same\")(pool2)\n",
    "        cnn3=LeakyReLU(alpha=0.2)(cnn3)\n",
    "        fm_output=Flatten()(cnn3)\n",
    "\n",
    "\n",
    "        #input from sequential features\n",
    "        input_rnn=Input(shape=(60,17),name=\"rnn_input\")\n",
    "        lstm1=Bidirectional(keras.layers.LSTM(64, activation='tanh', return_sequences=True))(input_rnn)\n",
    "        #lstm1=SeqSelfAttention(attention_activation='sigmoid')(lstm1)\n",
    "        lstm1=Dropout(0.3)(lstm1)\n",
    "        rnn_output=Bidirectional(keras.layers.LSTM(32, activation='tanh', return_sequences=False))(lstm1)\n",
    "        \n",
    "\n",
    "        #input from non-sequential features\n",
    "        input_non_sequential=Input(shape=self.non_seq_shape,name=\"dense_input\")\n",
    "        dense1=Dense(32)(input_non_sequential)\n",
    "        dense1=LeakyReLU(alpha=0.2)(dense1)\n",
    "        bn3=BatchNormalization()(dense1)\n",
    "        drop2=Dropout(0.3)(bn3)\n",
    "        dense2_output=Dense(32)(drop2)\n",
    "        dense2_output=LeakyReLU(alpha=0.2)(dense2_output)\n",
    "\n",
    "        \n",
    "        #combine three types of input\n",
    "        merged = concatenate([dense2_output,fm_output, rnn_output])\n",
    "        dense3=Dense(128)(merged)\n",
    "        dense3=LeakyReLU(alpha=0.2)(dense3)\n",
    "        bn4=BatchNormalization()(dense3)\n",
    "        bn4=Dropout(0.3)(bn4)\n",
    "        dense4=Dense(64)(bn4)\n",
    "        dense4=LeakyReLU(alpha=0.2)(dense4)\n",
    "        bn5=BatchNormalization()(dense4)\n",
    "        out = Dense(1, activation='sigmoid', name='output_layer')(bn5)\n",
    "\n",
    "        model = Model(inputs=[input_non_sequential,input_fm,input_rnn], outputs=[out])\n",
    "        #model.summary()\n",
    "        return model\n",
    "    \n",
    "    def build_generator1(self):\n",
    "        # generator to generate fake data like non-sequential features \n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(32))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(64))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(14,activation='tanh'))\n",
    "\n",
    "        #model.summary()\n",
    "\n",
    "        noise = Input(shape=(14,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "    \n",
    "    def build_generator2(self):\n",
    "         # generator to generate fake data like feature matrix\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2DTranspose(8,strides=(2, 2), kernel_size=(3,3)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(keras.layers.BatchNormalization(momentum=0.8))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(filters=4,kernel_size=4,strides=2,padding='valid'))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(keras.layers.BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(filters=1,kernel_size=4,strides=1,padding='same',activation='tanh'))\n",
    "        noise = Input(shape=(32,32,1))\n",
    "        img = model(noise)\n",
    "        #model.summary()\n",
    "        return Model(noise, img)\n",
    "    \n",
    "    def build_generator3(self):\n",
    "        # generator to generate fake data like sequential features\n",
    "        model = Sequential()\n",
    "        model.add(keras.layers.LSTM(32, activation=\"tanh\", return_sequences=True))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(keras.layers.LSTM(64, activation=\"tanh\", return_sequences=True))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(keras.layers.LSTM(128, activation=\"tanh\", return_sequences=True))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(keras.layers.LSTM(17, activation=\"tanh\",return_sequences=True))\n",
    "        noise = Input(shape=(60,17))\n",
    "        img = model(noise)\n",
    "       # model.summary()\n",
    "        return Model(noise, img)\n",
    "    \n",
    "\n",
    "    def train(self,X_train_arr,X_train_f1,X_train_f2,y_train,X_test_arr,X_test_f1,X_test_f2,y_test,\n",
    "              epochs=200, batch_size=128):\n",
    "        auc_list=[]\n",
    "        ks_list=[]\n",
    "        auc_progress = []\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        noise_until = epochs\n",
    "\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            # Select a random half batch of real benign users data\n",
    "            idx = np.random.randint(0, y_train.shape[0], half_batch)\n",
    "            arr=X_train_arr[idx]\n",
    "            sequential=X_train_f1[idx]\n",
    "            non_sequential=X_train_f2[idx]\n",
    "\n",
    "           # Sample noise and generate a half batch of new fake data\n",
    "            noise1 = np.random.normal(0, 1, (half_batch, 14))\n",
    "            noise2 = np.random.normal(0, 1, (half_batch, 32,32,1))\n",
    "            noise3 = np.random.normal(0, 1, (half_batch, 60,17))\n",
    "                \n",
    "            non_sequential_fake=self.generator1.predict(noise1)\n",
    "            fm_fake=self.generator2.predict(noise2)\n",
    "            rnn_fake=self.generator3.predict(noise3)\n",
    "\n",
    "            valid = np.ones((half_batch, 1))\n",
    "            fake = np.zeros((half_batch, 1))\n",
    "\n",
    "            # Train the discriminator\n",
    "            \n",
    "            d_loss_real = self.discriminator.train_on_batch([non_sequential,arr,sequential], valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch([non_sequential_fake,fm_fake,rnn_fake], fake)\n",
    "\n",
    " \n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "\n",
    "\n",
    "            #  Train Generator\n",
    "  \n",
    "            noise1 = np.random.normal(0, 1, (batch_size, 14))\n",
    "            noise2 = np.random.normal(0, 1, (batch_size, 32,32,1))\n",
    "            noise3 = np.random.normal(0, 1, (batch_size, 60,17))\n",
    "                \n",
    "            validity = np.ones((batch_size, 1))\n",
    "\n",
    "           # if epoch<=1000 or epoch%3==0:\n",
    "            g_loss = self.combined.train_on_batch([noise1,noise2,noise3], validity)\n",
    "           \n",
    "            #g_loss = self.combined.test_on_batch([noise1,noise2,noise3], validity)\n",
    "\n",
    "            # Plot the progress\n",
    "            if epoch % 5 == 0:\n",
    "                print (\"%d [D loss: %f, acc: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            if epoch % 50 == 0:\n",
    "                y_pred = self.discriminator.predict([X_test_f2,X_valid_arr,X_test_f1],batch_size=batch_size)\n",
    "                (fpr, tpr, thresholds) = roc_curve(y_test,y_pred)\n",
    "                area = auc(fpr,tpr)\n",
    "                auc_list.append(area)\n",
    "\n",
    "                ks=(tpr-fpr)\n",
    "                max_ks=np.max(ks)\n",
    "                ks_list.append(max_ks)\n",
    "                print('Epoch: {}, auc: {:.5f}, ks: {}'.format(epoch,area,max_ks))\n",
    "                self.discriminator.save(\"/home/yz3698/gans_model_saved/\"+str(epoch)+\"_model_gans.h5\")\n",
    "\n",
    "\n",
    "        return auc_list,ks_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yz3698/.local/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n",
      "/home/yz3698/.local/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.909577, acc: 48.83%] [G loss: 0.912915]\n",
      "Epoch: 0, auc: 0.48100, ks: 7.141581860381496e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yz3698/.local/lib/python3.7/site-packages/keras/engine/saving.py:165: UserWarning: TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n",
      "  'TensorFlow optimizers do not '\n",
      "/home/yz3698/.local/lib/python3.7/site-packages/keras/engine/training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 [D loss: 0.860117, acc: 48.83%] [G loss: 0.868136]\n",
      "10 [D loss: 0.848213, acc: 48.63%] [G loss: 0.838002]\n",
      "15 [D loss: 0.816462, acc: 50.20%] [G loss: 0.805718]\n",
      "20 [D loss: 0.809749, acc: 50.78%] [G loss: 0.807006]\n",
      "25 [D loss: 0.807147, acc: 49.22%] [G loss: 0.796257]\n",
      "30 [D loss: 0.793322, acc: 50.00%] [G loss: 0.797292]\n",
      "35 [D loss: 0.787203, acc: 50.20%] [G loss: 0.791473]\n",
      "40 [D loss: 0.780677, acc: 51.56%] [G loss: 0.771913]\n",
      "45 [D loss: 0.788931, acc: 51.37%] [G loss: 0.777997]\n",
      "50 [D loss: 0.776365, acc: 46.88%] [G loss: 0.780361]\n",
      "Epoch: 50, auc: 0.51845, ks: 0.031289509258949855\n",
      "55 [D loss: 0.775696, acc: 51.37%] [G loss: 0.768522]\n",
      "60 [D loss: 0.769747, acc: 48.83%] [G loss: 0.767423]\n",
      "65 [D loss: 0.762148, acc: 53.12%] [G loss: 0.766120]\n",
      "70 [D loss: 0.758430, acc: 51.95%] [G loss: 0.764964]\n",
      "75 [D loss: 0.755272, acc: 50.00%] [G loss: 0.758107]\n",
      "80 [D loss: 0.753541, acc: 46.48%] [G loss: 0.756498]\n",
      "85 [D loss: 0.753806, acc: 50.78%] [G loss: 0.752269]\n",
      "90 [D loss: 0.745001, acc: 51.37%] [G loss: 0.749874]\n",
      "95 [D loss: 0.753266, acc: 51.76%] [G loss: 0.744700]\n",
      "100 [D loss: 0.747510, acc: 49.61%] [G loss: 0.747351]\n",
      "Epoch: 100, auc: 0.52340, ks: 0.0379449809942749\n",
      "105 [D loss: 0.746051, acc: 51.17%] [G loss: 0.745561]\n",
      "110 [D loss: 0.743998, acc: 50.20%] [G loss: 0.741509]\n",
      "115 [D loss: 0.738686, acc: 49.80%] [G loss: 0.737484]\n",
      "120 [D loss: 0.739632, acc: 50.59%] [G loss: 0.740658]\n",
      "125 [D loss: 0.737153, acc: 49.02%] [G loss: 0.739066]\n",
      "130 [D loss: 0.734817, acc: 49.22%] [G loss: 0.738205]\n",
      "135 [D loss: 0.736347, acc: 49.02%] [G loss: 0.738572]\n",
      "140 [D loss: 0.733547, acc: 50.20%] [G loss: 0.734219]\n",
      "145 [D loss: 0.732037, acc: 50.39%] [G loss: 0.732077]\n",
      "150 [D loss: 0.736872, acc: 50.59%] [G loss: 0.730299]\n",
      "Epoch: 150, auc: 0.52404, ks: 0.03773010009139788\n",
      "155 [D loss: 0.728627, acc: 47.85%] [G loss: 0.728446]\n",
      "160 [D loss: 0.729745, acc: 48.83%] [G loss: 0.726399]\n",
      "165 [D loss: 0.729563, acc: 50.39%] [G loss: 0.729571]\n",
      "170 [D loss: 0.729227, acc: 48.05%] [G loss: 0.729414]\n",
      "175 [D loss: 0.733169, acc: 50.98%] [G loss: 0.724694]\n",
      "180 [D loss: 0.726189, acc: 47.66%] [G loss: 0.724056]\n",
      "185 [D loss: 0.724601, acc: 51.17%] [G loss: 0.726089]\n",
      "190 [D loss: 0.723203, acc: 50.20%] [G loss: 0.725559]\n",
      "195 [D loss: 0.722951, acc: 49.22%] [G loss: 0.727523]\n",
      "200 [D loss: 0.723624, acc: 48.44%] [G loss: 0.723089]\n",
      "Epoch: 200, auc: 0.52640, ks: 0.04083059572102238\n",
      "205 [D loss: 0.722504, acc: 48.83%] [G loss: 0.722168]\n",
      "210 [D loss: 0.722376, acc: 51.56%] [G loss: 0.722625]\n",
      "215 [D loss: 0.722208, acc: 50.78%] [G loss: 0.720615]\n",
      "220 [D loss: 0.721471, acc: 51.17%] [G loss: 0.724066]\n",
      "225 [D loss: 0.722377, acc: 47.27%] [G loss: 0.721075]\n",
      "230 [D loss: 0.717911, acc: 49.80%] [G loss: 0.721110]\n",
      "235 [D loss: 0.718561, acc: 47.85%] [G loss: 0.720986]\n",
      "240 [D loss: 0.718467, acc: 51.76%] [G loss: 0.719846]\n",
      "245 [D loss: 0.717493, acc: 50.78%] [G loss: 0.717574]\n",
      "250 [D loss: 0.718060, acc: 51.17%] [G loss: 0.716154]\n",
      "Epoch: 250, auc: 0.52759, ks: 0.04145150195815217\n",
      "255 [D loss: 0.715958, acc: 49.02%] [G loss: 0.720280]\n",
      "260 [D loss: 0.716168, acc: 49.22%] [G loss: 0.718994]\n",
      "265 [D loss: 0.714530, acc: 47.85%] [G loss: 0.718499]\n",
      "270 [D loss: 0.716426, acc: 48.63%] [G loss: 0.716819]\n",
      "275 [D loss: 0.717458, acc: 48.63%] [G loss: 0.716980]\n",
      "280 [D loss: 0.715898, acc: 50.00%] [G loss: 0.716318]\n",
      "285 [D loss: 0.714884, acc: 47.85%] [G loss: 0.714003]\n",
      "290 [D loss: 0.715732, acc: 53.32%] [G loss: 0.714925]\n",
      "295 [D loss: 0.713988, acc: 49.61%] [G loss: 0.712177]\n",
      "300 [D loss: 0.712734, acc: 49.22%] [G loss: 0.714902]\n",
      "Epoch: 300, auc: 0.52829, ks: 0.041488007014323036\n",
      "305 [D loss: 0.714421, acc: 46.68%] [G loss: 0.714853]\n",
      "310 [D loss: 0.713716, acc: 47.85%] [G loss: 0.714040]\n",
      "315 [D loss: 0.711531, acc: 51.17%] [G loss: 0.711112]\n",
      "320 [D loss: 0.710435, acc: 49.41%] [G loss: 0.711681]\n",
      "325 [D loss: 0.712609, acc: 49.61%] [G loss: 0.711195]\n",
      "330 [D loss: 0.713542, acc: 51.56%] [G loss: 0.712274]\n",
      "335 [D loss: 0.712501, acc: 49.22%] [G loss: 0.711493]\n",
      "340 [D loss: 0.712814, acc: 50.78%] [G loss: 0.711372]\n",
      "345 [D loss: 0.711827, acc: 49.02%] [G loss: 0.708759]\n",
      "350 [D loss: 0.709872, acc: 48.24%] [G loss: 0.711279]\n",
      "Epoch: 350, auc: 0.52789, ks: 0.04011772578126355\n",
      "355 [D loss: 0.711785, acc: 52.15%] [G loss: 0.711623]\n",
      "360 [D loss: 0.709252, acc: 48.05%] [G loss: 0.709770]\n",
      "365 [D loss: 0.711410, acc: 49.80%] [G loss: 0.709055]\n",
      "370 [D loss: 0.709835, acc: 47.46%] [G loss: 0.710304]\n",
      "375 [D loss: 0.713230, acc: 51.76%] [G loss: 0.708820]\n",
      "380 [D loss: 0.710357, acc: 50.78%] [G loss: 0.707988]\n",
      "385 [D loss: 0.709571, acc: 50.39%] [G loss: 0.711354]\n",
      "390 [D loss: 0.709697, acc: 48.63%] [G loss: 0.709798]\n",
      "395 [D loss: 0.708529, acc: 48.24%] [G loss: 0.708604]\n",
      "400 [D loss: 0.710545, acc: 50.20%] [G loss: 0.707984]\n",
      "Epoch: 400, auc: 0.52809, ks: 0.03948709719928495\n",
      "405 [D loss: 0.710690, acc: 50.59%] [G loss: 0.708318]\n",
      "410 [D loss: 0.707871, acc: 49.02%] [G loss: 0.708360]\n",
      "415 [D loss: 0.708254, acc: 51.76%] [G loss: 0.707347]\n",
      "420 [D loss: 0.709327, acc: 50.00%] [G loss: 0.706814]\n",
      "425 [D loss: 0.707630, acc: 49.61%] [G loss: 0.706054]\n",
      "430 [D loss: 0.709228, acc: 50.98%] [G loss: 0.706916]\n",
      "435 [D loss: 0.707124, acc: 48.24%] [G loss: 0.705921]\n",
      "440 [D loss: 0.708044, acc: 48.63%] [G loss: 0.707996]\n",
      "445 [D loss: 0.708524, acc: 49.80%] [G loss: 0.707127]\n",
      "450 [D loss: 0.706011, acc: 48.24%] [G loss: 0.707147]\n",
      "Epoch: 450, auc: 0.52843, ks: 0.03859615746027867\n",
      "455 [D loss: 0.706518, acc: 51.17%] [G loss: 0.706139]\n",
      "460 [D loss: 0.707107, acc: 47.66%] [G loss: 0.707456]\n",
      "465 [D loss: 0.708453, acc: 51.56%] [G loss: 0.705567]\n",
      "470 [D loss: 0.706520, acc: 50.59%] [G loss: 0.706089]\n",
      "475 [D loss: 0.706636, acc: 49.22%] [G loss: 0.705750]\n",
      "480 [D loss: 0.707976, acc: 50.39%] [G loss: 0.706111]\n",
      "485 [D loss: 0.706844, acc: 50.00%] [G loss: 0.706550]\n",
      "490 [D loss: 0.706130, acc: 48.05%] [G loss: 0.704418]\n",
      "495 [D loss: 0.705842, acc: 50.78%] [G loss: 0.705000]\n",
      "500 [D loss: 0.706189, acc: 50.39%] [G loss: 0.705110]\n",
      "Epoch: 500, auc: 0.52915, ks: 0.03866922451168253\n",
      "505 [D loss: 0.704822, acc: 47.66%] [G loss: 0.703937]\n",
      "510 [D loss: 0.704841, acc: 50.39%] [G loss: 0.704764]\n",
      "515 [D loss: 0.704597, acc: 49.41%] [G loss: 0.705052]\n",
      "520 [D loss: 0.705166, acc: 50.98%] [G loss: 0.705249]\n",
      "525 [D loss: 0.705215, acc: 47.07%] [G loss: 0.704893]\n",
      "530 [D loss: 0.704352, acc: 49.22%] [G loss: 0.703967]\n",
      "535 [D loss: 0.704376, acc: 48.44%] [G loss: 0.704014]\n",
      "540 [D loss: 0.703807, acc: 48.44%] [G loss: 0.704679]\n",
      "545 [D loss: 0.705378, acc: 49.02%] [G loss: 0.704492]\n",
      "550 [D loss: 0.704085, acc: 49.41%] [G loss: 0.703912]\n",
      "Epoch: 550, auc: 0.52900, ks: 0.038397354725054234\n",
      "555 [D loss: 0.705327, acc: 49.02%] [G loss: 0.703770]\n",
      "560 [D loss: 0.703992, acc: 48.05%] [G loss: 0.703723]\n",
      "565 [D loss: 0.704714, acc: 51.17%] [G loss: 0.704194]\n",
      "570 [D loss: 0.703721, acc: 51.56%] [G loss: 0.702243]\n",
      "575 [D loss: 0.704181, acc: 50.78%] [G loss: 0.703590]\n",
      "580 [D loss: 0.703717, acc: 49.22%] [G loss: 0.702492]\n",
      "585 [D loss: 0.704138, acc: 47.66%] [G loss: 0.702879]\n",
      "590 [D loss: 0.703669, acc: 50.00%] [G loss: 0.703756]\n",
      "595 [D loss: 0.704060, acc: 50.98%] [G loss: 0.702280]\n",
      "600 [D loss: 0.703517, acc: 50.78%] [G loss: 0.702834]\n",
      "Epoch: 600, auc: 0.52915, ks: 0.03933956808945638\n",
      "605 [D loss: 0.702712, acc: 51.37%] [G loss: 0.703103]\n",
      "610 [D loss: 0.703555, acc: 48.24%] [G loss: 0.703591]\n",
      "615 [D loss: 0.703463, acc: 48.83%] [G loss: 0.702768]\n",
      "620 [D loss: 0.703685, acc: 48.83%] [G loss: 0.702908]\n",
      "625 [D loss: 0.702541, acc: 47.85%] [G loss: 0.702760]\n",
      "630 [D loss: 0.703304, acc: 50.00%] [G loss: 0.703794]\n",
      "635 [D loss: 0.702862, acc: 48.44%] [G loss: 0.701483]\n",
      "640 [D loss: 0.702559, acc: 49.22%] [G loss: 0.702652]\n",
      "645 [D loss: 0.702838, acc: 48.83%] [G loss: 0.702294]\n",
      "650 [D loss: 0.702482, acc: 50.39%] [G loss: 0.701989]\n",
      "Epoch: 650, auc: 0.52932, ks: 0.0405674376108911\n",
      "655 [D loss: 0.701248, acc: 52.73%] [G loss: 0.701705]\n",
      "660 [D loss: 0.702200, acc: 51.95%] [G loss: 0.701482]\n",
      "665 [D loss: 0.701777, acc: 48.44%] [G loss: 0.702252]\n",
      "670 [D loss: 0.702744, acc: 49.80%] [G loss: 0.702131]\n",
      "675 [D loss: 0.702759, acc: 50.59%] [G loss: 0.702190]\n",
      "680 [D loss: 0.701910, acc: 50.39%] [G loss: 0.701726]\n",
      "685 [D loss: 0.702530, acc: 49.41%] [G loss: 0.702313]\n",
      "690 [D loss: 0.701972, acc: 50.20%] [G loss: 0.702542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "695 [D loss: 0.701787, acc: 49.22%] [G loss: 0.701110]\n",
      "700 [D loss: 0.701649, acc: 48.83%] [G loss: 0.701648]\n",
      "Epoch: 700, auc: 0.52970, ks: 0.04034501228228948\n",
      "705 [D loss: 0.701531, acc: 49.02%] [G loss: 0.701755]\n",
      "710 [D loss: 0.700906, acc: 49.22%] [G loss: 0.701574]\n",
      "715 [D loss: 0.700632, acc: 50.20%] [G loss: 0.701361]\n",
      "720 [D loss: 0.701633, acc: 48.05%] [G loss: 0.701533]\n",
      "725 [D loss: 0.701770, acc: 50.59%] [G loss: 0.700700]\n",
      "730 [D loss: 0.702211, acc: 47.66%] [G loss: 0.701680]\n",
      "735 [D loss: 0.700998, acc: 49.41%] [G loss: 0.701270]\n",
      "740 [D loss: 0.700160, acc: 49.02%] [G loss: 0.701492]\n",
      "745 [D loss: 0.701374, acc: 51.37%] [G loss: 0.700605]\n",
      "750 [D loss: 0.700964, acc: 51.76%] [G loss: 0.700848]\n",
      "Epoch: 750, auc: 0.53035, ks: 0.041982771876815095\n",
      "755 [D loss: 0.700612, acc: 48.24%] [G loss: 0.700624]\n",
      "760 [D loss: 0.700901, acc: 48.44%] [G loss: 0.701299]\n",
      "765 [D loss: 0.701165, acc: 50.59%] [G loss: 0.700351]\n",
      "770 [D loss: 0.701104, acc: 48.63%] [G loss: 0.699798]\n",
      "775 [D loss: 0.701307, acc: 48.63%] [G loss: 0.700463]\n",
      "780 [D loss: 0.699967, acc: 47.85%] [G loss: 0.700005]\n",
      "785 [D loss: 0.700738, acc: 49.22%] [G loss: 0.700567]\n",
      "790 [D loss: 0.700247, acc: 49.80%] [G loss: 0.700515]\n",
      "795 [D loss: 0.700010, acc: 50.39%] [G loss: 0.700631]\n",
      "800 [D loss: 0.700056, acc: 50.78%] [G loss: 0.700300]\n",
      "Epoch: 800, auc: 0.53072, ks: 0.04215946090380729\n",
      "805 [D loss: 0.701095, acc: 49.41%] [G loss: 0.699946]\n",
      "810 [D loss: 0.700875, acc: 48.44%] [G loss: 0.700771]\n",
      "815 [D loss: 0.700535, acc: 50.00%] [G loss: 0.700021]\n",
      "820 [D loss: 0.700701, acc: 49.80%] [G loss: 0.700477]\n",
      "825 [D loss: 0.700584, acc: 50.78%] [G loss: 0.700251]\n",
      "830 [D loss: 0.699536, acc: 50.39%] [G loss: 0.699909]\n",
      "835 [D loss: 0.700468, acc: 48.44%] [G loss: 0.699488]\n",
      "840 [D loss: 0.700627, acc: 51.17%] [G loss: 0.699846]\n",
      "845 [D loss: 0.699667, acc: 50.20%] [G loss: 0.699565]\n",
      "850 [D loss: 0.700428, acc: 49.02%] [G loss: 0.699198]\n",
      "Epoch: 850, auc: 0.53060, ks: 0.043904579099869556\n",
      "855 [D loss: 0.699683, acc: 48.05%] [G loss: 0.700054]\n",
      "860 [D loss: 0.699903, acc: 45.90%] [G loss: 0.699462]\n",
      "865 [D loss: 0.699703, acc: 47.85%] [G loss: 0.700188]\n",
      "870 [D loss: 0.699420, acc: 47.07%] [G loss: 0.699723]\n",
      "875 [D loss: 0.700412, acc: 49.80%] [G loss: 0.699960]\n",
      "880 [D loss: 0.700118, acc: 45.51%] [G loss: 0.699889]\n",
      "885 [D loss: 0.699674, acc: 49.80%] [G loss: 0.699648]\n",
      "890 [D loss: 0.700724, acc: 48.83%] [G loss: 0.699614]\n",
      "895 [D loss: 0.699196, acc: 46.48%] [G loss: 0.699517]\n",
      "900 [D loss: 0.699412, acc: 50.00%] [G loss: 0.699214]\n",
      "Epoch: 900, auc: 0.53114, ks: 0.045328674863969964\n",
      "905 [D loss: 0.699336, acc: 47.46%] [G loss: 0.699418]\n",
      "910 [D loss: 0.699780, acc: 50.20%] [G loss: 0.698302]\n",
      "915 [D loss: 0.699390, acc: 49.22%] [G loss: 0.699657]\n",
      "920 [D loss: 0.699640, acc: 49.61%] [G loss: 0.699230]\n",
      "925 [D loss: 0.699208, acc: 46.48%] [G loss: 0.699299]\n",
      "930 [D loss: 0.699722, acc: 50.78%] [G loss: 0.699163]\n",
      "935 [D loss: 0.699850, acc: 48.05%] [G loss: 0.698865]\n",
      "940 [D loss: 0.699413, acc: 48.05%] [G loss: 0.699015]\n",
      "945 [D loss: 0.698927, acc: 48.63%] [G loss: 0.699103]\n",
      "950 [D loss: 0.699272, acc: 46.68%] [G loss: 0.698956]\n",
      "Epoch: 950, auc: 0.53078, ks: 0.044640011142974445\n",
      "955 [D loss: 0.699737, acc: 51.17%] [G loss: 0.698424]\n",
      "960 [D loss: 0.699014, acc: 49.80%] [G loss: 0.698815]\n",
      "965 [D loss: 0.699201, acc: 49.41%] [G loss: 0.698748]\n",
      "970 [D loss: 0.699384, acc: 49.61%] [G loss: 0.699294]\n",
      "975 [D loss: 0.698972, acc: 50.39%] [G loss: 0.698715]\n",
      "980 [D loss: 0.699301, acc: 48.63%] [G loss: 0.699071]\n",
      "985 [D loss: 0.698735, acc: 48.05%] [G loss: 0.698524]\n",
      "990 [D loss: 0.699019, acc: 49.02%] [G loss: 0.698871]\n",
      "995 [D loss: 0.698615, acc: 48.05%] [G loss: 0.698808]\n",
      "1000 [D loss: 0.698638, acc: 49.02%] [G loss: 0.698726]\n",
      "Epoch: 1000, auc: 0.53071, ks: 0.0449977948568866\n",
      "1005 [D loss: 0.698801, acc: 50.78%] [G loss: 0.698438]\n",
      "1010 [D loss: 0.698860, acc: 49.80%] [G loss: 0.698440]\n",
      "1015 [D loss: 0.698854, acc: 48.63%] [G loss: 0.698220]\n",
      "1020 [D loss: 0.698933, acc: 47.85%] [G loss: 0.698600]\n",
      "1025 [D loss: 0.698311, acc: 50.98%] [G loss: 0.697772]\n",
      "1030 [D loss: 0.698517, acc: 47.66%] [G loss: 0.698326]\n",
      "1035 [D loss: 0.698478, acc: 49.80%] [G loss: 0.698480]\n",
      "1040 [D loss: 0.698259, acc: 48.24%] [G loss: 0.698717]\n",
      "1045 [D loss: 0.698120, acc: 51.56%] [G loss: 0.698800]\n",
      "1050 [D loss: 0.698323, acc: 48.05%] [G loss: 0.698115]\n",
      "Epoch: 1050, auc: 0.53075, ks: 0.04649295768783568\n",
      "1055 [D loss: 0.698447, acc: 50.98%] [G loss: 0.698159]\n",
      "1060 [D loss: 0.698094, acc: 46.68%] [G loss: 0.698205]\n",
      "1065 [D loss: 0.698225, acc: 50.20%] [G loss: 0.698177]\n",
      "1070 [D loss: 0.698451, acc: 49.61%] [G loss: 0.698230]\n",
      "1075 [D loss: 0.698213, acc: 47.07%] [G loss: 0.698236]\n",
      "1080 [D loss: 0.698293, acc: 48.83%] [G loss: 0.697957]\n",
      "1085 [D loss: 0.698536, acc: 49.80%] [G loss: 0.698253]\n",
      "1090 [D loss: 0.697923, acc: 51.17%] [G loss: 0.698024]\n",
      "1095 [D loss: 0.698019, acc: 48.05%] [G loss: 0.698346]\n",
      "1100 [D loss: 0.698489, acc: 49.61%] [G loss: 0.698064]\n",
      "Epoch: 1100, auc: 0.53081, ks: 0.04909209064115061\n",
      "1105 [D loss: 0.697807, acc: 49.02%] [G loss: 0.698023]\n",
      "1110 [D loss: 0.698211, acc: 48.83%] [G loss: 0.697943]\n",
      "1115 [D loss: 0.698038, acc: 49.22%] [G loss: 0.697982]\n",
      "1120 [D loss: 0.698069, acc: 50.59%] [G loss: 0.698075]\n",
      "1125 [D loss: 0.697995, acc: 47.46%] [G loss: 0.697820]\n",
      "1130 [D loss: 0.698119, acc: 50.39%] [G loss: 0.697769]\n",
      "1135 [D loss: 0.697859, acc: 50.78%] [G loss: 0.698270]\n",
      "1140 [D loss: 0.698053, acc: 50.39%] [G loss: 0.697471]\n",
      "1145 [D loss: 0.698199, acc: 49.61%] [G loss: 0.697623]\n",
      "1150 [D loss: 0.697896, acc: 50.00%] [G loss: 0.697851]\n",
      "Epoch: 1150, auc: 0.53065, ks: 0.04882721012439151\n",
      "1155 [D loss: 0.697771, acc: 49.02%] [G loss: 0.697313]\n",
      "1160 [D loss: 0.698038, acc: 50.20%] [G loss: 0.697644]\n",
      "1165 [D loss: 0.697863, acc: 45.90%] [G loss: 0.697647]\n",
      "1170 [D loss: 0.697643, acc: 50.20%] [G loss: 0.697499]\n",
      "1175 [D loss: 0.697541, acc: 49.02%] [G loss: 0.697521]\n",
      "1180 [D loss: 0.698043, acc: 49.22%] [G loss: 0.697480]\n",
      "1185 [D loss: 0.697787, acc: 49.41%] [G loss: 0.697857]\n",
      "1190 [D loss: 0.697508, acc: 48.83%] [G loss: 0.697808]\n",
      "1195 [D loss: 0.698082, acc: 49.80%] [G loss: 0.697348]\n",
      "1200 [D loss: 0.697640, acc: 49.61%] [G loss: 0.697629]\n",
      "Epoch: 1200, auc: 0.53059, ks: 0.04968949528041011\n",
      "1205 [D loss: 0.697648, acc: 50.78%] [G loss: 0.697347]\n",
      "1210 [D loss: 0.697710, acc: 47.66%] [G loss: 0.697561]\n",
      "1215 [D loss: 0.697639, acc: 49.61%] [G loss: 0.697308]\n",
      "1220 [D loss: 0.697720, acc: 47.07%] [G loss: 0.697549]\n",
      "1225 [D loss: 0.697610, acc: 50.39%] [G loss: 0.697249]\n",
      "1230 [D loss: 0.697793, acc: 49.80%] [G loss: 0.697243]\n",
      "1235 [D loss: 0.697634, acc: 50.59%] [G loss: 0.697666]\n",
      "1240 [D loss: 0.697700, acc: 51.56%] [G loss: 0.697326]\n",
      "1245 [D loss: 0.697818, acc: 47.46%] [G loss: 0.697373]\n",
      "1250 [D loss: 0.697364, acc: 47.27%] [G loss: 0.697401]\n",
      "Epoch: 1250, auc: 0.53091, ks: 0.04907355697644622\n",
      "1255 [D loss: 0.697392, acc: 50.59%] [G loss: 0.697193]\n",
      "1260 [D loss: 0.697878, acc: 50.39%] [G loss: 0.697131]\n",
      "1265 [D loss: 0.697518, acc: 49.22%] [G loss: 0.697119]\n",
      "1270 [D loss: 0.697386, acc: 49.02%] [G loss: 0.697091]\n",
      "1275 [D loss: 0.697288, acc: 49.41%] [G loss: 0.697383]\n",
      "1280 [D loss: 0.697439, acc: 47.27%] [G loss: 0.696974]\n",
      "1285 [D loss: 0.697419, acc: 45.90%] [G loss: 0.697163]\n",
      "1290 [D loss: 0.697639, acc: 49.02%] [G loss: 0.696685]\n",
      "1295 [D loss: 0.697363, acc: 50.39%] [G loss: 0.697118]\n",
      "1300 [D loss: 0.697326, acc: 50.20%] [G loss: 0.697563]\n",
      "Epoch: 1300, auc: 0.53088, ks: 0.048828733244301925\n",
      "1305 [D loss: 0.697522, acc: 49.80%] [G loss: 0.697342]\n",
      "1310 [D loss: 0.697042, acc: 51.76%] [G loss: 0.696959]\n",
      "1315 [D loss: 0.697232, acc: 49.22%] [G loss: 0.697290]\n",
      "1320 [D loss: 0.697295, acc: 48.83%] [G loss: 0.697129]\n",
      "1325 [D loss: 0.697366, acc: 50.00%] [G loss: 0.697004]\n",
      "1330 [D loss: 0.697135, acc: 50.00%] [G loss: 0.697167]\n",
      "1335 [D loss: 0.697351, acc: 51.76%] [G loss: 0.696901]\n",
      "1340 [D loss: 0.697386, acc: 47.27%] [G loss: 0.696761]\n",
      "1345 [D loss: 0.697235, acc: 49.02%] [G loss: 0.696807]\n",
      "1350 [D loss: 0.697246, acc: 48.83%] [G loss: 0.697040]\n",
      "Epoch: 1350, auc: 0.53070, ks: 0.04895150809689475\n",
      "1355 [D loss: 0.697117, acc: 48.83%] [G loss: 0.697089]\n",
      "1360 [D loss: 0.696908, acc: 48.05%] [G loss: 0.696610]\n",
      "1365 [D loss: 0.697228, acc: 46.68%] [G loss: 0.696724]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1370 [D loss: 0.697039, acc: 47.85%] [G loss: 0.696783]\n",
      "1375 [D loss: 0.696783, acc: 49.41%] [G loss: 0.697133]\n",
      "1380 [D loss: 0.697330, acc: 50.78%] [G loss: 0.697107]\n",
      "1385 [D loss: 0.697230, acc: 49.80%] [G loss: 0.696824]\n",
      "1390 [D loss: 0.697008, acc: 49.61%] [G loss: 0.696645]\n",
      "1395 [D loss: 0.697264, acc: 51.17%] [G loss: 0.696589]\n",
      "1400 [D loss: 0.696843, acc: 51.17%] [G loss: 0.696676]\n",
      "Epoch: 1400, auc: 0.53110, ks: 0.04844742658037127\n",
      "1405 [D loss: 0.696847, acc: 49.41%] [G loss: 0.696731]\n",
      "1410 [D loss: 0.696776, acc: 46.68%] [G loss: 0.696567]\n",
      "1415 [D loss: 0.696694, acc: 47.66%] [G loss: 0.696811]\n",
      "1420 [D loss: 0.696974, acc: 49.61%] [G loss: 0.696596]\n",
      "1425 [D loss: 0.696863, acc: 50.59%] [G loss: 0.696738]\n",
      "1430 [D loss: 0.696729, acc: 49.41%] [G loss: 0.696661]\n",
      "1435 [D loss: 0.696982, acc: 48.05%] [G loss: 0.696628]\n",
      "1440 [D loss: 0.697191, acc: 46.88%] [G loss: 0.696738]\n",
      "1445 [D loss: 0.696788, acc: 46.48%] [G loss: 0.696349]\n",
      "1450 [D loss: 0.696632, acc: 52.73%] [G loss: 0.696585]\n",
      "Epoch: 1450, auc: 0.53141, ks: 0.04893923061163552\n",
      "1455 [D loss: 0.696597, acc: 49.80%] [G loss: 0.696642]\n",
      "1460 [D loss: 0.696834, acc: 46.68%] [G loss: 0.696571]\n",
      "1465 [D loss: 0.696778, acc: 49.22%] [G loss: 0.696538]\n",
      "1470 [D loss: 0.696663, acc: 46.88%] [G loss: 0.696389]\n",
      "1475 [D loss: 0.696707, acc: 49.61%] [G loss: 0.696689]\n",
      "1480 [D loss: 0.696488, acc: 48.83%] [G loss: 0.696662]\n",
      "1485 [D loss: 0.696372, acc: 48.24%] [G loss: 0.696512]\n",
      "1490 [D loss: 0.696575, acc: 49.80%] [G loss: 0.696907]\n",
      "1495 [D loss: 0.696713, acc: 48.05%] [G loss: 0.696516]\n",
      "1500 [D loss: 0.696655, acc: 50.39%] [G loss: 0.696557]\n",
      "Epoch: 1500, auc: 0.53091, ks: 0.04889405658326407\n",
      "1505 [D loss: 0.696713, acc: 51.95%] [G loss: 0.696659]\n",
      "1510 [D loss: 0.696633, acc: 48.44%] [G loss: 0.696319]\n",
      "1515 [D loss: 0.696636, acc: 48.83%] [G loss: 0.696474]\n",
      "1520 [D loss: 0.696833, acc: 47.27%] [G loss: 0.696504]\n",
      "1525 [D loss: 0.696783, acc: 46.88%] [G loss: 0.696282]\n",
      "1530 [D loss: 0.696421, acc: 48.05%] [G loss: 0.696454]\n",
      "1535 [D loss: 0.696425, acc: 48.44%] [G loss: 0.696375]\n",
      "1540 [D loss: 0.696532, acc: 48.44%] [G loss: 0.696338]\n",
      "1545 [D loss: 0.696228, acc: 51.95%] [G loss: 0.696309]\n",
      "1550 [D loss: 0.696550, acc: 49.41%] [G loss: 0.696402]\n",
      "Epoch: 1550, auc: 0.53078, ks: 0.0484154908839316\n",
      "1555 [D loss: 0.696437, acc: 47.46%] [G loss: 0.696312]\n",
      "1560 [D loss: 0.696651, acc: 49.61%] [G loss: 0.696278]\n",
      "1565 [D loss: 0.696398, acc: 46.29%] [G loss: 0.696478]\n",
      "1570 [D loss: 0.696494, acc: 50.98%] [G loss: 0.696350]\n",
      "1575 [D loss: 0.696240, acc: 49.02%] [G loss: 0.696333]\n",
      "1580 [D loss: 0.696609, acc: 50.59%] [G loss: 0.696556]\n",
      "1585 [D loss: 0.696531, acc: 49.41%] [G loss: 0.696398]\n",
      "1590 [D loss: 0.696393, acc: 47.85%] [G loss: 0.696355]\n",
      "1595 [D loss: 0.696328, acc: 49.22%] [G loss: 0.696273]\n",
      "1600 [D loss: 0.696555, acc: 52.15%] [G loss: 0.696275]\n",
      "Epoch: 1600, auc: 0.53097, ks: 0.04944565374708676\n",
      "1605 [D loss: 0.696243, acc: 50.00%] [G loss: 0.696472]\n",
      "1610 [D loss: 0.696411, acc: 49.61%] [G loss: 0.696318]\n",
      "1615 [D loss: 0.696279, acc: 49.02%] [G loss: 0.696289]\n",
      "1620 [D loss: 0.696193, acc: 50.39%] [G loss: 0.696065]\n",
      "1625 [D loss: 0.696149, acc: 49.80%] [G loss: 0.696364]\n",
      "1630 [D loss: 0.696449, acc: 47.66%] [G loss: 0.696194]\n",
      "1635 [D loss: 0.696437, acc: 47.66%] [G loss: 0.696256]\n",
      "1640 [D loss: 0.696410, acc: 50.98%] [G loss: 0.696192]\n",
      "1645 [D loss: 0.696430, acc: 47.27%] [G loss: 0.696089]\n",
      "1650 [D loss: 0.696254, acc: 47.66%] [G loss: 0.696203]\n",
      "Epoch: 1650, auc: 0.53122, ks: 0.04946498455866011\n",
      "1655 [D loss: 0.696164, acc: 47.66%] [G loss: 0.696104]\n",
      "1660 [D loss: 0.696147, acc: 46.68%] [G loss: 0.696102]\n",
      "1665 [D loss: 0.696352, acc: 50.00%] [G loss: 0.696123]\n",
      "1670 [D loss: 0.696246, acc: 47.85%] [G loss: 0.696058]\n",
      "1675 [D loss: 0.696209, acc: 46.48%] [G loss: 0.695943]\n",
      "1680 [D loss: 0.696196, acc: 49.22%] [G loss: 0.695852]\n",
      "1685 [D loss: 0.695876, acc: 50.59%] [G loss: 0.696199]\n",
      "1690 [D loss: 0.696250, acc: 50.59%] [G loss: 0.696248]\n",
      "1695 [D loss: 0.696047, acc: 47.07%] [G loss: 0.695987]\n",
      "1700 [D loss: 0.696087, acc: 48.05%] [G loss: 0.695955]\n",
      "Epoch: 1700, auc: 0.53144, ks: 0.04977086831749378\n",
      "1705 [D loss: 0.696212, acc: 46.68%] [G loss: 0.695872]\n",
      "1710 [D loss: 0.696275, acc: 47.85%] [G loss: 0.695929]\n",
      "1715 [D loss: 0.696223, acc: 47.66%] [G loss: 0.695895]\n",
      "1720 [D loss: 0.696020, acc: 49.02%] [G loss: 0.695845]\n",
      "1725 [D loss: 0.696180, acc: 50.00%] [G loss: 0.695806]\n",
      "1730 [D loss: 0.695999, acc: 48.24%] [G loss: 0.695968]\n",
      "1735 [D loss: 0.696149, acc: 48.44%] [G loss: 0.695907]\n",
      "1740 [D loss: 0.695989, acc: 49.61%] [G loss: 0.695829]\n",
      "1745 [D loss: 0.696073, acc: 52.15%] [G loss: 0.695961]\n",
      "1750 [D loss: 0.695929, acc: 48.83%] [G loss: 0.695848]\n",
      "Epoch: 1750, auc: 0.53122, ks: 0.050048987166184555\n",
      "1755 [D loss: 0.696051, acc: 47.27%] [G loss: 0.695989]\n",
      "1760 [D loss: 0.696081, acc: 49.80%] [G loss: 0.695826]\n",
      "1765 [D loss: 0.696102, acc: 49.02%] [G loss: 0.695867]\n",
      "1770 [D loss: 0.696021, acc: 49.41%] [G loss: 0.695879]\n",
      "1775 [D loss: 0.695810, acc: 50.00%] [G loss: 0.695920]\n",
      "1780 [D loss: 0.696039, acc: 47.46%] [G loss: 0.695832]\n",
      "1785 [D loss: 0.695903, acc: 49.22%] [G loss: 0.695880]\n",
      "1790 [D loss: 0.695903, acc: 48.05%] [G loss: 0.695892]\n",
      "1795 [D loss: 0.696137, acc: 48.05%] [G loss: 0.695743]\n",
      "1800 [D loss: 0.695911, acc: 47.66%] [G loss: 0.695939]\n",
      "Epoch: 1800, auc: 0.53117, ks: 0.050135328136433954\n",
      "1805 [D loss: 0.695941, acc: 48.63%] [G loss: 0.695860]\n",
      "1810 [D loss: 0.695728, acc: 47.27%] [G loss: 0.695877]\n",
      "1815 [D loss: 0.695898, acc: 45.90%] [G loss: 0.695605]\n",
      "1820 [D loss: 0.696180, acc: 46.68%] [G loss: 0.695761]\n",
      "1825 [D loss: 0.695767, acc: 47.46%] [G loss: 0.695873]\n",
      "1830 [D loss: 0.696051, acc: 47.66%] [G loss: 0.695903]\n",
      "1835 [D loss: 0.695976, acc: 47.07%] [G loss: 0.695825]\n",
      "1840 [D loss: 0.695705, acc: 47.46%] [G loss: 0.695749]\n",
      "1845 [D loss: 0.695854, acc: 45.90%] [G loss: 0.695858]\n",
      "1850 [D loss: 0.695871, acc: 48.44%] [G loss: 0.695776]\n",
      "Epoch: 1850, auc: 0.53143, ks: 0.05040719080567946\n",
      "1855 [D loss: 0.696051, acc: 48.63%] [G loss: 0.695754]\n",
      "1860 [D loss: 0.695926, acc: 50.00%] [G loss: 0.695664]\n",
      "1865 [D loss: 0.695912, acc: 48.44%] [G loss: 0.695943]\n",
      "1870 [D loss: 0.695712, acc: 48.83%] [G loss: 0.695671]\n",
      "1875 [D loss: 0.695736, acc: 49.80%] [G loss: 0.695764]\n",
      "1880 [D loss: 0.695812, acc: 49.61%] [G loss: 0.695880]\n",
      "1885 [D loss: 0.695824, acc: 49.61%] [G loss: 0.695672]\n",
      "1890 [D loss: 0.695738, acc: 48.24%] [G loss: 0.695659]\n",
      "1895 [D loss: 0.695817, acc: 49.02%] [G loss: 0.695613]\n",
      "1900 [D loss: 0.695708, acc: 50.98%] [G loss: 0.695706]\n",
      "Epoch: 1900, auc: 0.53125, ks: 0.05090068165665762\n",
      "1905 [D loss: 0.696018, acc: 48.63%] [G loss: 0.695655]\n",
      "1910 [D loss: 0.695821, acc: 48.63%] [G loss: 0.695688]\n",
      "1915 [D loss: 0.695780, acc: 50.00%] [G loss: 0.695750]\n",
      "1920 [D loss: 0.695657, acc: 50.39%] [G loss: 0.695715]\n",
      "1925 [D loss: 0.695765, acc: 49.22%] [G loss: 0.695596]\n",
      "1930 [D loss: 0.695635, acc: 46.68%] [G loss: 0.695733]\n",
      "1935 [D loss: 0.695645, acc: 49.22%] [G loss: 0.695557]\n",
      "1940 [D loss: 0.695674, acc: 47.27%] [G loss: 0.695618]\n",
      "1945 [D loss: 0.695770, acc: 49.61%] [G loss: 0.695599]\n",
      "1950 [D loss: 0.695645, acc: 50.78%] [G loss: 0.695641]\n",
      "Epoch: 1950, auc: 0.53116, ks: 0.05061822832186669\n",
      "1955 [D loss: 0.695816, acc: 50.78%] [G loss: 0.695525]\n",
      "1960 [D loss: 0.695629, acc: 49.41%] [G loss: 0.695597]\n",
      "1965 [D loss: 0.695788, acc: 49.61%] [G loss: 0.695602]\n",
      "1970 [D loss: 0.695889, acc: 51.17%] [G loss: 0.695554]\n",
      "1975 [D loss: 0.695649, acc: 48.83%] [G loss: 0.695378]\n",
      "1980 [D loss: 0.695859, acc: 50.20%] [G loss: 0.695462]\n",
      "1985 [D loss: 0.695641, acc: 50.20%] [G loss: 0.695437]\n",
      "1990 [D loss: 0.695706, acc: 50.00%] [G loss: 0.695507]\n",
      "1995 [D loss: 0.695655, acc: 48.44%] [G loss: 0.695577]\n",
      "2000 [D loss: 0.695610, acc: 48.63%] [G loss: 0.695447]\n",
      "Epoch: 2000, auc: 0.53157, ks: 0.05030271474416015\n",
      "2005 [D loss: 0.695595, acc: 47.66%] [G loss: 0.695518]\n",
      "2010 [D loss: 0.695606, acc: 48.24%] [G loss: 0.695447]\n",
      "2015 [D loss: 0.695556, acc: 49.02%] [G loss: 0.695494]\n",
      "2020 [D loss: 0.695654, acc: 46.48%] [G loss: 0.695605]\n",
      "2025 [D loss: 0.695552, acc: 49.41%] [G loss: 0.695684]\n",
      "2030 [D loss: 0.695608, acc: 50.39%] [G loss: 0.695469]\n",
      "2035 [D loss: 0.695563, acc: 50.98%] [G loss: 0.695565]\n",
      "2040 [D loss: 0.695512, acc: 47.85%] [G loss: 0.695558]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2045 [D loss: 0.695418, acc: 48.83%] [G loss: 0.695545]\n",
      "2050 [D loss: 0.695648, acc: 47.27%] [G loss: 0.695444]\n",
      "Epoch: 2050, auc: 0.53145, ks: 0.05028080744002805\n",
      "2055 [D loss: 0.695520, acc: 48.05%] [G loss: 0.695538]\n",
      "2060 [D loss: 0.695668, acc: 50.98%] [G loss: 0.695375]\n",
      "2065 [D loss: 0.695486, acc: 47.27%] [G loss: 0.695510]\n",
      "2070 [D loss: 0.695569, acc: 51.56%] [G loss: 0.695402]\n",
      "2075 [D loss: 0.695391, acc: 47.66%] [G loss: 0.695439]\n",
      "2080 [D loss: 0.695578, acc: 48.63%] [G loss: 0.695374]\n",
      "2085 [D loss: 0.695433, acc: 48.05%] [G loss: 0.695490]\n",
      "2090 [D loss: 0.695495, acc: 48.24%] [G loss: 0.695309]\n",
      "2095 [D loss: 0.695513, acc: 49.61%] [G loss: 0.695449]\n",
      "2100 [D loss: 0.695444, acc: 49.80%] [G loss: 0.695449]\n",
      "Epoch: 2100, auc: 0.53085, ks: 0.05070834150497838\n",
      "2105 [D loss: 0.695577, acc: 51.56%] [G loss: 0.695377]\n",
      "2110 [D loss: 0.695560, acc: 48.44%] [G loss: 0.695384]\n",
      "2115 [D loss: 0.695465, acc: 48.63%] [G loss: 0.695458]\n",
      "2120 [D loss: 0.695400, acc: 48.05%] [G loss: 0.695256]\n",
      "2125 [D loss: 0.695540, acc: 49.80%] [G loss: 0.695393]\n",
      "2130 [D loss: 0.695447, acc: 47.46%] [G loss: 0.695320]\n",
      "2135 [D loss: 0.695607, acc: 47.46%] [G loss: 0.695340]\n",
      "2140 [D loss: 0.695454, acc: 48.83%] [G loss: 0.695301]\n",
      "2145 [D loss: 0.695399, acc: 48.24%] [G loss: 0.695481]\n",
      "2150 [D loss: 0.695411, acc: 46.09%] [G loss: 0.695358]\n",
      "Epoch: 2150, auc: 0.53104, ks: 0.049583823498587254\n",
      "2155 [D loss: 0.695438, acc: 49.22%] [G loss: 0.695382]\n",
      "2160 [D loss: 0.695502, acc: 47.07%] [G loss: 0.695285]\n",
      "2165 [D loss: 0.695377, acc: 47.07%] [G loss: 0.695352]\n",
      "2170 [D loss: 0.695245, acc: 48.44%] [G loss: 0.695291]\n",
      "2175 [D loss: 0.695382, acc: 47.27%] [G loss: 0.695292]\n",
      "2180 [D loss: 0.695361, acc: 49.02%] [G loss: 0.695318]\n",
      "2185 [D loss: 0.695350, acc: 50.39%] [G loss: 0.695289]\n",
      "2190 [D loss: 0.695427, acc: 48.44%] [G loss: 0.695333]\n",
      "2195 [D loss: 0.695373, acc: 50.78%] [G loss: 0.695263]\n",
      "2200 [D loss: 0.695285, acc: 49.02%] [G loss: 0.695170]\n",
      "Epoch: 2200, auc: 0.53145, ks: 0.04919986916827013\n",
      "2205 [D loss: 0.695318, acc: 47.66%] [G loss: 0.695397]\n",
      "2210 [D loss: 0.695409, acc: 48.63%] [G loss: 0.695333]\n",
      "2215 [D loss: 0.695469, acc: 49.22%] [G loss: 0.695276]\n",
      "2220 [D loss: 0.695437, acc: 48.83%] [G loss: 0.695235]\n",
      "2225 [D loss: 0.695397, acc: 50.78%] [G loss: 0.695301]\n",
      "2230 [D loss: 0.695338, acc: 48.63%] [G loss: 0.695264]\n",
      "2235 [D loss: 0.695319, acc: 49.80%] [G loss: 0.695285]\n",
      "2240 [D loss: 0.695379, acc: 47.85%] [G loss: 0.695290]\n",
      "2245 [D loss: 0.695339, acc: 47.66%] [G loss: 0.695197]\n",
      "2250 [D loss: 0.695332, acc: 49.02%] [G loss: 0.695148]\n",
      "Epoch: 2250, auc: 0.53138, ks: 0.049990973379315906\n",
      "2255 [D loss: 0.695196, acc: 47.66%] [G loss: 0.695164]\n",
      "2260 [D loss: 0.695276, acc: 46.48%] [G loss: 0.695254]\n",
      "2265 [D loss: 0.695302, acc: 47.85%] [G loss: 0.695310]\n",
      "2270 [D loss: 0.695325, acc: 49.02%] [G loss: 0.695200]\n",
      "2275 [D loss: 0.695318, acc: 46.48%] [G loss: 0.695268]\n",
      "2280 [D loss: 0.695272, acc: 47.27%] [G loss: 0.695213]\n",
      "2285 [D loss: 0.695370, acc: 47.27%] [G loss: 0.695235]\n",
      "2290 [D loss: 0.695359, acc: 51.37%] [G loss: 0.695194]\n",
      "2295 [D loss: 0.695331, acc: 48.24%] [G loss: 0.695137]\n",
      "2300 [D loss: 0.695347, acc: 48.63%] [G loss: 0.695167]\n",
      "Epoch: 2300, auc: 0.53157, ks: 0.05058797944514093\n",
      "2305 [D loss: 0.695263, acc: 48.83%] [G loss: 0.695270]\n",
      "2310 [D loss: 0.695239, acc: 47.07%] [G loss: 0.695161]\n",
      "2315 [D loss: 0.695437, acc: 47.85%] [G loss: 0.695127]\n",
      "2320 [D loss: 0.695222, acc: 46.88%] [G loss: 0.695120]\n",
      "2325 [D loss: 0.695293, acc: 46.88%] [G loss: 0.695116]\n",
      "2330 [D loss: 0.695272, acc: 50.98%] [G loss: 0.695097]\n",
      "2335 [D loss: 0.695291, acc: 48.63%] [G loss: 0.695158]\n",
      "2340 [D loss: 0.695292, acc: 47.66%] [G loss: 0.695271]\n",
      "2345 [D loss: 0.695278, acc: 46.09%] [G loss: 0.695158]\n",
      "2350 [D loss: 0.695223, acc: 47.46%] [G loss: 0.695177]\n",
      "Epoch: 2350, auc: 0.53157, ks: 0.0503017538974877\n",
      "2355 [D loss: 0.695325, acc: 51.95%] [G loss: 0.695142]\n",
      "2360 [D loss: 0.695148, acc: 50.00%] [G loss: 0.695123]\n",
      "2365 [D loss: 0.695236, acc: 49.22%] [G loss: 0.695153]\n",
      "2370 [D loss: 0.695196, acc: 50.78%] [G loss: 0.695087]\n",
      "2375 [D loss: 0.695217, acc: 48.63%] [G loss: 0.695102]\n",
      "2380 [D loss: 0.695260, acc: 49.61%] [G loss: 0.695192]\n",
      "2385 [D loss: 0.695267, acc: 50.59%] [G loss: 0.695099]\n",
      "2390 [D loss: 0.695196, acc: 49.41%] [G loss: 0.695121]\n",
      "2395 [D loss: 0.695253, acc: 51.37%] [G loss: 0.695135]\n",
      "2400 [D loss: 0.695204, acc: 48.24%] [G loss: 0.695065]\n",
      "Epoch: 2400, auc: 0.53158, ks: 0.04988401335121362\n",
      "2405 [D loss: 0.695228, acc: 47.46%] [G loss: 0.695152]\n",
      "2410 [D loss: 0.695155, acc: 50.20%] [G loss: 0.695074]\n",
      "2415 [D loss: 0.695158, acc: 47.66%] [G loss: 0.695116]\n",
      "2420 [D loss: 0.695227, acc: 49.02%] [G loss: 0.695138]\n",
      "2425 [D loss: 0.695119, acc: 49.41%] [G loss: 0.695008]\n",
      "2430 [D loss: 0.695128, acc: 53.32%] [G loss: 0.695044]\n",
      "2435 [D loss: 0.695140, acc: 48.24%] [G loss: 0.695123]\n",
      "2440 [D loss: 0.695114, acc: 48.63%] [G loss: 0.695106]\n",
      "2445 [D loss: 0.695200, acc: 49.80%] [G loss: 0.695075]\n",
      "2450 [D loss: 0.695176, acc: 47.46%] [G loss: 0.695059]\n",
      "Epoch: 2450, auc: 0.53153, ks: 0.049144175648180966\n",
      "2455 [D loss: 0.695145, acc: 51.56%] [G loss: 0.695026]\n",
      "2460 [D loss: 0.695215, acc: 49.41%] [G loss: 0.695034]\n",
      "2465 [D loss: 0.695205, acc: 50.00%] [G loss: 0.695023]\n",
      "2470 [D loss: 0.695101, acc: 47.66%] [G loss: 0.695092]\n",
      "2475 [D loss: 0.695137, acc: 49.22%] [G loss: 0.695086]\n",
      "2480 [D loss: 0.695222, acc: 49.80%] [G loss: 0.695022]\n",
      "2485 [D loss: 0.695105, acc: 48.44%] [G loss: 0.695041]\n",
      "2490 [D loss: 0.695067, acc: 49.61%] [G loss: 0.694991]\n",
      "2495 [D loss: 0.695135, acc: 46.88%] [G loss: 0.695013]\n",
      "2500 [D loss: 0.695094, acc: 45.51%] [G loss: 0.695058]\n",
      "Epoch: 2500, auc: 0.53160, ks: 0.049287804432257665\n",
      "2505 [D loss: 0.695068, acc: 47.27%] [G loss: 0.695047]\n",
      "2510 [D loss: 0.695139, acc: 48.83%] [G loss: 0.695005]\n",
      "2515 [D loss: 0.695066, acc: 48.44%] [G loss: 0.695028]\n",
      "2520 [D loss: 0.695061, acc: 48.83%] [G loss: 0.695065]\n",
      "2525 [D loss: 0.695094, acc: 47.46%] [G loss: 0.694994]\n",
      "2530 [D loss: 0.695047, acc: 48.63%] [G loss: 0.695001]\n",
      "2535 [D loss: 0.695084, acc: 47.27%] [G loss: 0.694997]\n",
      "2540 [D loss: 0.695061, acc: 47.46%] [G loss: 0.695070]\n",
      "2545 [D loss: 0.695103, acc: 48.24%] [G loss: 0.695036]\n",
      "2550 [D loss: 0.695091, acc: 48.63%] [G loss: 0.694966]\n",
      "Epoch: 2550, auc: 0.53131, ks: 0.04867547464135247\n",
      "2555 [D loss: 0.695029, acc: 50.20%] [G loss: 0.695029]\n",
      "2560 [D loss: 0.695085, acc: 48.83%] [G loss: 0.694970]\n",
      "2565 [D loss: 0.695064, acc: 49.41%] [G loss: 0.695016]\n",
      "2570 [D loss: 0.695008, acc: 50.59%] [G loss: 0.694960]\n",
      "2575 [D loss: 0.695111, acc: 47.85%] [G loss: 0.694980]\n",
      "2580 [D loss: 0.695019, acc: 49.41%] [G loss: 0.694936]\n",
      "2585 [D loss: 0.694989, acc: 46.68%] [G loss: 0.694991]\n",
      "2590 [D loss: 0.695031, acc: 49.22%] [G loss: 0.694963]\n",
      "2595 [D loss: 0.695091, acc: 50.00%] [G loss: 0.694989]\n",
      "2600 [D loss: 0.695087, acc: 48.44%] [G loss: 0.695001]\n",
      "Epoch: 2600, auc: 0.53125, ks: 0.048565140973822474\n",
      "2605 [D loss: 0.695045, acc: 50.00%] [G loss: 0.695016]\n",
      "2610 [D loss: 0.695039, acc: 48.63%] [G loss: 0.694924]\n",
      "2615 [D loss: 0.695033, acc: 51.56%] [G loss: 0.695005]\n",
      "2620 [D loss: 0.695043, acc: 46.29%] [G loss: 0.694898]\n",
      "2625 [D loss: 0.695016, acc: 49.61%] [G loss: 0.694939]\n",
      "2630 [D loss: 0.694953, acc: 47.66%] [G loss: 0.695023]\n",
      "2635 [D loss: 0.695009, acc: 46.88%] [G loss: 0.694959]\n",
      "2640 [D loss: 0.695005, acc: 47.27%] [G loss: 0.694959]\n",
      "2645 [D loss: 0.695012, acc: 46.09%] [G loss: 0.694947]\n",
      "2650 [D loss: 0.695027, acc: 48.83%] [G loss: 0.694926]\n",
      "Epoch: 2650, auc: 0.53119, ks: 0.04803594221354235\n",
      "2655 [D loss: 0.695014, acc: 48.63%] [G loss: 0.694962]\n",
      "2660 [D loss: 0.694973, acc: 49.02%] [G loss: 0.694981]\n",
      "2665 [D loss: 0.694979, acc: 47.46%] [G loss: 0.694944]\n",
      "2670 [D loss: 0.694981, acc: 48.44%] [G loss: 0.694946]\n",
      "2675 [D loss: 0.694939, acc: 46.88%] [G loss: 0.694960]\n",
      "2680 [D loss: 0.694881, acc: 49.80%] [G loss: 0.694884]\n",
      "2685 [D loss: 0.694984, acc: 49.80%] [G loss: 0.694962]\n",
      "2690 [D loss: 0.695056, acc: 47.27%] [G loss: 0.694851]\n",
      "2695 [D loss: 0.694938, acc: 47.85%] [G loss: 0.694873]\n",
      "2700 [D loss: 0.695005, acc: 46.88%] [G loss: 0.694913]\n",
      "Epoch: 2700, auc: 0.53147, ks: 0.04870002961187092\n",
      "2705 [D loss: 0.695016, acc: 48.24%] [G loss: 0.694884]\n",
      "2710 [D loss: 0.694990, acc: 48.05%] [G loss: 0.694868]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2715 [D loss: 0.694974, acc: 47.27%] [G loss: 0.694932]\n",
      "2720 [D loss: 0.694947, acc: 48.05%] [G loss: 0.694909]\n",
      "2725 [D loss: 0.695014, acc: 46.09%] [G loss: 0.694923]\n",
      "2730 [D loss: 0.694939, acc: 50.78%] [G loss: 0.694878]\n",
      "2735 [D loss: 0.694934, acc: 48.83%] [G loss: 0.694840]\n",
      "2740 [D loss: 0.694920, acc: 45.31%] [G loss: 0.694924]\n",
      "2745 [D loss: 0.694990, acc: 47.07%] [G loss: 0.694886]\n",
      "2750 [D loss: 0.694934, acc: 47.66%] [G loss: 0.694878]\n",
      "Epoch: 2750, auc: 0.53125, ks: 0.049059194098038494\n",
      "2755 [D loss: 0.694906, acc: 48.83%] [G loss: 0.694869]\n",
      "2760 [D loss: 0.694877, acc: 48.05%] [G loss: 0.694896]\n",
      "2765 [D loss: 0.694959, acc: 48.44%] [G loss: 0.694858]\n",
      "2770 [D loss: 0.694939, acc: 48.05%] [G loss: 0.694855]\n",
      "2775 [D loss: 0.694967, acc: 47.66%] [G loss: 0.694880]\n",
      "2780 [D loss: 0.694959, acc: 48.44%] [G loss: 0.694855]\n",
      "2785 [D loss: 0.694875, acc: 49.02%] [G loss: 0.694848]\n",
      "2790 [D loss: 0.694965, acc: 45.51%] [G loss: 0.694858]\n",
      "2795 [D loss: 0.694874, acc: 47.46%] [G loss: 0.694845]\n",
      "2800 [D loss: 0.694900, acc: 46.68%] [G loss: 0.694849]\n",
      "Epoch: 2800, auc: 0.53145, ks: 0.04806523024359566\n",
      "2805 [D loss: 0.694838, acc: 47.66%] [G loss: 0.694856]\n",
      "2810 [D loss: 0.694900, acc: 49.22%] [G loss: 0.694889]\n",
      "2815 [D loss: 0.694883, acc: 49.22%] [G loss: 0.694841]\n",
      "2820 [D loss: 0.694883, acc: 47.66%] [G loss: 0.694822]\n",
      "2825 [D loss: 0.694892, acc: 48.44%] [G loss: 0.694804]\n",
      "2830 [D loss: 0.694890, acc: 46.68%] [G loss: 0.694797]\n",
      "2835 [D loss: 0.694855, acc: 47.27%] [G loss: 0.694883]\n",
      "2840 [D loss: 0.694868, acc: 46.68%] [G loss: 0.694926]\n",
      "2845 [D loss: 0.694883, acc: 47.85%] [G loss: 0.694828]\n",
      "2850 [D loss: 0.694888, acc: 46.68%] [G loss: 0.694829]\n",
      "Epoch: 2850, auc: 0.53125, ks: 0.04823582679094607\n",
      "2855 [D loss: 0.694914, acc: 48.63%] [G loss: 0.694816]\n",
      "2860 [D loss: 0.694884, acc: 46.88%] [G loss: 0.694782]\n",
      "2865 [D loss: 0.694880, acc: 47.46%] [G loss: 0.694803]\n",
      "2870 [D loss: 0.694848, acc: 47.85%] [G loss: 0.694789]\n",
      "2875 [D loss: 0.694833, acc: 46.29%] [G loss: 0.694815]\n",
      "2880 [D loss: 0.694844, acc: 50.59%] [G loss: 0.694808]\n",
      "2885 [D loss: 0.694851, acc: 47.66%] [G loss: 0.694838]\n",
      "2890 [D loss: 0.694890, acc: 48.63%] [G loss: 0.694828]\n",
      "2895 [D loss: 0.694882, acc: 47.46%] [G loss: 0.694829]\n",
      "2900 [D loss: 0.694845, acc: 47.07%] [G loss: 0.694828]\n",
      "Epoch: 2900, auc: 0.53124, ks: 0.04727548545265681\n",
      "2905 [D loss: 0.694839, acc: 49.22%] [G loss: 0.694797]\n",
      "2910 [D loss: 0.694854, acc: 47.46%] [G loss: 0.694810]\n",
      "2915 [D loss: 0.694841, acc: 48.63%] [G loss: 0.694823]\n",
      "2920 [D loss: 0.694854, acc: 50.20%] [G loss: 0.694811]\n",
      "2925 [D loss: 0.694846, acc: 48.44%] [G loss: 0.694850]\n",
      "2930 [D loss: 0.694849, acc: 47.07%] [G loss: 0.694784]\n",
      "2935 [D loss: 0.694895, acc: 47.66%] [G loss: 0.694788]\n",
      "2940 [D loss: 0.694806, acc: 47.85%] [G loss: 0.694766]\n",
      "2945 [D loss: 0.694906, acc: 45.70%] [G loss: 0.694775]\n",
      "2950 [D loss: 0.694812, acc: 48.83%] [G loss: 0.694790]\n",
      "Epoch: 2950, auc: 0.53117, ks: 0.04830347039668759\n",
      "2955 [D loss: 0.694863, acc: 48.44%] [G loss: 0.694774]\n",
      "2960 [D loss: 0.694858, acc: 49.41%] [G loss: 0.694785]\n",
      "2965 [D loss: 0.694838, acc: 48.83%] [G loss: 0.694797]\n",
      "2970 [D loss: 0.694844, acc: 48.44%] [G loss: 0.694773]\n",
      "2975 [D loss: 0.694840, acc: 47.66%] [G loss: 0.694774]\n",
      "2980 [D loss: 0.694855, acc: 46.68%] [G loss: 0.694800]\n",
      "2985 [D loss: 0.694857, acc: 50.78%] [G loss: 0.694794]\n",
      "2990 [D loss: 0.694803, acc: 46.68%] [G loss: 0.694764]\n",
      "2995 [D loss: 0.694781, acc: 47.46%] [G loss: 0.694731]\n",
      "3000 [D loss: 0.694832, acc: 45.70%] [G loss: 0.694760]\n",
      "Epoch: 3000, auc: 0.53109, ks: 0.04714645442061893\n",
      "3005 [D loss: 0.694852, acc: 47.46%] [G loss: 0.694760]\n",
      "3010 [D loss: 0.694845, acc: 48.63%] [G loss: 0.694787]\n",
      "3015 [D loss: 0.694827, acc: 48.63%] [G loss: 0.694728]\n",
      "3020 [D loss: 0.694788, acc: 47.07%] [G loss: 0.694772]\n",
      "3025 [D loss: 0.694787, acc: 47.27%] [G loss: 0.694734]\n",
      "3030 [D loss: 0.694802, acc: 46.88%] [G loss: 0.694702]\n",
      "3035 [D loss: 0.694808, acc: 48.83%] [G loss: 0.694803]\n",
      "3040 [D loss: 0.694754, acc: 46.88%] [G loss: 0.694739]\n",
      "3045 [D loss: 0.694782, acc: 46.48%] [G loss: 0.694737]\n",
      "3050 [D loss: 0.694845, acc: 49.02%] [G loss: 0.694750]\n",
      "Epoch: 3050, auc: 0.53114, ks: 0.04665610233543738\n",
      "3055 [D loss: 0.694802, acc: 47.46%] [G loss: 0.694768]\n",
      "3060 [D loss: 0.694788, acc: 48.83%] [G loss: 0.694721]\n",
      "3065 [D loss: 0.694754, acc: 48.24%] [G loss: 0.694706]\n",
      "3070 [D loss: 0.694833, acc: 48.05%] [G loss: 0.694756]\n",
      "3075 [D loss: 0.694825, acc: 47.46%] [G loss: 0.694727]\n",
      "3080 [D loss: 0.694776, acc: 48.05%] [G loss: 0.694768]\n",
      "3085 [D loss: 0.694822, acc: 49.61%] [G loss: 0.694770]\n",
      "3090 [D loss: 0.694786, acc: 47.27%] [G loss: 0.694683]\n",
      "3095 [D loss: 0.694786, acc: 47.46%] [G loss: 0.694708]\n",
      "3100 [D loss: 0.694813, acc: 49.22%] [G loss: 0.694732]\n",
      "Epoch: 3100, auc: 0.53119, ks: 0.04728014733836394\n",
      "3105 [D loss: 0.694784, acc: 49.61%] [G loss: 0.694761]\n",
      "3110 [D loss: 0.694754, acc: 48.05%] [G loss: 0.694703]\n",
      "3115 [D loss: 0.694807, acc: 47.46%] [G loss: 0.694693]\n",
      "3120 [D loss: 0.694757, acc: 47.66%] [G loss: 0.694708]\n",
      "3125 [D loss: 0.694741, acc: 47.66%] [G loss: 0.694727]\n",
      "3130 [D loss: 0.694781, acc: 48.83%] [G loss: 0.694722]\n",
      "3135 [D loss: 0.694779, acc: 46.68%] [G loss: 0.694718]\n",
      "3140 [D loss: 0.694757, acc: 47.27%] [G loss: 0.694722]\n",
      "3145 [D loss: 0.694752, acc: 45.31%] [G loss: 0.694735]\n",
      "3150 [D loss: 0.694770, acc: 46.68%] [G loss: 0.694714]\n",
      "Epoch: 3150, auc: 0.53083, ks: 0.04680751041886966\n",
      "3155 [D loss: 0.694733, acc: 46.68%] [G loss: 0.694730]\n",
      "3160 [D loss: 0.694725, acc: 49.02%] [G loss: 0.694710]\n",
      "3165 [D loss: 0.694746, acc: 50.39%] [G loss: 0.694754]\n",
      "3170 [D loss: 0.694734, acc: 48.44%] [G loss: 0.694715]\n",
      "3175 [D loss: 0.694763, acc: 49.22%] [G loss: 0.694748]\n",
      "3180 [D loss: 0.694768, acc: 49.02%] [G loss: 0.694709]\n",
      "3185 [D loss: 0.694756, acc: 46.09%] [G loss: 0.694676]\n",
      "3190 [D loss: 0.694771, acc: 48.44%] [G loss: 0.694713]\n",
      "3195 [D loss: 0.694713, acc: 44.92%] [G loss: 0.694707]\n",
      "3200 [D loss: 0.694722, acc: 47.66%] [G loss: 0.694745]\n",
      "Epoch: 3200, auc: 0.53136, ks: 0.04652690760359601\n",
      "3205 [D loss: 0.694767, acc: 46.68%] [G loss: 0.694699]\n",
      "3210 [D loss: 0.694752, acc: 46.48%] [G loss: 0.694700]\n",
      "3215 [D loss: 0.694704, acc: 47.27%] [G loss: 0.694683]\n",
      "3220 [D loss: 0.694774, acc: 49.22%] [G loss: 0.694694]\n",
      "3225 [D loss: 0.694686, acc: 49.41%] [G loss: 0.694671]\n",
      "3230 [D loss: 0.694787, acc: 49.61%] [G loss: 0.694662]\n",
      "3235 [D loss: 0.694755, acc: 49.22%] [G loss: 0.694694]\n",
      "3240 [D loss: 0.694707, acc: 49.41%] [G loss: 0.694709]\n",
      "3245 [D loss: 0.694737, acc: 47.85%] [G loss: 0.694679]\n",
      "3250 [D loss: 0.694760, acc: 47.85%] [G loss: 0.694683]\n",
      "Epoch: 3250, auc: 0.53092, ks: 0.04688494754328765\n",
      "3255 [D loss: 0.694770, acc: 45.51%] [G loss: 0.694696]\n",
      "3260 [D loss: 0.694741, acc: 46.48%] [G loss: 0.694673]\n",
      "3265 [D loss: 0.694706, acc: 48.05%] [G loss: 0.694693]\n",
      "3270 [D loss: 0.694715, acc: 45.90%] [G loss: 0.694662]\n",
      "3275 [D loss: 0.694727, acc: 46.48%] [G loss: 0.694676]\n",
      "3280 [D loss: 0.694714, acc: 48.63%] [G loss: 0.694689]\n",
      "3285 [D loss: 0.694697, acc: 48.63%] [G loss: 0.694672]\n",
      "3290 [D loss: 0.694700, acc: 47.46%] [G loss: 0.694668]\n",
      "3295 [D loss: 0.694715, acc: 48.83%] [G loss: 0.694647]\n",
      "3300 [D loss: 0.694723, acc: 48.44%] [G loss: 0.694670]\n",
      "Epoch: 3300, auc: 0.53113, ks: 0.047051465830317274\n",
      "3305 [D loss: 0.694712, acc: 48.44%] [G loss: 0.694666]\n",
      "3310 [D loss: 0.694689, acc: 49.02%] [G loss: 0.694678]\n",
      "3315 [D loss: 0.694731, acc: 49.22%] [G loss: 0.694674]\n",
      "3320 [D loss: 0.694705, acc: 46.09%] [G loss: 0.694675]\n",
      "3325 [D loss: 0.694663, acc: 50.59%] [G loss: 0.694622]\n",
      "3330 [D loss: 0.694698, acc: 48.05%] [G loss: 0.694676]\n",
      "3335 [D loss: 0.694686, acc: 48.44%] [G loss: 0.694680]\n",
      "3340 [D loss: 0.694694, acc: 46.48%] [G loss: 0.694623]\n",
      "3345 [D loss: 0.694690, acc: 48.05%] [G loss: 0.694652]\n",
      "3350 [D loss: 0.694677, acc: 48.63%] [G loss: 0.694676]\n",
      "Epoch: 3350, auc: 0.53132, ks: 0.04799454039803319\n",
      "3355 [D loss: 0.694675, acc: 46.09%] [G loss: 0.694641]\n",
      "3360 [D loss: 0.694715, acc: 46.88%] [G loss: 0.694646]\n",
      "3365 [D loss: 0.694695, acc: 45.70%] [G loss: 0.694638]\n",
      "3370 [D loss: 0.694673, acc: 46.88%] [G loss: 0.694666]\n",
      "3375 [D loss: 0.694663, acc: 45.90%] [G loss: 0.694622]\n",
      "3380 [D loss: 0.694665, acc: 47.66%] [G loss: 0.694662]\n",
      "3385 [D loss: 0.694716, acc: 47.66%] [G loss: 0.694643]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3390 [D loss: 0.694666, acc: 46.68%] [G loss: 0.694657]\n",
      "3395 [D loss: 0.694666, acc: 49.22%] [G loss: 0.694671]\n",
      "3400 [D loss: 0.694665, acc: 45.90%] [G loss: 0.694658]\n",
      "Epoch: 3400, auc: 0.53119, ks: 0.04711325183004822\n",
      "3405 [D loss: 0.694671, acc: 45.31%] [G loss: 0.694623]\n",
      "3410 [D loss: 0.694646, acc: 46.29%] [G loss: 0.694628]\n",
      "3415 [D loss: 0.694640, acc: 48.05%] [G loss: 0.694654]\n",
      "3420 [D loss: 0.694680, acc: 45.90%] [G loss: 0.694638]\n",
      "3425 [D loss: 0.694666, acc: 47.85%] [G loss: 0.694635]\n",
      "3430 [D loss: 0.694636, acc: 45.51%] [G loss: 0.694640]\n",
      "3435 [D loss: 0.694645, acc: 48.05%] [G loss: 0.694653]\n",
      "3440 [D loss: 0.694644, acc: 47.85%] [G loss: 0.694640]\n",
      "3445 [D loss: 0.694662, acc: 48.24%] [G loss: 0.694628]\n",
      "3450 [D loss: 0.694651, acc: 46.88%] [G loss: 0.694618]\n",
      "Epoch: 3450, auc: 0.53118, ks: 0.04642421088776649\n",
      "3455 [D loss: 0.694660, acc: 46.29%] [G loss: 0.694599]\n",
      "3460 [D loss: 0.694623, acc: 46.88%] [G loss: 0.694637]\n",
      "3465 [D loss: 0.694670, acc: 46.29%] [G loss: 0.694628]\n",
      "3470 [D loss: 0.694651, acc: 47.46%] [G loss: 0.694638]\n",
      "3475 [D loss: 0.694668, acc: 47.46%] [G loss: 0.694630]\n",
      "3480 [D loss: 0.694639, acc: 47.27%] [G loss: 0.694615]\n",
      "3485 [D loss: 0.694651, acc: 48.05%] [G loss: 0.694612]\n",
      "3490 [D loss: 0.694664, acc: 47.27%] [G loss: 0.694629]\n",
      "3495 [D loss: 0.694649, acc: 47.46%] [G loss: 0.694603]\n",
      "3500 [D loss: 0.694635, acc: 48.24%] [G loss: 0.694619]\n",
      "Epoch: 3500, auc: 0.53110, ks: 0.04695926725405708\n",
      "3505 [D loss: 0.694635, acc: 46.48%] [G loss: 0.694632]\n",
      "3510 [D loss: 0.694642, acc: 46.29%] [G loss: 0.694620]\n",
      "3515 [D loss: 0.694642, acc: 46.48%] [G loss: 0.694608]\n",
      "3520 [D loss: 0.694625, acc: 46.68%] [G loss: 0.694629]\n",
      "3525 [D loss: 0.694645, acc: 46.88%] [G loss: 0.694610]\n",
      "3530 [D loss: 0.694642, acc: 49.61%] [G loss: 0.694622]\n",
      "3535 [D loss: 0.694611, acc: 47.46%] [G loss: 0.694628]\n",
      "3540 [D loss: 0.694639, acc: 48.63%] [G loss: 0.694606]\n",
      "3545 [D loss: 0.694646, acc: 46.48%] [G loss: 0.694599]\n",
      "3550 [D loss: 0.694633, acc: 48.44%] [G loss: 0.694618]\n",
      "Epoch: 3550, auc: 0.53116, ks: 0.04660483582742447\n",
      "3555 [D loss: 0.694625, acc: 43.75%] [G loss: 0.694631]\n",
      "3560 [D loss: 0.694656, acc: 49.61%] [G loss: 0.694598]\n",
      "3565 [D loss: 0.694636, acc: 49.02%] [G loss: 0.694593]\n",
      "3570 [D loss: 0.694640, acc: 46.88%] [G loss: 0.694607]\n",
      "3575 [D loss: 0.694618, acc: 46.88%] [G loss: 0.694586]\n",
      "3580 [D loss: 0.694634, acc: 47.27%] [G loss: 0.694582]\n",
      "3585 [D loss: 0.694634, acc: 47.66%] [G loss: 0.694600]\n",
      "3590 [D loss: 0.694650, acc: 46.88%] [G loss: 0.694599]\n",
      "3595 [D loss: 0.694616, acc: 45.90%] [G loss: 0.694602]\n",
      "3600 [D loss: 0.694611, acc: 50.78%] [G loss: 0.694600]\n",
      "Epoch: 3600, auc: 0.53117, ks: 0.046926769284379555\n",
      "3605 [D loss: 0.694619, acc: 45.70%] [G loss: 0.694585]\n",
      "3610 [D loss: 0.694623, acc: 47.85%] [G loss: 0.694600]\n",
      "3615 [D loss: 0.694604, acc: 46.68%] [G loss: 0.694568]\n",
      "3620 [D loss: 0.694590, acc: 46.68%] [G loss: 0.694591]\n",
      "3625 [D loss: 0.694620, acc: 47.27%] [G loss: 0.694586]\n",
      "3630 [D loss: 0.694626, acc: 48.63%] [G loss: 0.694618]\n",
      "3635 [D loss: 0.694615, acc: 46.68%] [G loss: 0.694591]\n",
      "3640 [D loss: 0.694613, acc: 46.09%] [G loss: 0.694607]\n",
      "3645 [D loss: 0.694600, acc: 46.88%] [G loss: 0.694581]\n",
      "3650 [D loss: 0.694618, acc: 44.53%] [G loss: 0.694596]\n",
      "Epoch: 3650, auc: 0.53130, ks: 0.04640750639043112\n",
      "3655 [D loss: 0.694624, acc: 46.88%] [G loss: 0.694570]\n",
      "3660 [D loss: 0.694633, acc: 47.27%] [G loss: 0.694593]\n",
      "3665 [D loss: 0.694592, acc: 46.09%] [G loss: 0.694588]\n",
      "3670 [D loss: 0.694632, acc: 46.88%] [G loss: 0.694579]\n",
      "3675 [D loss: 0.694598, acc: 48.83%] [G loss: 0.694568]\n",
      "3680 [D loss: 0.694612, acc: 46.88%] [G loss: 0.694591]\n",
      "3685 [D loss: 0.694635, acc: 46.09%] [G loss: 0.694570]\n",
      "3690 [D loss: 0.694608, acc: 46.29%] [G loss: 0.694579]\n",
      "3695 [D loss: 0.694598, acc: 47.07%] [G loss: 0.694571]\n",
      "3700 [D loss: 0.694606, acc: 45.12%] [G loss: 0.694590]\n",
      "Epoch: 3700, auc: 0.53133, ks: 0.04677864231439921\n",
      "3705 [D loss: 0.694597, acc: 45.90%] [G loss: 0.694577]\n",
      "3710 [D loss: 0.694607, acc: 47.07%] [G loss: 0.694579]\n",
      "3715 [D loss: 0.694605, acc: 49.41%] [G loss: 0.694566]\n",
      "3720 [D loss: 0.694584, acc: 50.00%] [G loss: 0.694568]\n",
      "3725 [D loss: 0.694581, acc: 48.24%] [G loss: 0.694564]\n",
      "3730 [D loss: 0.694594, acc: 44.92%] [G loss: 0.694577]\n",
      "3735 [D loss: 0.694617, acc: 48.44%] [G loss: 0.694585]\n",
      "3740 [D loss: 0.694605, acc: 45.70%] [G loss: 0.694558]\n",
      "3745 [D loss: 0.694591, acc: 48.24%] [G loss: 0.694578]\n",
      "3750 [D loss: 0.694571, acc: 48.24%] [G loss: 0.694602]\n",
      "Epoch: 3750, auc: 0.53148, ks: 0.04656880051851564\n",
      "3755 [D loss: 0.694584, acc: 46.09%] [G loss: 0.694566]\n",
      "3760 [D loss: 0.694598, acc: 49.02%] [G loss: 0.694572]\n",
      "3765 [D loss: 0.694585, acc: 45.90%] [G loss: 0.694568]\n",
      "3770 [D loss: 0.694592, acc: 45.31%] [G loss: 0.694562]\n",
      "3775 [D loss: 0.694590, acc: 46.88%] [G loss: 0.694572]\n",
      "3780 [D loss: 0.694600, acc: 47.85%] [G loss: 0.694583]\n",
      "3785 [D loss: 0.694591, acc: 47.07%] [G loss: 0.694555]\n",
      "3790 [D loss: 0.694588, acc: 47.66%] [G loss: 0.694577]\n",
      "3795 [D loss: 0.694600, acc: 45.51%] [G loss: 0.694566]\n",
      "3800 [D loss: 0.694570, acc: 49.22%] [G loss: 0.694568]\n",
      "Epoch: 3800, auc: 0.53128, ks: 0.047479398468702194\n",
      "3805 [D loss: 0.694573, acc: 49.02%] [G loss: 0.694577]\n",
      "3810 [D loss: 0.694565, acc: 47.46%] [G loss: 0.694573]\n",
      "3815 [D loss: 0.694593, acc: 47.46%] [G loss: 0.694555]\n",
      "3820 [D loss: 0.694567, acc: 47.66%] [G loss: 0.694570]\n",
      "3825 [D loss: 0.694584, acc: 45.31%] [G loss: 0.694568]\n",
      "3830 [D loss: 0.694591, acc: 48.24%] [G loss: 0.694570]\n",
      "3835 [D loss: 0.694579, acc: 47.07%] [G loss: 0.694554]\n",
      "3840 [D loss: 0.694579, acc: 46.68%] [G loss: 0.694545]\n",
      "3845 [D loss: 0.694580, acc: 47.66%] [G loss: 0.694555]\n",
      "3850 [D loss: 0.694573, acc: 47.07%] [G loss: 0.694540]\n",
      "Epoch: 3850, auc: 0.53114, ks: 0.04759085668270824\n",
      "3855 [D loss: 0.694575, acc: 45.70%] [G loss: 0.694534]\n",
      "3860 [D loss: 0.694590, acc: 47.07%] [G loss: 0.694551]\n",
      "3865 [D loss: 0.694573, acc: 44.34%] [G loss: 0.694567]\n",
      "3870 [D loss: 0.694556, acc: 47.46%] [G loss: 0.694559]\n",
      "3875 [D loss: 0.694574, acc: 48.24%] [G loss: 0.694556]\n",
      "3880 [D loss: 0.694564, acc: 45.90%] [G loss: 0.694558]\n",
      "3885 [D loss: 0.694575, acc: 48.63%] [G loss: 0.694543]\n",
      "3890 [D loss: 0.694571, acc: 45.31%] [G loss: 0.694552]\n",
      "3895 [D loss: 0.694575, acc: 47.85%] [G loss: 0.694548]\n",
      "3900 [D loss: 0.694564, acc: 46.09%] [G loss: 0.694551]\n",
      "Epoch: 3900, auc: 0.53124, ks: 0.04762391692562373\n",
      "3905 [D loss: 0.694565, acc: 45.51%] [G loss: 0.694557]\n",
      "3910 [D loss: 0.694572, acc: 44.73%] [G loss: 0.694530]\n",
      "3915 [D loss: 0.694547, acc: 47.66%] [G loss: 0.694551]\n",
      "3920 [D loss: 0.694550, acc: 47.07%] [G loss: 0.694523]\n",
      "3925 [D loss: 0.694557, acc: 49.61%] [G loss: 0.694538]\n",
      "3930 [D loss: 0.694546, acc: 48.63%] [G loss: 0.694543]\n",
      "3935 [D loss: 0.694562, acc: 45.12%] [G loss: 0.694534]\n",
      "3940 [D loss: 0.694575, acc: 47.46%] [G loss: 0.694528]\n",
      "3945 [D loss: 0.694555, acc: 46.29%] [G loss: 0.694543]\n",
      "3950 [D loss: 0.694543, acc: 47.07%] [G loss: 0.694543]\n",
      "Epoch: 3950, auc: 0.53126, ks: 0.0472478130684898\n",
      "3955 [D loss: 0.694567, acc: 45.51%] [G loss: 0.694541]\n",
      "3960 [D loss: 0.694551, acc: 47.46%] [G loss: 0.694544]\n",
      "3965 [D loss: 0.694551, acc: 45.70%] [G loss: 0.694531]\n",
      "3970 [D loss: 0.694552, acc: 46.09%] [G loss: 0.694525]\n",
      "3975 [D loss: 0.694565, acc: 47.46%] [G loss: 0.694524]\n",
      "3980 [D loss: 0.694574, acc: 49.02%] [G loss: 0.694532]\n",
      "3985 [D loss: 0.694546, acc: 48.44%] [G loss: 0.694520]\n",
      "3990 [D loss: 0.694557, acc: 45.90%] [G loss: 0.694531]\n",
      "3995 [D loss: 0.694559, acc: 45.70%] [G loss: 0.694528]\n",
      "4000 [D loss: 0.694543, acc: 47.46%] [G loss: 0.694530]\n",
      "Epoch: 4000, auc: 0.53113, ks: 0.04778945301383264\n",
      "4005 [D loss: 0.694569, acc: 47.27%] [G loss: 0.694537]\n",
      "4010 [D loss: 0.694554, acc: 45.51%] [G loss: 0.694550]\n",
      "4015 [D loss: 0.694557, acc: 47.27%] [G loss: 0.694530]\n",
      "4020 [D loss: 0.694538, acc: 45.51%] [G loss: 0.694534]\n",
      "4025 [D loss: 0.694558, acc: 47.07%] [G loss: 0.694528]\n",
      "4030 [D loss: 0.694545, acc: 46.29%] [G loss: 0.694524]\n",
      "4035 [D loss: 0.694556, acc: 44.73%] [G loss: 0.694520]\n",
      "4040 [D loss: 0.694526, acc: 46.09%] [G loss: 0.694523]\n",
      "4045 [D loss: 0.694541, acc: 46.29%] [G loss: 0.694534]\n",
      "4050 [D loss: 0.694532, acc: 45.90%] [G loss: 0.694528]\n",
      "Epoch: 4050, auc: 0.53079, ks: 0.047927551591505524\n",
      "4055 [D loss: 0.694546, acc: 49.22%] [G loss: 0.694517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4060 [D loss: 0.694558, acc: 47.27%] [G loss: 0.694523]\n",
      "4065 [D loss: 0.694551, acc: 49.02%] [G loss: 0.694529]\n",
      "4070 [D loss: 0.694539, acc: 48.05%] [G loss: 0.694535]\n",
      "4075 [D loss: 0.694547, acc: 45.90%] [G loss: 0.694522]\n",
      "4080 [D loss: 0.694535, acc: 46.88%] [G loss: 0.694525]\n",
      "4085 [D loss: 0.694540, acc: 49.02%] [G loss: 0.694525]\n",
      "4090 [D loss: 0.694531, acc: 47.85%] [G loss: 0.694516]\n",
      "4095 [D loss: 0.694547, acc: 47.85%] [G loss: 0.694515]\n",
      "4100 [D loss: 0.694540, acc: 44.14%] [G loss: 0.694536]\n",
      "Epoch: 4100, auc: 0.53082, ks: 0.047620941859630506\n",
      "4105 [D loss: 0.694524, acc: 45.31%] [G loss: 0.694516]\n",
      "4110 [D loss: 0.694532, acc: 44.53%] [G loss: 0.694525]\n",
      "4115 [D loss: 0.694543, acc: 46.09%] [G loss: 0.694517]\n",
      "4120 [D loss: 0.694535, acc: 46.48%] [G loss: 0.694516]\n",
      "4125 [D loss: 0.694512, acc: 47.46%] [G loss: 0.694516]\n",
      "4130 [D loss: 0.694536, acc: 46.88%] [G loss: 0.694516]\n",
      "4135 [D loss: 0.694533, acc: 45.12%] [G loss: 0.694525]\n",
      "4140 [D loss: 0.694536, acc: 47.85%] [G loss: 0.694520]\n",
      "4145 [D loss: 0.694522, acc: 46.88%] [G loss: 0.694519]\n",
      "4150 [D loss: 0.694525, acc: 43.75%] [G loss: 0.694523]\n",
      "Epoch: 4150, auc: 0.53096, ks: 0.04761412340694737\n",
      "4155 [D loss: 0.694517, acc: 43.75%] [G loss: 0.694508]\n",
      "4160 [D loss: 0.694530, acc: 46.68%] [G loss: 0.694514]\n",
      "4165 [D loss: 0.694520, acc: 46.68%] [G loss: 0.694517]\n",
      "4170 [D loss: 0.694523, acc: 44.14%] [G loss: 0.694510]\n",
      "4175 [D loss: 0.694528, acc: 45.31%] [G loss: 0.694511]\n",
      "4180 [D loss: 0.694529, acc: 46.09%] [G loss: 0.694501]\n",
      "4185 [D loss: 0.694527, acc: 45.90%] [G loss: 0.694505]\n",
      "4190 [D loss: 0.694520, acc: 45.12%] [G loss: 0.694515]\n",
      "4195 [D loss: 0.694533, acc: 47.85%] [G loss: 0.694496]\n",
      "4200 [D loss: 0.694525, acc: 47.46%] [G loss: 0.694511]\n",
      "Epoch: 4200, auc: 0.53077, ks: 0.047174475556541084\n",
      "4205 [D loss: 0.694519, acc: 47.85%] [G loss: 0.694508]\n",
      "4210 [D loss: 0.694533, acc: 48.44%] [G loss: 0.694505]\n",
      "4215 [D loss: 0.694521, acc: 45.70%] [G loss: 0.694518]\n",
      "4220 [D loss: 0.694517, acc: 47.85%] [G loss: 0.694507]\n",
      "4225 [D loss: 0.694533, acc: 47.27%] [G loss: 0.694514]\n",
      "4230 [D loss: 0.694518, acc: 46.09%] [G loss: 0.694506]\n",
      "4235 [D loss: 0.694515, acc: 45.12%] [G loss: 0.694493]\n",
      "4240 [D loss: 0.694510, acc: 49.22%] [G loss: 0.694503]\n",
      "4245 [D loss: 0.694510, acc: 45.12%] [G loss: 0.694509]\n",
      "4250 [D loss: 0.694529, acc: 45.90%] [G loss: 0.694498]\n",
      "Epoch: 4250, auc: 0.53075, ks: 0.048141236774079\n",
      "4255 [D loss: 0.694516, acc: 45.70%] [G loss: 0.694497]\n",
      "4260 [D loss: 0.694510, acc: 45.12%] [G loss: 0.694504]\n",
      "4265 [D loss: 0.694500, acc: 45.90%] [G loss: 0.694499]\n",
      "4270 [D loss: 0.694514, acc: 44.34%] [G loss: 0.694494]\n",
      "4275 [D loss: 0.694519, acc: 45.31%] [G loss: 0.694498]\n",
      "4280 [D loss: 0.694525, acc: 46.48%] [G loss: 0.694495]\n",
      "4285 [D loss: 0.694510, acc: 48.24%] [G loss: 0.694492]\n",
      "4290 [D loss: 0.694508, acc: 45.31%] [G loss: 0.694500]\n",
      "4295 [D loss: 0.694502, acc: 44.73%] [G loss: 0.694492]\n",
      "4300 [D loss: 0.694499, acc: 44.14%] [G loss: 0.694506]\n",
      "Epoch: 4300, auc: 0.53053, ks: 0.04898065377929284\n",
      "4305 [D loss: 0.694507, acc: 46.68%] [G loss: 0.694492]\n",
      "4310 [D loss: 0.694514, acc: 44.73%] [G loss: 0.694489]\n",
      "4315 [D loss: 0.694505, acc: 45.51%] [G loss: 0.694498]\n",
      "4320 [D loss: 0.694507, acc: 45.70%] [G loss: 0.694499]\n",
      "4325 [D loss: 0.694510, acc: 44.92%] [G loss: 0.694491]\n",
      "4330 [D loss: 0.694509, acc: 45.90%] [G loss: 0.694489]\n",
      "4335 [D loss: 0.694507, acc: 44.73%] [G loss: 0.694493]\n",
      "4340 [D loss: 0.694507, acc: 45.51%] [G loss: 0.694495]\n",
      "4345 [D loss: 0.694502, acc: 45.51%] [G loss: 0.694488]\n",
      "4350 [D loss: 0.694503, acc: 46.88%] [G loss: 0.694484]\n",
      "Epoch: 4350, auc: 0.53071, ks: 0.04920926411351201\n",
      "4355 [D loss: 0.694505, acc: 46.29%] [G loss: 0.694485]\n",
      "4360 [D loss: 0.694504, acc: 44.73%] [G loss: 0.694490]\n",
      "4365 [D loss: 0.694500, acc: 45.90%] [G loss: 0.694501]\n",
      "4370 [D loss: 0.694500, acc: 46.88%] [G loss: 0.694494]\n",
      "4375 [D loss: 0.694496, acc: 44.53%] [G loss: 0.694477]\n",
      "4380 [D loss: 0.694511, acc: 45.90%] [G loss: 0.694482]\n",
      "4385 [D loss: 0.694506, acc: 46.68%] [G loss: 0.694488]\n",
      "4390 [D loss: 0.694489, acc: 47.07%] [G loss: 0.694494]\n",
      "4395 [D loss: 0.694504, acc: 42.97%] [G loss: 0.694490]\n",
      "4400 [D loss: 0.694501, acc: 42.38%] [G loss: 0.694482]\n",
      "Epoch: 4400, auc: 0.53100, ks: 0.04921833165914702\n",
      "4405 [D loss: 0.694495, acc: 44.73%] [G loss: 0.694479]\n",
      "4410 [D loss: 0.694511, acc: 46.48%] [G loss: 0.694498]\n",
      "4415 [D loss: 0.694510, acc: 43.55%] [G loss: 0.694493]\n",
      "4420 [D loss: 0.694513, acc: 44.92%] [G loss: 0.694476]\n",
      "4425 [D loss: 0.694510, acc: 45.31%] [G loss: 0.694484]\n",
      "4430 [D loss: 0.694491, acc: 48.05%] [G loss: 0.694484]\n",
      "4435 [D loss: 0.694497, acc: 45.31%] [G loss: 0.694482]\n",
      "4440 [D loss: 0.694493, acc: 44.34%] [G loss: 0.694478]\n",
      "4445 [D loss: 0.694495, acc: 42.97%] [G loss: 0.694481]\n",
      "4450 [D loss: 0.694489, acc: 46.09%] [G loss: 0.694479]\n",
      "Epoch: 4450, auc: 0.53094, ks: 0.049096845052833515\n",
      "4455 [D loss: 0.694497, acc: 44.73%] [G loss: 0.694474]\n",
      "4460 [D loss: 0.694498, acc: 44.73%] [G loss: 0.694479]\n",
      "4465 [D loss: 0.694497, acc: 42.97%] [G loss: 0.694483]\n",
      "4470 [D loss: 0.694490, acc: 45.70%] [G loss: 0.694481]\n",
      "4475 [D loss: 0.694490, acc: 47.07%] [G loss: 0.694489]\n",
      "4480 [D loss: 0.694496, acc: 45.12%] [G loss: 0.694478]\n",
      "4485 [D loss: 0.694501, acc: 45.12%] [G loss: 0.694480]\n",
      "4490 [D loss: 0.694498, acc: 42.19%] [G loss: 0.694479]\n",
      "4495 [D loss: 0.694487, acc: 43.36%] [G loss: 0.694474]\n",
      "4500 [D loss: 0.694489, acc: 46.29%] [G loss: 0.694480]\n",
      "Epoch: 4500, auc: 0.53082, ks: 0.04875148117183581\n",
      "4505 [D loss: 0.694490, acc: 47.07%] [G loss: 0.694477]\n",
      "4510 [D loss: 0.694486, acc: 47.46%] [G loss: 0.694481]\n",
      "4515 [D loss: 0.694481, acc: 46.68%] [G loss: 0.694480]\n",
      "4520 [D loss: 0.694492, acc: 45.31%] [G loss: 0.694475]\n",
      "4525 [D loss: 0.694482, acc: 46.48%] [G loss: 0.694479]\n",
      "4530 [D loss: 0.694486, acc: 44.34%] [G loss: 0.694479]\n",
      "4535 [D loss: 0.694482, acc: 46.48%] [G loss: 0.694476]\n",
      "4540 [D loss: 0.694492, acc: 43.95%] [G loss: 0.694476]\n",
      "4545 [D loss: 0.694489, acc: 42.58%] [G loss: 0.694476]\n",
      "4550 [D loss: 0.694484, acc: 45.31%] [G loss: 0.694477]\n",
      "Epoch: 4550, auc: 0.53102, ks: 0.04856916941246392\n",
      "4555 [D loss: 0.694483, acc: 44.92%] [G loss: 0.694476]\n",
      "4560 [D loss: 0.694483, acc: 45.12%] [G loss: 0.694472]\n",
      "4565 [D loss: 0.694475, acc: 45.12%] [G loss: 0.694480]\n",
      "4570 [D loss: 0.694476, acc: 45.31%] [G loss: 0.694470]\n",
      "4575 [D loss: 0.694485, acc: 43.95%] [G loss: 0.694468]\n",
      "4580 [D loss: 0.694479, acc: 43.36%] [G loss: 0.694476]\n",
      "4585 [D loss: 0.694490, acc: 46.88%] [G loss: 0.694476]\n",
      "4590 [D loss: 0.694484, acc: 44.73%] [G loss: 0.694474]\n",
      "4595 [D loss: 0.694486, acc: 48.24%] [G loss: 0.694474]\n",
      "4600 [D loss: 0.694490, acc: 46.48%] [G loss: 0.694473]\n",
      "Epoch: 4600, auc: 0.53089, ks: 0.04888412071693249\n",
      "4605 [D loss: 0.694479, acc: 44.34%] [G loss: 0.694474]\n",
      "4610 [D loss: 0.694484, acc: 46.48%] [G loss: 0.694484]\n",
      "4615 [D loss: 0.694477, acc: 45.12%] [G loss: 0.694465]\n",
      "4620 [D loss: 0.694470, acc: 44.53%] [G loss: 0.694474]\n",
      "4625 [D loss: 0.694485, acc: 46.09%] [G loss: 0.694468]\n",
      "4630 [D loss: 0.694480, acc: 42.77%] [G loss: 0.694470]\n",
      "4635 [D loss: 0.694477, acc: 47.07%] [G loss: 0.694464]\n",
      "4640 [D loss: 0.694471, acc: 43.75%] [G loss: 0.694461]\n",
      "4645 [D loss: 0.694472, acc: 43.75%] [G loss: 0.694474]\n",
      "4650 [D loss: 0.694478, acc: 45.31%] [G loss: 0.694466]\n",
      "Epoch: 4650, auc: 0.53079, ks: 0.04936116329635465\n",
      "4655 [D loss: 0.694475, acc: 44.92%] [G loss: 0.694473]\n",
      "4660 [D loss: 0.694472, acc: 47.66%] [G loss: 0.694469]\n",
      "4665 [D loss: 0.694472, acc: 43.16%] [G loss: 0.694464]\n",
      "4670 [D loss: 0.694474, acc: 46.48%] [G loss: 0.694476]\n",
      "4675 [D loss: 0.694471, acc: 43.75%] [G loss: 0.694465]\n",
      "4680 [D loss: 0.694473, acc: 42.97%] [G loss: 0.694462]\n",
      "4685 [D loss: 0.694465, acc: 45.70%] [G loss: 0.694468]\n",
      "4690 [D loss: 0.694475, acc: 42.97%] [G loss: 0.694458]\n",
      "4695 [D loss: 0.694484, acc: 48.05%] [G loss: 0.694468]\n",
      "4700 [D loss: 0.694469, acc: 45.31%] [G loss: 0.694469]\n",
      "Epoch: 4700, auc: 0.53094, ks: 0.0497224131756705\n",
      "4705 [D loss: 0.694474, acc: 45.90%] [G loss: 0.694463]\n",
      "4710 [D loss: 0.694468, acc: 46.29%] [G loss: 0.694460]\n",
      "4715 [D loss: 0.694468, acc: 46.09%] [G loss: 0.694467]\n",
      "4720 [D loss: 0.694469, acc: 42.97%] [G loss: 0.694457]\n",
      "4725 [D loss: 0.694467, acc: 44.34%] [G loss: 0.694458]\n",
      "4730 [D loss: 0.694464, acc: 44.53%] [G loss: 0.694465]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4735 [D loss: 0.694468, acc: 47.46%] [G loss: 0.694462]\n",
      "4740 [D loss: 0.694474, acc: 45.90%] [G loss: 0.694455]\n",
      "4745 [D loss: 0.694469, acc: 45.90%] [G loss: 0.694463]\n",
      "4750 [D loss: 0.694467, acc: 43.55%] [G loss: 0.694461]\n",
      "Epoch: 4750, auc: 0.53051, ks: 0.04917676614383437\n",
      "4755 [D loss: 0.694465, acc: 42.77%] [G loss: 0.694460]\n",
      "4760 [D loss: 0.694468, acc: 47.07%] [G loss: 0.694461]\n",
      "4765 [D loss: 0.694474, acc: 43.75%] [G loss: 0.694457]\n",
      "4770 [D loss: 0.694468, acc: 46.48%] [G loss: 0.694458]\n",
      "4775 [D loss: 0.694470, acc: 44.92%] [G loss: 0.694457]\n",
      "4780 [D loss: 0.694468, acc: 44.92%] [G loss: 0.694464]\n",
      "4785 [D loss: 0.694471, acc: 44.34%] [G loss: 0.694454]\n",
      "4790 [D loss: 0.694471, acc: 45.70%] [G loss: 0.694460]\n",
      "4795 [D loss: 0.694464, acc: 45.90%] [G loss: 0.694459]\n",
      "4800 [D loss: 0.694468, acc: 46.09%] [G loss: 0.694455]\n",
      "Epoch: 4800, auc: 0.53082, ks: 0.04887353005138706\n",
      "4805 [D loss: 0.694462, acc: 47.66%] [G loss: 0.694458]\n",
      "4810 [D loss: 0.694463, acc: 45.12%] [G loss: 0.694459]\n",
      "4815 [D loss: 0.694462, acc: 43.95%] [G loss: 0.694457]\n",
      "4820 [D loss: 0.694469, acc: 46.29%] [G loss: 0.694461]\n",
      "4825 [D loss: 0.694467, acc: 45.90%] [G loss: 0.694452]\n",
      "4830 [D loss: 0.694461, acc: 45.51%] [G loss: 0.694459]\n",
      "4835 [D loss: 0.694459, acc: 44.34%] [G loss: 0.694458]\n",
      "4840 [D loss: 0.694465, acc: 45.51%] [G loss: 0.694456]\n",
      "4845 [D loss: 0.694464, acc: 42.19%] [G loss: 0.694459]\n",
      "4850 [D loss: 0.694470, acc: 45.12%] [G loss: 0.694458]\n",
      "Epoch: 4850, auc: 0.53081, ks: 0.049070204689166674\n",
      "4855 [D loss: 0.694454, acc: 44.34%] [G loss: 0.694462]\n",
      "4860 [D loss: 0.694474, acc: 45.31%] [G loss: 0.694452]\n",
      "4865 [D loss: 0.694470, acc: 43.55%] [G loss: 0.694458]\n",
      "4870 [D loss: 0.694461, acc: 45.70%] [G loss: 0.694451]\n",
      "4875 [D loss: 0.694457, acc: 45.90%] [G loss: 0.694449]\n",
      "4880 [D loss: 0.694458, acc: 45.31%] [G loss: 0.694455]\n",
      "4885 [D loss: 0.694463, acc: 47.66%] [G loss: 0.694449]\n",
      "4890 [D loss: 0.694459, acc: 45.31%] [G loss: 0.694460]\n",
      "4895 [D loss: 0.694462, acc: 43.95%] [G loss: 0.694449]\n",
      "4900 [D loss: 0.694462, acc: 43.16%] [G loss: 0.694447]\n",
      "Epoch: 4900, auc: 0.53100, ks: 0.04885218502049293\n",
      "4905 [D loss: 0.694449, acc: 41.02%] [G loss: 0.694455]\n",
      "4910 [D loss: 0.694467, acc: 45.70%] [G loss: 0.694454]\n",
      "4915 [D loss: 0.694463, acc: 43.36%] [G loss: 0.694451]\n",
      "4920 [D loss: 0.694453, acc: 45.12%] [G loss: 0.694449]\n",
      "4925 [D loss: 0.694454, acc: 43.55%] [G loss: 0.694450]\n",
      "4930 [D loss: 0.694455, acc: 42.58%] [G loss: 0.694446]\n",
      "4935 [D loss: 0.694453, acc: 44.92%] [G loss: 0.694450]\n",
      "4940 [D loss: 0.694461, acc: 42.77%] [G loss: 0.694455]\n",
      "4945 [D loss: 0.694455, acc: 47.85%] [G loss: 0.694451]\n",
      "4950 [D loss: 0.694460, acc: 44.34%] [G loss: 0.694452]\n",
      "Epoch: 4950, auc: 0.53106, ks: 0.049384593720397274\n",
      "4955 [D loss: 0.694456, acc: 46.48%] [G loss: 0.694451]\n",
      "4960 [D loss: 0.694454, acc: 44.14%] [G loss: 0.694449]\n",
      "4965 [D loss: 0.694455, acc: 46.09%] [G loss: 0.694451]\n",
      "4970 [D loss: 0.694454, acc: 48.05%] [G loss: 0.694450]\n",
      "4975 [D loss: 0.694456, acc: 42.97%] [G loss: 0.694448]\n",
      "4980 [D loss: 0.694457, acc: 42.38%] [G loss: 0.694450]\n",
      "4985 [D loss: 0.694450, acc: 43.75%] [G loss: 0.694446]\n",
      "4990 [D loss: 0.694462, acc: 44.53%] [G loss: 0.694445]\n",
      "4995 [D loss: 0.694463, acc: 43.55%] [G loss: 0.694448]\n",
      "5000 [D loss: 0.694451, acc: 47.85%] [G loss: 0.694445]\n",
      "Epoch: 5000, auc: 0.53114, ks: 0.04894061138389072\n",
      "5005 [D loss: 0.694456, acc: 45.70%] [G loss: 0.694445]\n",
      "5010 [D loss: 0.694450, acc: 42.97%] [G loss: 0.694448]\n",
      "5015 [D loss: 0.694453, acc: 44.14%] [G loss: 0.694445]\n",
      "5020 [D loss: 0.694455, acc: 42.38%] [G loss: 0.694443]\n",
      "5025 [D loss: 0.694460, acc: 43.75%] [G loss: 0.694445]\n",
      "5030 [D loss: 0.694453, acc: 43.55%] [G loss: 0.694443]\n",
      "5035 [D loss: 0.694455, acc: 44.53%] [G loss: 0.694447]\n",
      "5040 [D loss: 0.694452, acc: 45.51%] [G loss: 0.694448]\n",
      "5045 [D loss: 0.694453, acc: 45.12%] [G loss: 0.694449]\n",
      "5050 [D loss: 0.694457, acc: 43.95%] [G loss: 0.694443]\n",
      "Epoch: 5050, auc: 0.53103, ks: 0.049450151932990405\n",
      "5055 [D loss: 0.694448, acc: 46.88%] [G loss: 0.694445]\n",
      "5060 [D loss: 0.694455, acc: 42.38%] [G loss: 0.694443]\n",
      "5065 [D loss: 0.694448, acc: 45.12%] [G loss: 0.694441]\n",
      "5070 [D loss: 0.694456, acc: 44.92%] [G loss: 0.694444]\n",
      "5075 [D loss: 0.694451, acc: 44.73%] [G loss: 0.694444]\n",
      "5080 [D loss: 0.694441, acc: 44.53%] [G loss: 0.694442]\n",
      "5085 [D loss: 0.694451, acc: 45.51%] [G loss: 0.694445]\n",
      "5090 [D loss: 0.694451, acc: 43.16%] [G loss: 0.694445]\n",
      "5095 [D loss: 0.694450, acc: 43.95%] [G loss: 0.694447]\n",
      "5100 [D loss: 0.694448, acc: 45.70%] [G loss: 0.694440]\n",
      "Epoch: 5100, auc: 0.53101, ks: 0.048975194746716744\n",
      "5105 [D loss: 0.694447, acc: 41.60%] [G loss: 0.694447]\n",
      "5110 [D loss: 0.694445, acc: 43.75%] [G loss: 0.694445]\n",
      "5115 [D loss: 0.694447, acc: 43.95%] [G loss: 0.694439]\n",
      "5120 [D loss: 0.694448, acc: 44.53%] [G loss: 0.694437]\n",
      "5125 [D loss: 0.694443, acc: 44.14%] [G loss: 0.694440]\n",
      "5130 [D loss: 0.694449, acc: 43.75%] [G loss: 0.694443]\n",
      "5135 [D loss: 0.694446, acc: 40.82%] [G loss: 0.694443]\n",
      "5140 [D loss: 0.694445, acc: 45.31%] [G loss: 0.694442]\n",
      "5145 [D loss: 0.694447, acc: 47.27%] [G loss: 0.694439]\n",
      "5150 [D loss: 0.694440, acc: 44.53%] [G loss: 0.694437]\n",
      "Epoch: 5150, auc: 0.53105, ks: 0.04841839477609722\n",
      "5155 [D loss: 0.694442, acc: 43.16%] [G loss: 0.694439]\n",
      "5160 [D loss: 0.694448, acc: 42.19%] [G loss: 0.694440]\n",
      "5165 [D loss: 0.694443, acc: 46.48%] [G loss: 0.694440]\n",
      "5170 [D loss: 0.694443, acc: 43.36%] [G loss: 0.694438]\n",
      "5175 [D loss: 0.694442, acc: 43.55%] [G loss: 0.694443]\n",
      "5180 [D loss: 0.694443, acc: 44.53%] [G loss: 0.694440]\n",
      "5185 [D loss: 0.694443, acc: 42.38%] [G loss: 0.694438]\n",
      "5190 [D loss: 0.694449, acc: 45.51%] [G loss: 0.694437]\n",
      "5195 [D loss: 0.694441, acc: 44.53%] [G loss: 0.694439]\n",
      "5200 [D loss: 0.694446, acc: 43.16%] [G loss: 0.694442]\n",
      "Epoch: 5200, auc: 0.53101, ks: 0.04855103432119401\n",
      "5205 [D loss: 0.694446, acc: 43.75%] [G loss: 0.694441]\n",
      "5210 [D loss: 0.694444, acc: 44.53%] [G loss: 0.694440]\n",
      "5215 [D loss: 0.694442, acc: 42.38%] [G loss: 0.694438]\n",
      "5220 [D loss: 0.694445, acc: 43.95%] [G loss: 0.694439]\n",
      "5225 [D loss: 0.694445, acc: 40.04%] [G loss: 0.694436]\n",
      "5230 [D loss: 0.694444, acc: 44.34%] [G loss: 0.694439]\n",
      "5235 [D loss: 0.694444, acc: 44.92%] [G loss: 0.694434]\n",
      "5240 [D loss: 0.694446, acc: 45.31%] [G loss: 0.694439]\n",
      "5245 [D loss: 0.694440, acc: 43.95%] [G loss: 0.694435]\n",
      "5250 [D loss: 0.694438, acc: 45.12%] [G loss: 0.694436]\n",
      "Epoch: 5250, auc: 0.53101, ks: 0.04845354041216121\n",
      "5255 [D loss: 0.694444, acc: 41.80%] [G loss: 0.694437]\n",
      "5260 [D loss: 0.694438, acc: 42.77%] [G loss: 0.694437]\n",
      "5265 [D loss: 0.694444, acc: 43.55%] [G loss: 0.694435]\n",
      "5270 [D loss: 0.694443, acc: 43.16%] [G loss: 0.694438]\n",
      "5275 [D loss: 0.694441, acc: 45.12%] [G loss: 0.694438]\n",
      "5280 [D loss: 0.694443, acc: 47.46%] [G loss: 0.694437]\n",
      "5285 [D loss: 0.694445, acc: 42.58%] [G loss: 0.694435]\n",
      "5290 [D loss: 0.694440, acc: 44.14%] [G loss: 0.694434]\n",
      "5295 [D loss: 0.694440, acc: 44.34%] [G loss: 0.694434]\n",
      "5300 [D loss: 0.694441, acc: 42.97%] [G loss: 0.694436]\n",
      "Epoch: 5300, auc: 0.53086, ks: 0.04856595947283959\n",
      "5305 [D loss: 0.694435, acc: 44.53%] [G loss: 0.694437]\n",
      "5310 [D loss: 0.694443, acc: 42.38%] [G loss: 0.694438]\n",
      "5315 [D loss: 0.694444, acc: 45.70%] [G loss: 0.694435]\n",
      "5320 [D loss: 0.694439, acc: 43.75%] [G loss: 0.694434]\n",
      "5325 [D loss: 0.694439, acc: 43.95%] [G loss: 0.694433]\n",
      "5330 [D loss: 0.694439, acc: 43.75%] [G loss: 0.694439]\n",
      "5335 [D loss: 0.694444, acc: 43.36%] [G loss: 0.694434]\n",
      "5340 [D loss: 0.694439, acc: 46.29%] [G loss: 0.694435]\n",
      "5345 [D loss: 0.694436, acc: 41.80%] [G loss: 0.694433]\n",
      "5350 [D loss: 0.694443, acc: 42.97%] [G loss: 0.694435]\n",
      "Epoch: 5350, auc: 0.53090, ks: 0.04942559696247184\n",
      "5355 [D loss: 0.694439, acc: 45.31%] [G loss: 0.694436]\n",
      "5360 [D loss: 0.694432, acc: 43.16%] [G loss: 0.694431]\n",
      "5365 [D loss: 0.694435, acc: 41.80%] [G loss: 0.694431]\n",
      "5370 [D loss: 0.694438, acc: 43.55%] [G loss: 0.694431]\n",
      "5375 [D loss: 0.694441, acc: 41.41%] [G loss: 0.694434]\n",
      "5380 [D loss: 0.694442, acc: 44.73%] [G loss: 0.694435]\n",
      "5385 [D loss: 0.694440, acc: 42.58%] [G loss: 0.694430]\n",
      "5390 [D loss: 0.694439, acc: 42.58%] [G loss: 0.694434]\n",
      "5395 [D loss: 0.694439, acc: 45.12%] [G loss: 0.694432]\n",
      "5400 [D loss: 0.694434, acc: 43.16%] [G loss: 0.694430]\n",
      "Epoch: 5400, auc: 0.53070, ks: 0.049381946054010806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5405 [D loss: 0.694434, acc: 42.38%] [G loss: 0.694431]\n",
      "5410 [D loss: 0.694439, acc: 42.38%] [G loss: 0.694430]\n",
      "5415 [D loss: 0.694438, acc: 44.73%] [G loss: 0.694430]\n",
      "5420 [D loss: 0.694436, acc: 45.51%] [G loss: 0.694429]\n",
      "5425 [D loss: 0.694436, acc: 44.14%] [G loss: 0.694431]\n",
      "5430 [D loss: 0.694433, acc: 44.14%] [G loss: 0.694433]\n",
      "5435 [D loss: 0.694433, acc: 44.53%] [G loss: 0.694428]\n",
      "5440 [D loss: 0.694435, acc: 44.92%] [G loss: 0.694431]\n",
      "5445 [D loss: 0.694435, acc: 43.95%] [G loss: 0.694428]\n",
      "5450 [D loss: 0.694438, acc: 42.77%] [G loss: 0.694431]\n",
      "Epoch: 5450, auc: 0.53065, ks: 0.04825574122790588\n",
      "5455 [D loss: 0.694433, acc: 42.77%] [G loss: 0.694432]\n",
      "5460 [D loss: 0.694435, acc: 43.36%] [G loss: 0.694432]\n",
      "5465 [D loss: 0.694432, acc: 42.38%] [G loss: 0.694433]\n",
      "5470 [D loss: 0.694438, acc: 43.75%] [G loss: 0.694430]\n",
      "5475 [D loss: 0.694434, acc: 45.31%] [G loss: 0.694429]\n",
      "5480 [D loss: 0.694430, acc: 40.43%] [G loss: 0.694429]\n",
      "5485 [D loss: 0.694433, acc: 41.80%] [G loss: 0.694426]\n",
      "5490 [D loss: 0.694435, acc: 41.80%] [G loss: 0.694432]\n",
      "5495 [D loss: 0.694432, acc: 41.60%] [G loss: 0.694428]\n",
      "5500 [D loss: 0.694432, acc: 41.41%] [G loss: 0.694429]\n",
      "Epoch: 5500, auc: 0.53088, ks: 0.049249142809110746\n",
      "5505 [D loss: 0.694430, acc: 45.70%] [G loss: 0.694430]\n",
      "5510 [D loss: 0.694430, acc: 39.26%] [G loss: 0.694426]\n",
      "5515 [D loss: 0.694430, acc: 44.92%] [G loss: 0.694428]\n",
      "5520 [D loss: 0.694428, acc: 44.34%] [G loss: 0.694427]\n",
      "5525 [D loss: 0.694433, acc: 39.84%] [G loss: 0.694428]\n",
      "5530 [D loss: 0.694430, acc: 42.58%] [G loss: 0.694426]\n",
      "5535 [D loss: 0.694429, acc: 43.16%] [G loss: 0.694427]\n",
      "5540 [D loss: 0.694430, acc: 42.19%] [G loss: 0.694427]\n",
      "5545 [D loss: 0.694432, acc: 44.14%] [G loss: 0.694426]\n",
      "5550 [D loss: 0.694430, acc: 42.38%] [G loss: 0.694425]\n",
      "Epoch: 5550, auc: 0.53099, ks: 0.05027239469360689\n",
      "5555 [D loss: 0.694432, acc: 40.43%] [G loss: 0.694424]\n",
      "5560 [D loss: 0.694434, acc: 44.14%] [G loss: 0.694426]\n",
      "5565 [D loss: 0.694431, acc: 40.04%] [G loss: 0.694429]\n",
      "5570 [D loss: 0.694428, acc: 43.36%] [G loss: 0.694429]\n",
      "5575 [D loss: 0.694431, acc: 40.62%] [G loss: 0.694427]\n",
      "5580 [D loss: 0.694430, acc: 43.95%] [G loss: 0.694425]\n",
      "5585 [D loss: 0.694430, acc: 44.53%] [G loss: 0.694428]\n",
      "5590 [D loss: 0.694430, acc: 42.97%] [G loss: 0.694425]\n",
      "5595 [D loss: 0.694425, acc: 41.60%] [G loss: 0.694428]\n",
      "5600 [D loss: 0.694428, acc: 44.53%] [G loss: 0.694426]\n",
      "Epoch: 5600, auc: 0.53086, ks: 0.049022617868039964\n",
      "5605 [D loss: 0.694429, acc: 42.97%] [G loss: 0.694426]\n",
      "5610 [D loss: 0.694426, acc: 42.77%] [G loss: 0.694426]\n",
      "5615 [D loss: 0.694430, acc: 42.77%] [G loss: 0.694426]\n",
      "5620 [D loss: 0.694432, acc: 42.58%] [G loss: 0.694424]\n",
      "5625 [D loss: 0.694428, acc: 41.02%] [G loss: 0.694422]\n",
      "5630 [D loss: 0.694429, acc: 39.45%] [G loss: 0.694425]\n",
      "5635 [D loss: 0.694428, acc: 41.41%] [G loss: 0.694426]\n",
      "5640 [D loss: 0.694426, acc: 42.97%] [G loss: 0.694425]\n",
      "5645 [D loss: 0.694429, acc: 42.58%] [G loss: 0.694426]\n",
      "5650 [D loss: 0.694429, acc: 42.58%] [G loss: 0.694426]\n",
      "Epoch: 5650, auc: 0.53077, ks: 0.04871938177559265\n",
      "5655 [D loss: 0.694431, acc: 44.92%] [G loss: 0.694425]\n",
      "5660 [D loss: 0.694426, acc: 43.55%] [G loss: 0.694423]\n",
      "5665 [D loss: 0.694425, acc: 41.02%] [G loss: 0.694423]\n",
      "5670 [D loss: 0.694427, acc: 43.16%] [G loss: 0.694423]\n",
      "5675 [D loss: 0.694427, acc: 42.97%] [G loss: 0.694422]\n",
      "5680 [D loss: 0.694429, acc: 41.99%] [G loss: 0.694423]\n",
      "5685 [D loss: 0.694427, acc: 42.19%] [G loss: 0.694427]\n",
      "5690 [D loss: 0.694428, acc: 44.14%] [G loss: 0.694423]\n",
      "5695 [D loss: 0.694426, acc: 44.53%] [G loss: 0.694423]\n",
      "5700 [D loss: 0.694427, acc: 39.26%] [G loss: 0.694424]\n",
      "Epoch: 5700, auc: 0.53062, ks: 0.04836134183590113\n",
      "5705 [D loss: 0.694428, acc: 43.36%] [G loss: 0.694425]\n",
      "5710 [D loss: 0.694424, acc: 39.65%] [G loss: 0.694425]\n",
      "5715 [D loss: 0.694427, acc: 42.19%] [G loss: 0.694425]\n",
      "5720 [D loss: 0.694427, acc: 38.48%] [G loss: 0.694427]\n",
      "5725 [D loss: 0.694424, acc: 42.97%] [G loss: 0.694423]\n",
      "5730 [D loss: 0.694426, acc: 41.60%] [G loss: 0.694422]\n",
      "5735 [D loss: 0.694425, acc: 40.43%] [G loss: 0.694423]\n",
      "5740 [D loss: 0.694426, acc: 40.43%] [G loss: 0.694423]\n",
      "5745 [D loss: 0.694426, acc: 43.55%] [G loss: 0.694424]\n",
      "5750 [D loss: 0.694428, acc: 42.58%] [G loss: 0.694421]\n",
      "Epoch: 5750, auc: 0.53054, ks: 0.04785348810651524\n",
      "5755 [D loss: 0.694425, acc: 41.41%] [G loss: 0.694422]\n",
      "5760 [D loss: 0.694425, acc: 41.80%] [G loss: 0.694423]\n",
      "5765 [D loss: 0.694427, acc: 43.75%] [G loss: 0.694422]\n",
      "5770 [D loss: 0.694424, acc: 44.34%] [G loss: 0.694420]\n",
      "5775 [D loss: 0.694427, acc: 41.80%] [G loss: 0.694420]\n",
      "5780 [D loss: 0.694425, acc: 41.80%] [G loss: 0.694422]\n",
      "5785 [D loss: 0.694424, acc: 40.23%] [G loss: 0.694419]\n",
      "5790 [D loss: 0.694425, acc: 41.02%] [G loss: 0.694421]\n",
      "5795 [D loss: 0.694423, acc: 41.80%] [G loss: 0.694423]\n",
      "5800 [D loss: 0.694424, acc: 42.77%] [G loss: 0.694423]\n",
      "Epoch: 5800, auc: 0.53094, ks: 0.04852439395752717\n",
      "5805 [D loss: 0.694426, acc: 39.26%] [G loss: 0.694423]\n",
      "5810 [D loss: 0.694424, acc: 43.95%] [G loss: 0.694422]\n",
      "5815 [D loss: 0.694422, acc: 41.80%] [G loss: 0.694422]\n",
      "5820 [D loss: 0.694425, acc: 43.95%] [G loss: 0.694422]\n",
      "5825 [D loss: 0.694424, acc: 41.60%] [G loss: 0.694420]\n",
      "5830 [D loss: 0.694423, acc: 41.60%] [G loss: 0.694421]\n",
      "5835 [D loss: 0.694423, acc: 42.97%] [G loss: 0.694422]\n",
      "5840 [D loss: 0.694425, acc: 44.53%] [G loss: 0.694420]\n",
      "5845 [D loss: 0.694424, acc: 39.06%] [G loss: 0.694420]\n",
      "5850 [D loss: 0.694423, acc: 42.97%] [G loss: 0.694422]\n",
      "Epoch: 5850, auc: 0.53117, ks: 0.04842312783563196\n",
      "5855 [D loss: 0.694422, acc: 41.80%] [G loss: 0.694419]\n",
      "5860 [D loss: 0.694423, acc: 41.80%] [G loss: 0.694419]\n",
      "5865 [D loss: 0.694422, acc: 41.99%] [G loss: 0.694423]\n",
      "5870 [D loss: 0.694423, acc: 42.58%] [G loss: 0.694420]\n",
      "5875 [D loss: 0.694423, acc: 41.60%] [G loss: 0.694419]\n",
      "5880 [D loss: 0.694422, acc: 42.38%] [G loss: 0.694418]\n",
      "5885 [D loss: 0.694422, acc: 42.97%] [G loss: 0.694420]\n",
      "5890 [D loss: 0.694425, acc: 42.77%] [G loss: 0.694419]\n",
      "5895 [D loss: 0.694422, acc: 41.02%] [G loss: 0.694420]\n",
      "5900 [D loss: 0.694420, acc: 38.87%] [G loss: 0.694419]\n",
      "Epoch: 5900, auc: 0.53106, ks: 0.04886542335242461\n",
      "5905 [D loss: 0.694423, acc: 41.80%] [G loss: 0.694417]\n",
      "5910 [D loss: 0.694422, acc: 42.77%] [G loss: 0.694418]\n",
      "5915 [D loss: 0.694421, acc: 41.80%] [G loss: 0.694420]\n",
      "5920 [D loss: 0.694423, acc: 39.84%] [G loss: 0.694419]\n",
      "5925 [D loss: 0.694419, acc: 40.82%] [G loss: 0.694416]\n",
      "5930 [D loss: 0.694422, acc: 41.41%] [G loss: 0.694420]\n",
      "5935 [D loss: 0.694420, acc: 41.41%] [G loss: 0.694417]\n",
      "5940 [D loss: 0.694422, acc: 41.60%] [G loss: 0.694418]\n",
      "5945 [D loss: 0.694421, acc: 42.19%] [G loss: 0.694420]\n",
      "5950 [D loss: 0.694418, acc: 41.02%] [G loss: 0.694420]\n",
      "Epoch: 5950, auc: 0.53103, ks: 0.04837891465393307\n",
      "5955 [D loss: 0.694420, acc: 43.36%] [G loss: 0.694418]\n",
      "5960 [D loss: 0.694420, acc: 42.58%] [G loss: 0.694419]\n",
      "5965 [D loss: 0.694421, acc: 39.84%] [G loss: 0.694417]\n",
      "5970 [D loss: 0.694422, acc: 40.23%] [G loss: 0.694416]\n",
      "5975 [D loss: 0.694423, acc: 42.38%] [G loss: 0.694419]\n",
      "5980 [D loss: 0.694421, acc: 40.23%] [G loss: 0.694418]\n",
      "5985 [D loss: 0.694419, acc: 40.43%] [G loss: 0.694416]\n",
      "5990 [D loss: 0.694421, acc: 41.80%] [G loss: 0.694417]\n",
      "5995 [D loss: 0.694421, acc: 41.41%] [G loss: 0.694419]\n",
      "6000 [D loss: 0.694417, acc: 40.04%] [G loss: 0.694418]\n",
      "Epoch: 6000, auc: 0.53110, ks: 0.04835114974379029\n",
      "6005 [D loss: 0.694418, acc: 41.99%] [G loss: 0.694416]\n",
      "6010 [D loss: 0.694420, acc: 41.41%] [G loss: 0.694418]\n",
      "6015 [D loss: 0.694420, acc: 42.77%] [G loss: 0.694419]\n",
      "6020 [D loss: 0.694418, acc: 40.23%] [G loss: 0.694417]\n",
      "6025 [D loss: 0.694419, acc: 39.84%] [G loss: 0.694417]\n",
      "6030 [D loss: 0.694419, acc: 37.11%] [G loss: 0.694417]\n",
      "6035 [D loss: 0.694421, acc: 40.04%] [G loss: 0.694419]\n",
      "6040 [D loss: 0.694419, acc: 40.23%] [G loss: 0.694416]\n",
      "6045 [D loss: 0.694419, acc: 40.62%] [G loss: 0.694416]\n",
      "6050 [D loss: 0.694421, acc: 41.21%] [G loss: 0.694415]\n",
      "Epoch: 6050, auc: 0.53117, ks: 0.04839592519872704\n",
      "6055 [D loss: 0.694418, acc: 39.45%] [G loss: 0.694416]\n",
      "6060 [D loss: 0.694419, acc: 40.23%] [G loss: 0.694416]\n",
      "6065 [D loss: 0.694419, acc: 40.04%] [G loss: 0.694417]\n",
      "6070 [D loss: 0.694420, acc: 42.38%] [G loss: 0.694415]\n",
      "6075 [D loss: 0.694419, acc: 40.43%] [G loss: 0.694417]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6080 [D loss: 0.694418, acc: 41.02%] [G loss: 0.694416]\n",
      "6085 [D loss: 0.694416, acc: 38.28%] [G loss: 0.694414]\n",
      "6090 [D loss: 0.694420, acc: 41.41%] [G loss: 0.694417]\n",
      "6095 [D loss: 0.694418, acc: 39.84%] [G loss: 0.694414]\n",
      "6100 [D loss: 0.694418, acc: 38.09%] [G loss: 0.694418]\n",
      "Epoch: 6100, auc: 0.53117, ks: 0.049202844234263465\n",
      "6105 [D loss: 0.694417, acc: 38.67%] [G loss: 0.694417]\n",
      "6110 [D loss: 0.694416, acc: 41.41%] [G loss: 0.694415]\n",
      "6115 [D loss: 0.694419, acc: 36.72%] [G loss: 0.694414]\n",
      "6120 [D loss: 0.694417, acc: 38.28%] [G loss: 0.694414]\n",
      "6125 [D loss: 0.694418, acc: 43.95%] [G loss: 0.694416]\n",
      "6130 [D loss: 0.694415, acc: 40.82%] [G loss: 0.694417]\n",
      "6135 [D loss: 0.694417, acc: 40.04%] [G loss: 0.694414]\n",
      "6140 [D loss: 0.694418, acc: 41.80%] [G loss: 0.694416]\n",
      "6145 [D loss: 0.694419, acc: 41.02%] [G loss: 0.694415]\n",
      "6150 [D loss: 0.694417, acc: 42.19%] [G loss: 0.694414]\n",
      "Epoch: 6150, auc: 0.53131, ks: 0.04809325137951781\n",
      "6155 [D loss: 0.694417, acc: 39.84%] [G loss: 0.694416]\n",
      "6160 [D loss: 0.694417, acc: 39.84%] [G loss: 0.694415]\n",
      "6165 [D loss: 0.694417, acc: 38.48%] [G loss: 0.694415]\n",
      "6170 [D loss: 0.694416, acc: 39.45%] [G loss: 0.694414]\n",
      "6175 [D loss: 0.694416, acc: 39.65%] [G loss: 0.694415]\n",
      "6180 [D loss: 0.694416, acc: 42.97%] [G loss: 0.694414]\n",
      "6185 [D loss: 0.694417, acc: 42.19%] [G loss: 0.694414]\n",
      "6190 [D loss: 0.694414, acc: 40.04%] [G loss: 0.694414]\n",
      "6195 [D loss: 0.694417, acc: 40.23%] [G loss: 0.694414]\n",
      "6200 [D loss: 0.694415, acc: 39.84%] [G loss: 0.694414]\n",
      "Epoch: 6200, auc: 0.53123, ks: 0.04877170165625411\n",
      "6205 [D loss: 0.694417, acc: 38.67%] [G loss: 0.694415]\n",
      "6210 [D loss: 0.694417, acc: 41.21%] [G loss: 0.694413]\n",
      "6215 [D loss: 0.694416, acc: 39.26%] [G loss: 0.694414]\n",
      "6220 [D loss: 0.694416, acc: 39.65%] [G loss: 0.694413]\n",
      "6225 [D loss: 0.694415, acc: 38.87%] [G loss: 0.694415]\n",
      "6230 [D loss: 0.694416, acc: 39.45%] [G loss: 0.694414]\n",
      "6235 [D loss: 0.694416, acc: 42.97%] [G loss: 0.694413]\n",
      "6240 [D loss: 0.694415, acc: 39.26%] [G loss: 0.694413]\n",
      "6245 [D loss: 0.694415, acc: 39.45%] [G loss: 0.694414]\n",
      "6250 [D loss: 0.694416, acc: 40.43%] [G loss: 0.694413]\n",
      "Epoch: 6250, auc: 0.53108, ks: 0.048640022957829876\n",
      "6255 [D loss: 0.694417, acc: 41.80%] [G loss: 0.694414]\n",
      "6260 [D loss: 0.694416, acc: 42.58%] [G loss: 0.694413]\n",
      "6265 [D loss: 0.694417, acc: 41.21%] [G loss: 0.694413]\n",
      "6270 [D loss: 0.694416, acc: 39.84%] [G loss: 0.694412]\n",
      "6275 [D loss: 0.694412, acc: 38.48%] [G loss: 0.694412]\n",
      "6280 [D loss: 0.694413, acc: 39.65%] [G loss: 0.694412]\n",
      "6285 [D loss: 0.694414, acc: 40.43%] [G loss: 0.694412]\n",
      "6290 [D loss: 0.694416, acc: 39.65%] [G loss: 0.694414]\n",
      "6295 [D loss: 0.694417, acc: 42.77%] [G loss: 0.694413]\n",
      "6300 [D loss: 0.694413, acc: 37.30%] [G loss: 0.694413]\n",
      "Epoch: 6300, auc: 0.53082, ks: 0.04832129944049901\n",
      "6305 [D loss: 0.694412, acc: 38.48%] [G loss: 0.694413]\n",
      "6310 [D loss: 0.694414, acc: 40.04%] [G loss: 0.694413]\n",
      "6315 [D loss: 0.694414, acc: 39.65%] [G loss: 0.694413]\n",
      "6320 [D loss: 0.694414, acc: 40.62%] [G loss: 0.694414]\n",
      "6325 [D loss: 0.694414, acc: 41.60%] [G loss: 0.694413]\n",
      "6330 [D loss: 0.694412, acc: 39.06%] [G loss: 0.694412]\n",
      "6335 [D loss: 0.694416, acc: 37.50%] [G loss: 0.694412]\n",
      "6340 [D loss: 0.694414, acc: 36.33%] [G loss: 0.694413]\n",
      "6345 [D loss: 0.694413, acc: 39.06%] [G loss: 0.694413]\n",
      "6350 [D loss: 0.694415, acc: 39.06%] [G loss: 0.694413]\n",
      "Epoch: 6350, auc: 0.53079, ks: 0.04845297813892324\n",
      "6355 [D loss: 0.694412, acc: 40.82%] [G loss: 0.694411]\n",
      "6360 [D loss: 0.694412, acc: 36.91%] [G loss: 0.694412]\n",
      "6365 [D loss: 0.694413, acc: 38.28%] [G loss: 0.694412]\n",
      "6370 [D loss: 0.694413, acc: 38.67%] [G loss: 0.694412]\n",
      "6375 [D loss: 0.694413, acc: 41.21%] [G loss: 0.694412]\n",
      "6380 [D loss: 0.694415, acc: 42.38%] [G loss: 0.694412]\n",
      "6385 [D loss: 0.694413, acc: 37.89%] [G loss: 0.694412]\n",
      "6390 [D loss: 0.694413, acc: 38.09%] [G loss: 0.694411]\n",
      "6395 [D loss: 0.694413, acc: 37.11%] [G loss: 0.694410]\n",
      "6400 [D loss: 0.694412, acc: 38.87%] [G loss: 0.694411]\n",
      "Epoch: 6400, auc: 0.53081, ks: 0.047659461135122094\n",
      "6405 [D loss: 0.694414, acc: 38.48%] [G loss: 0.694412]\n",
      "6410 [D loss: 0.694412, acc: 38.48%] [G loss: 0.694411]\n",
      "6415 [D loss: 0.694412, acc: 38.09%] [G loss: 0.694411]\n",
      "6420 [D loss: 0.694413, acc: 40.62%] [G loss: 0.694412]\n",
      "6425 [D loss: 0.694413, acc: 40.43%] [G loss: 0.694412]\n",
      "6430 [D loss: 0.694413, acc: 37.30%] [G loss: 0.694410]\n",
      "6435 [D loss: 0.694412, acc: 36.33%] [G loss: 0.694411]\n",
      "6440 [D loss: 0.694413, acc: 39.26%] [G loss: 0.694412]\n",
      "6445 [D loss: 0.694414, acc: 41.21%] [G loss: 0.694411]\n",
      "6450 [D loss: 0.694412, acc: 39.84%] [G loss: 0.694411]\n",
      "Epoch: 6450, auc: 0.53063, ks: 0.047584835376894064\n",
      "6455 [D loss: 0.694413, acc: 37.50%] [G loss: 0.694410]\n",
      "6460 [D loss: 0.694412, acc: 37.11%] [G loss: 0.694410]\n",
      "6465 [D loss: 0.694412, acc: 39.06%] [G loss: 0.694411]\n",
      "6470 [D loss: 0.694412, acc: 38.67%] [G loss: 0.694410]\n",
      "6475 [D loss: 0.694411, acc: 39.84%] [G loss: 0.694410]\n",
      "6480 [D loss: 0.694413, acc: 39.26%] [G loss: 0.694410]\n",
      "6485 [D loss: 0.694412, acc: 37.89%] [G loss: 0.694410]\n",
      "6490 [D loss: 0.694411, acc: 37.30%] [G loss: 0.694411]\n",
      "6495 [D loss: 0.694412, acc: 36.91%] [G loss: 0.694411]\n",
      "6500 [D loss: 0.694412, acc: 38.87%] [G loss: 0.694411]\n",
      "Epoch: 6500, auc: 0.53086, ks: 0.04843805298727766\n",
      "6505 [D loss: 0.694412, acc: 39.26%] [G loss: 0.694411]\n",
      "6510 [D loss: 0.694412, acc: 38.09%] [G loss: 0.694410]\n",
      "6515 [D loss: 0.694413, acc: 40.04%] [G loss: 0.694412]\n",
      "6520 [D loss: 0.694414, acc: 36.52%] [G loss: 0.694409]\n",
      "6525 [D loss: 0.694410, acc: 36.72%] [G loss: 0.694410]\n",
      "6530 [D loss: 0.694412, acc: 41.99%] [G loss: 0.694410]\n",
      "6535 [D loss: 0.694411, acc: 38.28%] [G loss: 0.694409]\n",
      "6540 [D loss: 0.694413, acc: 39.26%] [G loss: 0.694410]\n",
      "6545 [D loss: 0.694413, acc: 38.67%] [G loss: 0.694409]\n",
      "6550 [D loss: 0.694411, acc: 37.30%] [G loss: 0.694410]\n",
      "Epoch: 6550, auc: 0.53103, ks: 0.04835869416951477\n",
      "6555 [D loss: 0.694411, acc: 38.48%] [G loss: 0.694410]\n",
      "6560 [D loss: 0.694410, acc: 37.89%] [G loss: 0.694409]\n",
      "6565 [D loss: 0.694410, acc: 37.89%] [G loss: 0.694410]\n",
      "6570 [D loss: 0.694411, acc: 41.21%] [G loss: 0.694410]\n",
      "6575 [D loss: 0.694412, acc: 38.48%] [G loss: 0.694409]\n",
      "6580 [D loss: 0.694409, acc: 37.50%] [G loss: 0.694410]\n",
      "6585 [D loss: 0.694411, acc: 38.28%] [G loss: 0.694410]\n",
      "6590 [D loss: 0.694411, acc: 36.72%] [G loss: 0.694410]\n",
      "6595 [D loss: 0.694411, acc: 37.50%] [G loss: 0.694410]\n",
      "6600 [D loss: 0.694411, acc: 35.35%] [G loss: 0.694410]\n",
      "Epoch: 6600, auc: 0.53101, ks: 0.047573518738307286\n",
      "6605 [D loss: 0.694411, acc: 38.87%] [G loss: 0.694410]\n",
      "6610 [D loss: 0.694411, acc: 38.48%] [G loss: 0.694409]\n",
      "6615 [D loss: 0.694411, acc: 37.70%] [G loss: 0.694410]\n",
      "6620 [D loss: 0.694409, acc: 39.65%] [G loss: 0.694408]\n",
      "6625 [D loss: 0.694411, acc: 34.77%] [G loss: 0.694408]\n",
      "6630 [D loss: 0.694410, acc: 37.70%] [G loss: 0.694410]\n",
      "6635 [D loss: 0.694410, acc: 37.30%] [G loss: 0.694408]\n",
      "6640 [D loss: 0.694410, acc: 36.91%] [G loss: 0.694408]\n",
      "6645 [D loss: 0.694410, acc: 37.70%] [G loss: 0.694409]\n",
      "6650 [D loss: 0.694410, acc: 38.28%] [G loss: 0.694409]\n",
      "Epoch: 6650, auc: 0.53086, ks: 0.04708267555371548\n",
      "6655 [D loss: 0.694410, acc: 37.11%] [G loss: 0.694409]\n",
      "6660 [D loss: 0.694411, acc: 38.48%] [G loss: 0.694408]\n",
      "6665 [D loss: 0.694410, acc: 39.26%] [G loss: 0.694410]\n",
      "6670 [D loss: 0.694410, acc: 35.55%] [G loss: 0.694409]\n",
      "6675 [D loss: 0.694411, acc: 35.94%] [G loss: 0.694408]\n",
      "6680 [D loss: 0.694409, acc: 38.48%] [G loss: 0.694408]\n",
      "6685 [D loss: 0.694410, acc: 36.72%] [G loss: 0.694409]\n",
      "6690 [D loss: 0.694410, acc: 37.30%] [G loss: 0.694408]\n",
      "6695 [D loss: 0.694410, acc: 38.28%] [G loss: 0.694408]\n",
      "6700 [D loss: 0.694410, acc: 39.26%] [G loss: 0.694408]\n",
      "Epoch: 6700, auc: 0.53099, ks: 0.047468644103353386\n",
      "6705 [D loss: 0.694410, acc: 37.30%] [G loss: 0.694408]\n",
      "6710 [D loss: 0.694409, acc: 36.72%] [G loss: 0.694407]\n",
      "6715 [D loss: 0.694408, acc: 35.35%] [G loss: 0.694407]\n",
      "6720 [D loss: 0.694410, acc: 37.11%] [G loss: 0.694408]\n",
      "6725 [D loss: 0.694409, acc: 38.48%] [G loss: 0.694409]\n",
      "6730 [D loss: 0.694409, acc: 36.91%] [G loss: 0.694407]\n",
      "6735 [D loss: 0.694409, acc: 35.74%] [G loss: 0.694408]\n",
      "6740 [D loss: 0.694410, acc: 34.18%] [G loss: 0.694409]\n",
      "6745 [D loss: 0.694408, acc: 39.06%] [G loss: 0.694408]\n",
      "6750 [D loss: 0.694409, acc: 38.09%] [G loss: 0.694409]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6750, auc: 0.53086, ks: 0.04761941873972009\n",
      "6755 [D loss: 0.694409, acc: 38.28%] [G loss: 0.694408]\n",
      "6760 [D loss: 0.694409, acc: 35.94%] [G loss: 0.694407]\n",
      "6765 [D loss: 0.694409, acc: 36.72%] [G loss: 0.694408]\n",
      "6770 [D loss: 0.694409, acc: 35.35%] [G loss: 0.694409]\n",
      "6775 [D loss: 0.694408, acc: 36.91%] [G loss: 0.694408]\n",
      "6780 [D loss: 0.694410, acc: 37.11%] [G loss: 0.694407]\n",
      "6785 [D loss: 0.694408, acc: 35.94%] [G loss: 0.694407]\n",
      "6790 [D loss: 0.694409, acc: 35.74%] [G loss: 0.694407]\n",
      "6795 [D loss: 0.694408, acc: 37.11%] [G loss: 0.694407]\n",
      "6800 [D loss: 0.694409, acc: 35.74%] [G loss: 0.694407]\n",
      "Epoch: 6800, auc: 0.53069, ks: 0.04752930555660839\n",
      "6805 [D loss: 0.694407, acc: 35.35%] [G loss: 0.694408]\n",
      "6810 [D loss: 0.694408, acc: 34.96%] [G loss: 0.694408]\n",
      "6815 [D loss: 0.694409, acc: 38.09%] [G loss: 0.694408]\n",
      "6820 [D loss: 0.694409, acc: 34.57%] [G loss: 0.694407]\n",
      "6825 [D loss: 0.694410, acc: 37.70%] [G loss: 0.694407]\n",
      "6830 [D loss: 0.694408, acc: 38.48%] [G loss: 0.694407]\n",
      "6835 [D loss: 0.694408, acc: 37.89%] [G loss: 0.694407]\n",
      "6840 [D loss: 0.694409, acc: 36.72%] [G loss: 0.694407]\n",
      "6845 [D loss: 0.694409, acc: 36.72%] [G loss: 0.694408]\n",
      "6850 [D loss: 0.694408, acc: 38.09%] [G loss: 0.694407]\n",
      "Epoch: 6850, auc: 0.53079, ks: 0.048116681803560546\n",
      "6855 [D loss: 0.694408, acc: 40.04%] [G loss: 0.694408]\n",
      "6860 [D loss: 0.694409, acc: 37.30%] [G loss: 0.694408]\n",
      "6865 [D loss: 0.694407, acc: 36.52%] [G loss: 0.694407]\n",
      "6870 [D loss: 0.694409, acc: 36.91%] [G loss: 0.694407]\n",
      "6875 [D loss: 0.694408, acc: 37.89%] [G loss: 0.694407]\n",
      "6880 [D loss: 0.694408, acc: 35.35%] [G loss: 0.694407]\n",
      "6885 [D loss: 0.694408, acc: 37.11%] [G loss: 0.694407]\n",
      "6890 [D loss: 0.694409, acc: 36.91%] [G loss: 0.694407]\n",
      "6895 [D loss: 0.694408, acc: 36.91%] [G loss: 0.694407]\n",
      "6900 [D loss: 0.694408, acc: 34.38%] [G loss: 0.694407]\n",
      "Epoch: 6900, auc: 0.53071, ks: 0.04759542604243949\n",
      "6905 [D loss: 0.694408, acc: 37.11%] [G loss: 0.694407]\n",
      "6910 [D loss: 0.694407, acc: 36.72%] [G loss: 0.694407]\n",
      "6915 [D loss: 0.694407, acc: 36.72%] [G loss: 0.694407]\n",
      "6920 [D loss: 0.694407, acc: 36.91%] [G loss: 0.694407]\n",
      "6925 [D loss: 0.694406, acc: 35.74%] [G loss: 0.694406]\n",
      "6930 [D loss: 0.694409, acc: 39.06%] [G loss: 0.694408]\n",
      "6935 [D loss: 0.694408, acc: 38.09%] [G loss: 0.694407]\n",
      "6940 [D loss: 0.694408, acc: 35.94%] [G loss: 0.694407]\n",
      "6945 [D loss: 0.694407, acc: 34.96%] [G loss: 0.694407]\n",
      "6950 [D loss: 0.694409, acc: 35.94%] [G loss: 0.694407]\n",
      "Epoch: 6950, auc: 0.53117, ks: 0.047784157681059924\n",
      "6955 [D loss: 0.694407, acc: 36.13%] [G loss: 0.694406]\n",
      "6960 [D loss: 0.694407, acc: 35.35%] [G loss: 0.694407]\n",
      "6965 [D loss: 0.694407, acc: 35.35%] [G loss: 0.694406]\n",
      "6970 [D loss: 0.694407, acc: 34.38%] [G loss: 0.694406]\n",
      "6975 [D loss: 0.694407, acc: 35.55%] [G loss: 0.694407]\n",
      "6980 [D loss: 0.694407, acc: 35.94%] [G loss: 0.694406]\n",
      "6985 [D loss: 0.694407, acc: 36.13%] [G loss: 0.694407]\n",
      "6990 [D loss: 0.694407, acc: 38.48%] [G loss: 0.694406]\n",
      "6995 [D loss: 0.694407, acc: 34.38%] [G loss: 0.694406]\n",
      "7000 [D loss: 0.694407, acc: 34.77%] [G loss: 0.694406]\n",
      "Epoch: 7000, auc: 0.53098, ks: 0.04786672643844714\n",
      "7005 [D loss: 0.694407, acc: 37.89%] [G loss: 0.694406]\n",
      "7010 [D loss: 0.694407, acc: 37.30%] [G loss: 0.694406]\n",
      "7015 [D loss: 0.694407, acc: 35.55%] [G loss: 0.694406]\n",
      "7020 [D loss: 0.694406, acc: 34.77%] [G loss: 0.694406]\n",
      "7025 [D loss: 0.694407, acc: 34.77%] [G loss: 0.694405]\n",
      "7030 [D loss: 0.694406, acc: 36.91%] [G loss: 0.694407]\n",
      "7035 [D loss: 0.694406, acc: 35.16%] [G loss: 0.694406]\n",
      "7040 [D loss: 0.694406, acc: 35.74%] [G loss: 0.694406]\n",
      "7045 [D loss: 0.694407, acc: 36.13%] [G loss: 0.694406]\n",
      "7050 [D loss: 0.694406, acc: 35.16%] [G loss: 0.694406]\n",
      "Epoch: 7050, auc: 0.53071, ks: 0.04749303537406846\n",
      "7055 [D loss: 0.694408, acc: 33.20%] [G loss: 0.694406]\n",
      "7060 [D loss: 0.694408, acc: 37.50%] [G loss: 0.694406]\n",
      "7065 [D loss: 0.694407, acc: 33.98%] [G loss: 0.694406]\n",
      "7070 [D loss: 0.694407, acc: 35.55%] [G loss: 0.694406]\n",
      "7075 [D loss: 0.694406, acc: 34.18%] [G loss: 0.694406]\n",
      "7080 [D loss: 0.694406, acc: 33.01%] [G loss: 0.694405]\n",
      "7085 [D loss: 0.694406, acc: 35.74%] [G loss: 0.694406]\n",
      "7090 [D loss: 0.694406, acc: 33.98%] [G loss: 0.694405]\n",
      "7095 [D loss: 0.694406, acc: 33.40%] [G loss: 0.694406]\n",
      "7100 [D loss: 0.694406, acc: 35.74%] [G loss: 0.694405]\n",
      "Epoch: 7100, auc: 0.53084, ks: 0.04777188019580059\n",
      "7105 [D loss: 0.694407, acc: 34.77%] [G loss: 0.694406]\n",
      "7110 [D loss: 0.694407, acc: 35.74%] [G loss: 0.694405]\n",
      "7115 [D loss: 0.694406, acc: 35.74%] [G loss: 0.694406]\n",
      "7120 [D loss: 0.694406, acc: 34.77%] [G loss: 0.694405]\n",
      "7125 [D loss: 0.694406, acc: 35.16%] [G loss: 0.694405]\n",
      "7130 [D loss: 0.694406, acc: 34.96%] [G loss: 0.694406]\n",
      "7135 [D loss: 0.694406, acc: 36.72%] [G loss: 0.694407]\n",
      "7140 [D loss: 0.694406, acc: 34.96%] [G loss: 0.694405]\n",
      "7145 [D loss: 0.694406, acc: 34.57%] [G loss: 0.694406]\n",
      "7150 [D loss: 0.694406, acc: 33.98%] [G loss: 0.694405]\n",
      "Epoch: 7150, auc: 0.53101, ks: 0.04747450170936396\n",
      "7155 [D loss: 0.694406, acc: 31.84%] [G loss: 0.694405]\n",
      "7160 [D loss: 0.694406, acc: 33.59%] [G loss: 0.694405]\n",
      "7165 [D loss: 0.694406, acc: 35.74%] [G loss: 0.694405]\n",
      "7170 [D loss: 0.694406, acc: 36.52%] [G loss: 0.694405]\n",
      "7175 [D loss: 0.694405, acc: 33.01%] [G loss: 0.694405]\n",
      "7180 [D loss: 0.694406, acc: 33.59%] [G loss: 0.694405]\n",
      "7185 [D loss: 0.694406, acc: 34.57%] [G loss: 0.694405]\n",
      "7190 [D loss: 0.694406, acc: 33.79%] [G loss: 0.694405]\n",
      "7195 [D loss: 0.694406, acc: 33.20%] [G loss: 0.694405]\n",
      "7200 [D loss: 0.694407, acc: 34.77%] [G loss: 0.694405]\n",
      "Epoch: 7200, auc: 0.53094, ks: 0.04754856519435424\n",
      "7205 [D loss: 0.694406, acc: 34.38%] [G loss: 0.694405]\n",
      "7210 [D loss: 0.694405, acc: 34.57%] [G loss: 0.694405]\n",
      "7215 [D loss: 0.694406, acc: 33.98%] [G loss: 0.694405]\n",
      "7220 [D loss: 0.694405, acc: 31.05%] [G loss: 0.694405]\n",
      "7225 [D loss: 0.694406, acc: 34.38%] [G loss: 0.694405]\n",
      "7230 [D loss: 0.694406, acc: 32.42%] [G loss: 0.694405]\n",
      "7235 [D loss: 0.694405, acc: 32.81%] [G loss: 0.694405]\n",
      "7240 [D loss: 0.694405, acc: 32.42%] [G loss: 0.694405]\n",
      "7245 [D loss: 0.694405, acc: 34.57%] [G loss: 0.694405]\n",
      "7250 [D loss: 0.694405, acc: 33.01%] [G loss: 0.694404]\n",
      "Epoch: 7250, auc: 0.53051, ks: 0.04743879380006211\n",
      "7255 [D loss: 0.694406, acc: 34.18%] [G loss: 0.694405]\n",
      "7260 [D loss: 0.694406, acc: 33.98%] [G loss: 0.694405]\n",
      "7265 [D loss: 0.694406, acc: 35.16%] [G loss: 0.694404]\n",
      "7270 [D loss: 0.694405, acc: 33.20%] [G loss: 0.694405]\n",
      "7275 [D loss: 0.694406, acc: 32.42%] [G loss: 0.694405]\n",
      "7280 [D loss: 0.694406, acc: 34.77%] [G loss: 0.694405]\n",
      "7285 [D loss: 0.694405, acc: 33.40%] [G loss: 0.694405]\n",
      "7290 [D loss: 0.694405, acc: 33.98%] [G loss: 0.694404]\n",
      "7295 [D loss: 0.694405, acc: 33.40%] [G loss: 0.694405]\n",
      "7300 [D loss: 0.694405, acc: 33.59%] [G loss: 0.694404]\n",
      "Epoch: 7300, auc: 0.53066, ks: 0.047385513072728314\n",
      "7305 [D loss: 0.694405, acc: 33.79%] [G loss: 0.694405]\n",
      "7310 [D loss: 0.694405, acc: 33.01%] [G loss: 0.694404]\n",
      "7315 [D loss: 0.694405, acc: 34.77%] [G loss: 0.694405]\n",
      "7320 [D loss: 0.694405, acc: 35.35%] [G loss: 0.694405]\n",
      "7325 [D loss: 0.694405, acc: 34.57%] [G loss: 0.694404]\n",
      "7330 [D loss: 0.694404, acc: 32.62%] [G loss: 0.694404]\n",
      "7335 [D loss: 0.694405, acc: 33.59%] [G loss: 0.694404]\n",
      "7340 [D loss: 0.694405, acc: 34.57%] [G loss: 0.694405]\n",
      "7345 [D loss: 0.694405, acc: 33.59%] [G loss: 0.694405]\n",
      "7350 [D loss: 0.694405, acc: 34.18%] [G loss: 0.694404]\n",
      "Epoch: 7350, auc: 0.53065, ks: 0.048230787683952725\n",
      "7355 [D loss: 0.694405, acc: 35.35%] [G loss: 0.694404]\n",
      "7360 [D loss: 0.694405, acc: 34.38%] [G loss: 0.694404]\n",
      "7365 [D loss: 0.694404, acc: 33.59%] [G loss: 0.694404]\n",
      "7370 [D loss: 0.694405, acc: 33.59%] [G loss: 0.694404]\n",
      "7375 [D loss: 0.694405, acc: 31.45%] [G loss: 0.694404]\n",
      "7380 [D loss: 0.694405, acc: 31.64%] [G loss: 0.694404]\n",
      "7385 [D loss: 0.694404, acc: 33.20%] [G loss: 0.694404]\n",
      "7390 [D loss: 0.694404, acc: 33.59%] [G loss: 0.694404]\n",
      "7395 [D loss: 0.694405, acc: 34.18%] [G loss: 0.694404]\n",
      "7400 [D loss: 0.694405, acc: 33.59%] [G loss: 0.694405]\n",
      "Epoch: 7400, auc: 0.53055, ks: 0.04753155464956016\n",
      "7405 [D loss: 0.694405, acc: 32.23%] [G loss: 0.694404]\n",
      "7410 [D loss: 0.694404, acc: 31.64%] [G loss: 0.694404]\n",
      "7415 [D loss: 0.694405, acc: 36.52%] [G loss: 0.694404]\n",
      "7420 [D loss: 0.694405, acc: 31.25%] [G loss: 0.694404]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7425 [D loss: 0.694405, acc: 31.84%] [G loss: 0.694404]\n",
      "7430 [D loss: 0.694405, acc: 30.47%] [G loss: 0.694404]\n",
      "7435 [D loss: 0.694404, acc: 33.20%] [G loss: 0.694404]\n",
      "7440 [D loss: 0.694404, acc: 33.98%] [G loss: 0.694404]\n",
      "7445 [D loss: 0.694405, acc: 32.42%] [G loss: 0.694404]\n",
      "7450 [D loss: 0.694405, acc: 33.20%] [G loss: 0.694404]\n",
      "Epoch: 7450, auc: 0.53047, ks: 0.04705828428300041\n",
      "7455 [D loss: 0.694404, acc: 30.86%] [G loss: 0.694404]\n",
      "7460 [D loss: 0.694404, acc: 33.79%] [G loss: 0.694404]\n",
      "7465 [D loss: 0.694405, acc: 31.45%] [G loss: 0.694404]\n",
      "7470 [D loss: 0.694404, acc: 31.05%] [G loss: 0.694404]\n",
      "7475 [D loss: 0.694404, acc: 31.45%] [G loss: 0.694404]\n",
      "7480 [D loss: 0.694404, acc: 33.40%] [G loss: 0.694404]\n",
      "7485 [D loss: 0.694404, acc: 32.03%] [G loss: 0.694404]\n",
      "7490 [D loss: 0.694404, acc: 30.66%] [G loss: 0.694404]\n",
      "7495 [D loss: 0.694404, acc: 33.20%] [G loss: 0.694404]\n",
      "7500 [D loss: 0.694404, acc: 30.47%] [G loss: 0.694404]\n",
      "Epoch: 7500, auc: 0.53050, ks: 0.047115337223196496\n",
      "7505 [D loss: 0.694404, acc: 32.23%] [G loss: 0.694404]\n",
      "7510 [D loss: 0.694404, acc: 31.05%] [G loss: 0.694404]\n",
      "7515 [D loss: 0.694404, acc: 30.66%] [G loss: 0.694404]\n",
      "7520 [D loss: 0.694404, acc: 33.59%] [G loss: 0.694404]\n",
      "7525 [D loss: 0.694404, acc: 33.01%] [G loss: 0.694403]\n",
      "7530 [D loss: 0.694404, acc: 32.42%] [G loss: 0.694404]\n",
      "7535 [D loss: 0.694404, acc: 29.30%] [G loss: 0.694404]\n",
      "7540 [D loss: 0.694404, acc: 30.08%] [G loss: 0.694403]\n",
      "7545 [D loss: 0.694404, acc: 31.05%] [G loss: 0.694404]\n",
      "7550 [D loss: 0.694404, acc: 30.86%] [G loss: 0.694404]\n",
      "Epoch: 7550, auc: 0.53066, ks: 0.047867850984922966\n",
      "7555 [D loss: 0.694404, acc: 31.45%] [G loss: 0.694404]\n",
      "7560 [D loss: 0.694404, acc: 32.23%] [G loss: 0.694403]\n",
      "7565 [D loss: 0.694404, acc: 30.27%] [G loss: 0.694403]\n",
      "7570 [D loss: 0.694403, acc: 32.23%] [G loss: 0.694404]\n",
      "7575 [D loss: 0.694403, acc: 29.30%] [G loss: 0.694403]\n",
      "7580 [D loss: 0.694404, acc: 31.45%] [G loss: 0.694403]\n",
      "7585 [D loss: 0.694404, acc: 30.86%] [G loss: 0.694403]\n",
      "7590 [D loss: 0.694404, acc: 31.25%] [G loss: 0.694403]\n",
      "7595 [D loss: 0.694404, acc: 32.62%] [G loss: 0.694403]\n",
      "7600 [D loss: 0.694404, acc: 31.05%] [G loss: 0.694403]\n",
      "Epoch: 7600, auc: 0.53080, ks: 0.04752304937716323\n",
      "7605 [D loss: 0.694403, acc: 30.27%] [G loss: 0.694403]\n",
      "7610 [D loss: 0.694403, acc: 30.66%] [G loss: 0.694403]\n",
      "7615 [D loss: 0.694403, acc: 29.49%] [G loss: 0.694403]\n",
      "7620 [D loss: 0.694404, acc: 30.66%] [G loss: 0.694403]\n",
      "7625 [D loss: 0.694404, acc: 30.27%] [G loss: 0.694403]\n",
      "7630 [D loss: 0.694404, acc: 30.08%] [G loss: 0.694403]\n",
      "7635 [D loss: 0.694404, acc: 28.71%] [G loss: 0.694404]\n",
      "7640 [D loss: 0.694404, acc: 32.62%] [G loss: 0.694403]\n",
      "7645 [D loss: 0.694404, acc: 33.20%] [G loss: 0.694403]\n",
      "7650 [D loss: 0.694404, acc: 32.03%] [G loss: 0.694403]\n",
      "Epoch: 7650, auc: 0.53081, ks: 0.04741592564925745\n",
      "7655 [D loss: 0.694404, acc: 32.42%] [G loss: 0.694403]\n",
      "7660 [D loss: 0.694403, acc: 30.27%] [G loss: 0.694403]\n",
      "7665 [D loss: 0.694404, acc: 30.08%] [G loss: 0.694403]\n",
      "7670 [D loss: 0.694403, acc: 30.08%] [G loss: 0.694403]\n",
      "7675 [D loss: 0.694403, acc: 28.91%] [G loss: 0.694403]\n",
      "7680 [D loss: 0.694403, acc: 29.49%] [G loss: 0.694403]\n",
      "7685 [D loss: 0.694404, acc: 29.30%] [G loss: 0.694403]\n",
      "7690 [D loss: 0.694403, acc: 31.45%] [G loss: 0.694403]\n",
      "7695 [D loss: 0.694403, acc: 30.27%] [G loss: 0.694403]\n",
      "7700 [D loss: 0.694403, acc: 31.45%] [G loss: 0.694403]\n",
      "Epoch: 7700, auc: 0.53062, ks: 0.04721018346584305\n",
      "7705 [D loss: 0.694404, acc: 28.71%] [G loss: 0.694403]\n",
      "7710 [D loss: 0.694404, acc: 31.64%] [G loss: 0.694403]\n",
      "7715 [D loss: 0.694403, acc: 28.32%] [G loss: 0.694403]\n",
      "7720 [D loss: 0.694404, acc: 35.16%] [G loss: 0.694403]\n",
      "7725 [D loss: 0.694403, acc: 29.30%] [G loss: 0.694403]\n",
      "7730 [D loss: 0.694403, acc: 31.05%] [G loss: 0.694403]\n",
      "7735 [D loss: 0.694404, acc: 31.64%] [G loss: 0.694403]\n",
      "7740 [D loss: 0.694403, acc: 28.32%] [G loss: 0.694403]\n",
      "7745 [D loss: 0.694403, acc: 30.86%] [G loss: 0.694403]\n",
      "7750 [D loss: 0.694403, acc: 30.08%] [G loss: 0.694403]\n",
      "Epoch: 7750, auc: 0.53083, ks: 0.04776016498377933\n",
      "7755 [D loss: 0.694403, acc: 30.47%] [G loss: 0.694403]\n",
      "7760 [D loss: 0.694403, acc: 27.15%] [G loss: 0.694402]\n",
      "7765 [D loss: 0.694403, acc: 29.88%] [G loss: 0.694403]\n",
      "7770 [D loss: 0.694403, acc: 28.91%] [G loss: 0.694403]\n",
      "7775 [D loss: 0.694403, acc: 29.10%] [G loss: 0.694403]\n",
      "7780 [D loss: 0.694403, acc: 32.23%] [G loss: 0.694402]\n",
      "7785 [D loss: 0.694403, acc: 28.32%] [G loss: 0.694403]\n",
      "7790 [D loss: 0.694403, acc: 30.08%] [G loss: 0.694403]\n",
      "7795 [D loss: 0.694403, acc: 30.66%] [G loss: 0.694402]\n",
      "7800 [D loss: 0.694403, acc: 31.45%] [G loss: 0.694403]\n",
      "Epoch: 7800, auc: 0.53105, ks: 0.04752304937716323\n",
      "7805 [D loss: 0.694403, acc: 28.52%] [G loss: 0.694403]\n",
      "7810 [D loss: 0.694403, acc: 31.05%] [G loss: 0.694402]\n",
      "7815 [D loss: 0.694403, acc: 27.93%] [G loss: 0.694403]\n",
      "7820 [D loss: 0.694403, acc: 29.69%] [G loss: 0.694402]\n",
      "7825 [D loss: 0.694403, acc: 29.10%] [G loss: 0.694403]\n",
      "7830 [D loss: 0.694403, acc: 28.71%] [G loss: 0.694403]\n",
      "7835 [D loss: 0.694403, acc: 31.05%] [G loss: 0.694403]\n",
      "7840 [D loss: 0.694403, acc: 30.08%] [G loss: 0.694402]\n",
      "7845 [D loss: 0.694403, acc: 29.10%] [G loss: 0.694403]\n",
      "7850 [D loss: 0.694403, acc: 27.34%] [G loss: 0.694402]\n",
      "Epoch: 7850, auc: 0.53080, ks: 0.04762190270630284\n",
      "7855 [D loss: 0.694402, acc: 28.32%] [G loss: 0.694403]\n",
      "7860 [D loss: 0.694403, acc: 28.71%] [G loss: 0.694402]\n",
      "7865 [D loss: 0.694403, acc: 30.27%] [G loss: 0.694402]\n",
      "7870 [D loss: 0.694403, acc: 29.10%] [G loss: 0.694402]\n",
      "7875 [D loss: 0.694403, acc: 31.45%] [G loss: 0.694402]\n",
      "7880 [D loss: 0.694402, acc: 26.95%] [G loss: 0.694403]\n",
      "7885 [D loss: 0.694403, acc: 28.32%] [G loss: 0.694402]\n",
      "7890 [D loss: 0.694403, acc: 29.10%] [G loss: 0.694402]\n",
      "7895 [D loss: 0.694403, acc: 29.49%] [G loss: 0.694403]\n",
      "7900 [D loss: 0.694402, acc: 26.76%] [G loss: 0.694402]\n",
      "Epoch: 7900, auc: 0.53097, ks: 0.04727068121929445\n",
      "7905 [D loss: 0.694403, acc: 30.08%] [G loss: 0.694402]\n",
      "7910 [D loss: 0.694403, acc: 28.32%] [G loss: 0.694402]\n",
      "7915 [D loss: 0.694402, acc: 27.93%] [G loss: 0.694402]\n",
      "7920 [D loss: 0.694403, acc: 28.52%] [G loss: 0.694402]\n",
      "7925 [D loss: 0.694402, acc: 29.10%] [G loss: 0.694402]\n",
      "7930 [D loss: 0.694402, acc: 30.27%] [G loss: 0.694402]\n",
      "7935 [D loss: 0.694402, acc: 28.71%] [G loss: 0.694402]\n",
      "7940 [D loss: 0.694402, acc: 26.37%] [G loss: 0.694402]\n",
      "7945 [D loss: 0.694403, acc: 30.08%] [G loss: 0.694402]\n",
      "7950 [D loss: 0.694402, acc: 28.52%] [G loss: 0.694402]\n",
      "Epoch: 7950, auc: 0.53096, ks: 0.047650955862725164\n",
      "7955 [D loss: 0.694402, acc: 26.17%] [G loss: 0.694402]\n",
      "7960 [D loss: 0.694402, acc: 28.71%] [G loss: 0.694402]\n",
      "7965 [D loss: 0.694403, acc: 28.52%] [G loss: 0.694402]\n",
      "7970 [D loss: 0.694403, acc: 28.71%] [G loss: 0.694402]\n",
      "7975 [D loss: 0.694403, acc: 28.52%] [G loss: 0.694402]\n",
      "7980 [D loss: 0.694403, acc: 29.10%] [G loss: 0.694402]\n",
      "7985 [D loss: 0.694402, acc: 27.54%] [G loss: 0.694402]\n",
      "7990 [D loss: 0.694402, acc: 25.59%] [G loss: 0.694402]\n",
      "7995 [D loss: 0.694402, acc: 24.02%] [G loss: 0.694402]\n",
      "8000 [D loss: 0.694403, acc: 26.76%] [G loss: 0.694402]\n",
      "Epoch: 8000, auc: 0.53082, ks: 0.04658918470273732\n",
      "8005 [D loss: 0.694402, acc: 25.78%] [G loss: 0.694402]\n",
      "8010 [D loss: 0.694402, acc: 27.54%] [G loss: 0.694402]\n",
      "8015 [D loss: 0.694402, acc: 31.05%] [G loss: 0.694402]\n",
      "8020 [D loss: 0.694402, acc: 29.49%] [G loss: 0.694402]\n",
      "8025 [D loss: 0.694402, acc: 26.76%] [G loss: 0.694402]\n",
      "8030 [D loss: 0.694402, acc: 25.98%] [G loss: 0.694402]\n",
      "8035 [D loss: 0.694402, acc: 27.34%] [G loss: 0.694402]\n",
      "8040 [D loss: 0.694403, acc: 27.93%] [G loss: 0.694402]\n",
      "8045 [D loss: 0.694402, acc: 27.93%] [G loss: 0.694402]\n",
      "8050 [D loss: 0.694402, acc: 29.30%] [G loss: 0.694402]\n",
      "Epoch: 8050, auc: 0.53102, ks: 0.046650572129033785\n",
      "8055 [D loss: 0.694402, acc: 25.98%] [G loss: 0.694402]\n",
      "8060 [D loss: 0.694403, acc: 27.34%] [G loss: 0.694402]\n",
      "8065 [D loss: 0.694402, acc: 26.76%] [G loss: 0.694402]\n",
      "8070 [D loss: 0.694402, acc: 25.98%] [G loss: 0.694402]\n",
      "8075 [D loss: 0.694402, acc: 27.73%] [G loss: 0.694402]\n",
      "8080 [D loss: 0.694402, acc: 25.78%] [G loss: 0.694402]\n",
      "8085 [D loss: 0.694402, acc: 27.15%] [G loss: 0.694402]\n",
      "8090 [D loss: 0.694402, acc: 25.98%] [G loss: 0.694402]\n",
      "8095 [D loss: 0.694402, acc: 26.56%] [G loss: 0.694402]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8100 [D loss: 0.694402, acc: 26.56%] [G loss: 0.694402]\n",
      "Epoch: 8100, auc: 0.53074, ks: 0.04649546300656682\n",
      "8105 [D loss: 0.694402, acc: 26.17%] [G loss: 0.694402]\n",
      "8110 [D loss: 0.694402, acc: 28.91%] [G loss: 0.694402]\n",
      "8115 [D loss: 0.694402, acc: 27.15%] [G loss: 0.694402]\n",
      "8120 [D loss: 0.694402, acc: 26.95%] [G loss: 0.694402]\n",
      "8125 [D loss: 0.694402, acc: 26.17%] [G loss: 0.694402]\n",
      "8130 [D loss: 0.694402, acc: 26.76%] [G loss: 0.694402]\n",
      "8135 [D loss: 0.694402, acc: 26.37%] [G loss: 0.694402]\n",
      "8140 [D loss: 0.694402, acc: 24.61%] [G loss: 0.694401]\n",
      "8145 [D loss: 0.694402, acc: 24.22%] [G loss: 0.694402]\n",
      "8150 [D loss: 0.694402, acc: 25.20%] [G loss: 0.694402]\n",
      "Epoch: 8150, auc: 0.53097, ks: 0.04647339200263112\n",
      "8155 [D loss: 0.694402, acc: 25.00%] [G loss: 0.694402]\n",
      "8160 [D loss: 0.694402, acc: 26.95%] [G loss: 0.694402]\n",
      "8165 [D loss: 0.694402, acc: 25.98%] [G loss: 0.694402]\n",
      "8170 [D loss: 0.694402, acc: 26.17%] [G loss: 0.694402]\n",
      "8175 [D loss: 0.694402, acc: 24.80%] [G loss: 0.694402]\n",
      "8180 [D loss: 0.694402, acc: 25.20%] [G loss: 0.694402]\n",
      "8185 [D loss: 0.694402, acc: 24.41%] [G loss: 0.694402]\n",
      "8190 [D loss: 0.694402, acc: 25.98%] [G loss: 0.694402]\n",
      "8195 [D loss: 0.694402, acc: 26.17%] [G loss: 0.694402]\n",
      "8200 [D loss: 0.694402, acc: 26.56%] [G loss: 0.694402]\n",
      "Epoch: 8200, auc: 0.53084, ks: 0.046304645974798\n",
      "8205 [D loss: 0.694402, acc: 26.37%] [G loss: 0.694402]\n",
      "8210 [D loss: 0.694402, acc: 26.37%] [G loss: 0.694402]\n",
      "8215 [D loss: 0.694402, acc: 25.39%] [G loss: 0.694402]\n",
      "8220 [D loss: 0.694402, acc: 25.59%] [G loss: 0.694402]\n",
      "8225 [D loss: 0.694402, acc: 27.54%] [G loss: 0.694401]\n",
      "8230 [D loss: 0.694402, acc: 24.80%] [G loss: 0.694402]\n",
      "8235 [D loss: 0.694402, acc: 25.98%] [G loss: 0.694402]\n",
      "8240 [D loss: 0.694402, acc: 25.59%] [G loss: 0.694402]\n",
      "8245 [D loss: 0.694402, acc: 23.83%] [G loss: 0.694402]\n",
      "8250 [D loss: 0.694402, acc: 25.20%] [G loss: 0.694402]\n",
      "Epoch: 8250, auc: 0.53074, ks: 0.04676170294343274\n",
      "8255 [D loss: 0.694402, acc: 24.02%] [G loss: 0.694402]\n",
      "8260 [D loss: 0.694402, acc: 24.80%] [G loss: 0.694402]\n",
      "8265 [D loss: 0.694402, acc: 23.83%] [G loss: 0.694401]\n",
      "8270 [D loss: 0.694402, acc: 24.61%] [G loss: 0.694402]\n",
      "8275 [D loss: 0.694402, acc: 26.17%] [G loss: 0.694401]\n",
      "8280 [D loss: 0.694402, acc: 25.78%] [G loss: 0.694401]\n",
      "8285 [D loss: 0.694402, acc: 24.22%] [G loss: 0.694402]\n",
      "8290 [D loss: 0.694402, acc: 23.44%] [G loss: 0.694401]\n",
      "8295 [D loss: 0.694402, acc: 24.80%] [G loss: 0.694401]\n",
      "8300 [D loss: 0.694402, acc: 24.22%] [G loss: 0.694401]\n",
      "Epoch: 8300, auc: 0.53062, ks: 0.04688960942899478\n",
      "8305 [D loss: 0.694402, acc: 25.00%] [G loss: 0.694401]\n",
      "8310 [D loss: 0.694401, acc: 23.83%] [G loss: 0.694401]\n",
      "8315 [D loss: 0.694401, acc: 23.44%] [G loss: 0.694401]\n",
      "8320 [D loss: 0.694402, acc: 23.63%] [G loss: 0.694401]\n",
      "8325 [D loss: 0.694402, acc: 24.22%] [G loss: 0.694401]\n",
      "8330 [D loss: 0.694402, acc: 24.80%] [G loss: 0.694401]\n",
      "8335 [D loss: 0.694401, acc: 22.07%] [G loss: 0.694401]\n",
      "8340 [D loss: 0.694402, acc: 25.59%] [G loss: 0.694401]\n",
      "8345 [D loss: 0.694401, acc: 23.44%] [G loss: 0.694401]\n",
      "8350 [D loss: 0.694402, acc: 24.02%] [G loss: 0.694402]\n",
      "Epoch: 8350, auc: 0.53069, ks: 0.0471227179491176\n",
      "8355 [D loss: 0.694402, acc: 23.63%] [G loss: 0.694401]\n",
      "8360 [D loss: 0.694401, acc: 24.61%] [G loss: 0.694401]\n",
      "8365 [D loss: 0.694401, acc: 23.05%] [G loss: 0.694401]\n",
      "8370 [D loss: 0.694401, acc: 21.88%] [G loss: 0.694401]\n",
      "8375 [D loss: 0.694402, acc: 25.00%] [G loss: 0.694402]\n",
      "8380 [D loss: 0.694401, acc: 23.63%] [G loss: 0.694401]\n",
      "8385 [D loss: 0.694401, acc: 24.02%] [G loss: 0.694401]\n",
      "8390 [D loss: 0.694401, acc: 22.85%] [G loss: 0.694401]\n",
      "8395 [D loss: 0.694401, acc: 22.66%] [G loss: 0.694401]\n",
      "8400 [D loss: 0.694401, acc: 23.05%] [G loss: 0.694401]\n",
      "Epoch: 8400, auc: 0.53068, ks: 0.046839702341088696\n",
      "8405 [D loss: 0.694401, acc: 23.83%] [G loss: 0.694401]\n",
      "8410 [D loss: 0.694401, acc: 23.63%] [G loss: 0.694401]\n",
      "8415 [D loss: 0.694401, acc: 23.83%] [G loss: 0.694401]\n",
      "8420 [D loss: 0.694401, acc: 24.02%] [G loss: 0.694401]\n",
      "8425 [D loss: 0.694402, acc: 24.02%] [G loss: 0.694401]\n",
      "8430 [D loss: 0.694401, acc: 23.44%] [G loss: 0.694401]\n",
      "8435 [D loss: 0.694401, acc: 22.46%] [G loss: 0.694401]\n",
      "8440 [D loss: 0.694401, acc: 23.83%] [G loss: 0.694401]\n",
      "8445 [D loss: 0.694401, acc: 23.24%] [G loss: 0.694401]\n",
      "8450 [D loss: 0.694401, acc: 23.63%] [G loss: 0.694401]\n",
      "Epoch: 8450, auc: 0.53081, ks: 0.04617088188322538\n",
      "8455 [D loss: 0.694401, acc: 26.37%] [G loss: 0.694401]\n",
      "8460 [D loss: 0.694401, acc: 24.02%] [G loss: 0.694401]\n",
      "8465 [D loss: 0.694401, acc: 23.24%] [G loss: 0.694401]\n",
      "8470 [D loss: 0.694401, acc: 22.66%] [G loss: 0.694401]\n",
      "8475 [D loss: 0.694401, acc: 24.22%] [G loss: 0.694401]\n",
      "8480 [D loss: 0.694402, acc: 23.83%] [G loss: 0.694401]\n",
      "8485 [D loss: 0.694401, acc: 21.09%] [G loss: 0.694401]\n",
      "8490 [D loss: 0.694401, acc: 22.27%] [G loss: 0.694401]\n",
      "8495 [D loss: 0.694401, acc: 21.88%] [G loss: 0.694401]\n",
      "8500 [D loss: 0.694401, acc: 21.29%] [G loss: 0.694401]\n",
      "Epoch: 8500, auc: 0.53073, ks: 0.04657144818490189\n",
      "8505 [D loss: 0.694401, acc: 22.46%] [G loss: 0.694401]\n",
      "8510 [D loss: 0.694401, acc: 22.46%] [G loss: 0.694401]\n",
      "8515 [D loss: 0.694401, acc: 21.68%] [G loss: 0.694401]\n",
      "8520 [D loss: 0.694401, acc: 22.46%] [G loss: 0.694401]\n",
      "8525 [D loss: 0.694401, acc: 21.48%] [G loss: 0.694401]\n",
      "8530 [D loss: 0.694401, acc: 21.29%] [G loss: 0.694401]\n",
      "8535 [D loss: 0.694401, acc: 22.46%] [G loss: 0.694401]\n",
      "8540 [D loss: 0.694401, acc: 19.73%] [G loss: 0.694401]\n",
      "8545 [D loss: 0.694401, acc: 22.07%] [G loss: 0.694401]\n",
      "8550 [D loss: 0.694401, acc: 21.48%] [G loss: 0.694401]\n",
      "Epoch: 8550, auc: 0.53058, ks: 0.04593264173013323\n",
      "8555 [D loss: 0.694401, acc: 22.46%] [G loss: 0.694401]\n",
      "8560 [D loss: 0.694401, acc: 22.85%] [G loss: 0.694401]\n",
      "8565 [D loss: 0.694401, acc: 23.63%] [G loss: 0.694401]\n",
      "8570 [D loss: 0.694401, acc: 19.53%] [G loss: 0.694401]\n",
      "8575 [D loss: 0.694401, acc: 19.14%] [G loss: 0.694401]\n",
      "8580 [D loss: 0.694401, acc: 20.31%] [G loss: 0.694401]\n",
      "8585 [D loss: 0.694401, acc: 22.27%] [G loss: 0.694401]\n",
      "8590 [D loss: 0.694401, acc: 22.66%] [G loss: 0.694401]\n",
      "8595 [D loss: 0.694401, acc: 21.09%] [G loss: 0.694401]\n",
      "8600 [D loss: 0.694401, acc: 19.34%] [G loss: 0.694401]\n",
      "Epoch: 8600, auc: 0.53043, ks: 0.04552453100273213\n",
      "8605 [D loss: 0.694401, acc: 21.29%] [G loss: 0.694401]\n",
      "8610 [D loss: 0.694401, acc: 21.29%] [G loss: 0.694401]\n",
      "8615 [D loss: 0.694401, acc: 17.97%] [G loss: 0.694401]\n",
      "8620 [D loss: 0.694401, acc: 22.27%] [G loss: 0.694401]\n",
      "8625 [D loss: 0.694401, acc: 22.07%] [G loss: 0.694401]\n",
      "8630 [D loss: 0.694401, acc: 21.88%] [G loss: 0.694401]\n",
      "8635 [D loss: 0.694401, acc: 23.44%] [G loss: 0.694401]\n",
      "8640 [D loss: 0.694401, acc: 20.90%] [G loss: 0.694401]\n",
      "8645 [D loss: 0.694401, acc: 21.88%] [G loss: 0.694401]\n",
      "8650 [D loss: 0.694401, acc: 20.70%] [G loss: 0.694401]\n",
      "Epoch: 8650, auc: 0.53030, ks: 0.046175216369325534\n",
      "8655 [D loss: 0.694401, acc: 22.07%] [G loss: 0.694401]\n",
      "8660 [D loss: 0.694401, acc: 22.46%] [G loss: 0.694401]\n",
      "8665 [D loss: 0.694401, acc: 19.92%] [G loss: 0.694401]\n",
      "8670 [D loss: 0.694401, acc: 20.70%] [G loss: 0.694401]\n",
      "8675 [D loss: 0.694401, acc: 21.09%] [G loss: 0.694401]\n",
      "8680 [D loss: 0.694401, acc: 19.14%] [G loss: 0.694401]\n",
      "8685 [D loss: 0.694401, acc: 22.07%] [G loss: 0.694401]\n",
      "8690 [D loss: 0.694401, acc: 18.16%] [G loss: 0.694401]\n",
      "8695 [D loss: 0.694401, acc: 20.90%] [G loss: 0.694401]\n",
      "8700 [D loss: 0.694401, acc: 21.29%] [G loss: 0.694401]\n",
      "Epoch: 8700, auc: 0.53027, ks: 0.046258582273581705\n",
      "8705 [D loss: 0.694401, acc: 19.53%] [G loss: 0.694401]\n",
      "8710 [D loss: 0.694401, acc: 19.92%] [G loss: 0.694401]\n",
      "8715 [D loss: 0.694401, acc: 18.75%] [G loss: 0.694401]\n",
      "8720 [D loss: 0.694401, acc: 21.48%] [G loss: 0.694401]\n",
      "8725 [D loss: 0.694401, acc: 21.88%] [G loss: 0.694401]\n",
      "8730 [D loss: 0.694401, acc: 21.48%] [G loss: 0.694401]\n",
      "8735 [D loss: 0.694401, acc: 21.48%] [G loss: 0.694401]\n",
      "8740 [D loss: 0.694401, acc: 16.21%] [G loss: 0.694401]\n",
      "8745 [D loss: 0.694401, acc: 18.16%] [G loss: 0.694401]\n",
      "8750 [D loss: 0.694401, acc: 18.55%] [G loss: 0.694401]\n",
      "Epoch: 8750, auc: 0.53024, ks: 0.04625248979394003\n",
      "8755 [D loss: 0.694401, acc: 20.90%] [G loss: 0.694401]\n",
      "8760 [D loss: 0.694401, acc: 18.36%] [G loss: 0.694401]\n",
      "8765 [D loss: 0.694401, acc: 20.31%] [G loss: 0.694401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8770 [D loss: 0.694401, acc: 20.12%] [G loss: 0.694401]\n",
      "8775 [D loss: 0.694401, acc: 17.38%] [G loss: 0.694401]\n",
      "8780 [D loss: 0.694401, acc: 17.38%] [G loss: 0.694401]\n",
      "8785 [D loss: 0.694401, acc: 20.12%] [G loss: 0.694401]\n",
      "8790 [D loss: 0.694401, acc: 23.63%] [G loss: 0.694401]\n",
      "8795 [D loss: 0.694401, acc: 19.53%] [G loss: 0.694401]\n",
      "8800 [D loss: 0.694401, acc: 19.14%] [G loss: 0.694401]\n",
      "Epoch: 8800, auc: 0.53036, ks: 0.04557195412405535\n",
      "8805 [D loss: 0.694401, acc: 19.53%] [G loss: 0.694401]\n",
      "8810 [D loss: 0.694401, acc: 18.36%] [G loss: 0.694401]\n",
      "8815 [D loss: 0.694401, acc: 19.34%] [G loss: 0.694401]\n",
      "8820 [D loss: 0.694401, acc: 18.95%] [G loss: 0.694401]\n",
      "8825 [D loss: 0.694401, acc: 18.75%] [G loss: 0.694401]\n",
      "8830 [D loss: 0.694401, acc: 20.90%] [G loss: 0.694401]\n",
      "8835 [D loss: 0.694401, acc: 19.92%] [G loss: 0.694401]\n",
      "8840 [D loss: 0.694401, acc: 18.55%] [G loss: 0.694401]\n",
      "8845 [D loss: 0.694401, acc: 17.97%] [G loss: 0.694400]\n",
      "8850 [D loss: 0.694401, acc: 18.36%] [G loss: 0.694401]\n",
      "Epoch: 8850, auc: 0.53040, ks: 0.046250077001184664\n",
      "8855 [D loss: 0.694401, acc: 19.14%] [G loss: 0.694401]\n",
      "8860 [D loss: 0.694401, acc: 16.60%] [G loss: 0.694401]\n",
      "8865 [D loss: 0.694401, acc: 19.53%] [G loss: 0.694401]\n",
      "8870 [D loss: 0.694401, acc: 18.95%] [G loss: 0.694400]\n",
      "8875 [D loss: 0.694401, acc: 17.97%] [G loss: 0.694401]\n",
      "8880 [D loss: 0.694401, acc: 16.80%] [G loss: 0.694401]\n",
      "8885 [D loss: 0.694401, acc: 16.21%] [G loss: 0.694401]\n",
      "8890 [D loss: 0.694401, acc: 18.55%] [G loss: 0.694401]\n",
      "8895 [D loss: 0.694401, acc: 16.60%] [G loss: 0.694401]\n",
      "8900 [D loss: 0.694401, acc: 17.19%] [G loss: 0.694401]\n",
      "Epoch: 8900, auc: 0.53044, ks: 0.04610933075712542\n",
      "8905 [D loss: 0.694401, acc: 17.97%] [G loss: 0.694401]\n",
      "8910 [D loss: 0.694401, acc: 16.99%] [G loss: 0.694401]\n",
      "8915 [D loss: 0.694401, acc: 17.19%] [G loss: 0.694401]\n",
      "8920 [D loss: 0.694401, acc: 17.19%] [G loss: 0.694401]\n",
      "8925 [D loss: 0.694401, acc: 18.55%] [G loss: 0.694401]\n",
      "8930 [D loss: 0.694401, acc: 17.58%] [G loss: 0.694400]\n",
      "8935 [D loss: 0.694401, acc: 17.38%] [G loss: 0.694401]\n",
      "8940 [D loss: 0.694401, acc: 17.19%] [G loss: 0.694401]\n",
      "8945 [D loss: 0.694401, acc: 19.14%] [G loss: 0.694401]\n",
      "8950 [D loss: 0.694401, acc: 16.99%] [G loss: 0.694401]\n",
      "Epoch: 8950, auc: 0.53044, ks: 0.046306567668142895\n",
      "8955 [D loss: 0.694401, acc: 15.82%] [G loss: 0.694400]\n",
      "8960 [D loss: 0.694401, acc: 17.38%] [G loss: 0.694401]\n",
      "8965 [D loss: 0.694401, acc: 15.62%] [G loss: 0.694401]\n",
      "8970 [D loss: 0.694401, acc: 16.02%] [G loss: 0.694401]\n",
      "8975 [D loss: 0.694401, acc: 16.60%] [G loss: 0.694401]\n",
      "8980 [D loss: 0.694401, acc: 18.16%] [G loss: 0.694400]\n",
      "8985 [D loss: 0.694401, acc: 16.99%] [G loss: 0.694401]\n",
      "8990 [D loss: 0.694401, acc: 16.80%] [G loss: 0.694401]\n",
      "8995 [D loss: 0.694401, acc: 16.80%] [G loss: 0.694400]\n",
      "9000 [D loss: 0.694400, acc: 16.80%] [G loss: 0.694401]\n",
      "Epoch: 9000, auc: 0.53032, ks: 0.045782101967397515\n",
      "9005 [D loss: 0.694401, acc: 18.16%] [G loss: 0.694400]\n",
      "9010 [D loss: 0.694400, acc: 15.62%] [G loss: 0.694400]\n",
      "9015 [D loss: 0.694401, acc: 16.21%] [G loss: 0.694400]\n",
      "9020 [D loss: 0.694401, acc: 16.60%] [G loss: 0.694400]\n",
      "9025 [D loss: 0.694400, acc: 17.19%] [G loss: 0.694400]\n",
      "9030 [D loss: 0.694400, acc: 15.62%] [G loss: 0.694400]\n",
      "9035 [D loss: 0.694401, acc: 15.23%] [G loss: 0.694400]\n",
      "9040 [D loss: 0.694401, acc: 17.38%] [G loss: 0.694400]\n",
      "9045 [D loss: 0.694401, acc: 15.43%] [G loss: 0.694401]\n",
      "9050 [D loss: 0.694401, acc: 16.02%] [G loss: 0.694400]\n",
      "Epoch: 9050, auc: 0.53054, ks: 0.0460528400901673\n",
      "9055 [D loss: 0.694401, acc: 17.38%] [G loss: 0.694400]\n",
      "9060 [D loss: 0.694401, acc: 15.62%] [G loss: 0.694400]\n",
      "9065 [D loss: 0.694400, acc: 15.43%] [G loss: 0.694400]\n",
      "9070 [D loss: 0.694400, acc: 16.41%] [G loss: 0.694400]\n",
      "9075 [D loss: 0.694401, acc: 17.38%] [G loss: 0.694400]\n",
      "9080 [D loss: 0.694400, acc: 14.26%] [G loss: 0.694400]\n",
      "9085 [D loss: 0.694400, acc: 16.80%] [G loss: 0.694400]\n",
      "9090 [D loss: 0.694401, acc: 16.21%] [G loss: 0.694400]\n",
      "9095 [D loss: 0.694400, acc: 16.41%] [G loss: 0.694400]\n",
      "9100 [D loss: 0.694400, acc: 15.43%] [G loss: 0.694401]\n",
      "Epoch: 9100, auc: 0.53038, ks: 0.04608510318621384\n",
      "9105 [D loss: 0.694400, acc: 12.70%] [G loss: 0.694400]\n",
      "9110 [D loss: 0.694400, acc: 14.26%] [G loss: 0.694400]\n",
      "9115 [D loss: 0.694401, acc: 14.84%] [G loss: 0.694400]\n",
      "9120 [D loss: 0.694401, acc: 16.41%] [G loss: 0.694400]\n",
      "9125 [D loss: 0.694400, acc: 14.65%] [G loss: 0.694400]\n",
      "9130 [D loss: 0.694401, acc: 16.02%] [G loss: 0.694400]\n",
      "9135 [D loss: 0.694400, acc: 16.41%] [G loss: 0.694400]\n",
      "9140 [D loss: 0.694400, acc: 14.84%] [G loss: 0.694400]\n",
      "9145 [D loss: 0.694400, acc: 15.04%] [G loss: 0.694400]\n",
      "9150 [D loss: 0.694400, acc: 15.82%] [G loss: 0.694400]\n",
      "Epoch: 9150, auc: 0.53041, ks: 0.04660523440085884\n",
      "9155 [D loss: 0.694400, acc: 17.38%] [G loss: 0.694400]\n",
      "9160 [D loss: 0.694400, acc: 14.45%] [G loss: 0.694400]\n",
      "9165 [D loss: 0.694400, acc: 14.84%] [G loss: 0.694400]\n",
      "9170 [D loss: 0.694400, acc: 12.89%] [G loss: 0.694400]\n",
      "9175 [D loss: 0.694400, acc: 15.23%] [G loss: 0.694400]\n",
      "9180 [D loss: 0.694400, acc: 15.62%] [G loss: 0.694400]\n",
      "9185 [D loss: 0.694400, acc: 14.45%] [G loss: 0.694400]\n",
      "9190 [D loss: 0.694400, acc: 15.82%] [G loss: 0.694400]\n",
      "9195 [D loss: 0.694400, acc: 13.67%] [G loss: 0.694400]\n",
      "9200 [D loss: 0.694400, acc: 17.38%] [G loss: 0.694400]\n",
      "Epoch: 9200, auc: 0.53074, ks: 0.04638777700542307\n",
      "9205 [D loss: 0.694400, acc: 13.87%] [G loss: 0.694400]\n",
      "9210 [D loss: 0.694400, acc: 15.82%] [G loss: 0.694400]\n",
      "9215 [D loss: 0.694400, acc: 15.43%] [G loss: 0.694400]\n",
      "9220 [D loss: 0.694400, acc: 14.45%] [G loss: 0.694400]\n",
      "9225 [D loss: 0.694400, acc: 13.09%] [G loss: 0.694400]\n",
      "9230 [D loss: 0.694400, acc: 15.82%] [G loss: 0.694400]\n",
      "9235 [D loss: 0.694400, acc: 14.65%] [G loss: 0.694400]\n",
      "9240 [D loss: 0.694400, acc: 14.65%] [G loss: 0.694400]\n",
      "9245 [D loss: 0.694400, acc: 15.62%] [G loss: 0.694400]\n",
      "9250 [D loss: 0.694400, acc: 15.23%] [G loss: 0.694400]\n",
      "Epoch: 9250, auc: 0.53028, ks: 0.04522755108973009\n",
      "9255 [D loss: 0.694400, acc: 12.30%] [G loss: 0.694400]\n",
      "9260 [D loss: 0.694400, acc: 14.65%] [G loss: 0.694400]\n",
      "9265 [D loss: 0.694400, acc: 11.13%] [G loss: 0.694400]\n",
      "9270 [D loss: 0.694400, acc: 11.52%] [G loss: 0.694400]\n",
      "9275 [D loss: 0.694400, acc: 13.48%] [G loss: 0.694400]\n",
      "9280 [D loss: 0.694400, acc: 13.48%] [G loss: 0.694400]\n",
      "9285 [D loss: 0.694400, acc: 13.48%] [G loss: 0.694400]\n",
      "9290 [D loss: 0.694400, acc: 13.48%] [G loss: 0.694400]\n",
      "9295 [D loss: 0.694400, acc: 14.45%] [G loss: 0.694400]\n",
      "9300 [D loss: 0.694400, acc: 12.70%] [G loss: 0.694400]\n",
      "Epoch: 9300, auc: 0.53058, ks: 0.04676034352332581\n",
      "9305 [D loss: 0.694400, acc: 11.52%] [G loss: 0.694400]\n",
      "9310 [D loss: 0.694400, acc: 12.70%] [G loss: 0.694400]\n",
      "9315 [D loss: 0.694400, acc: 12.11%] [G loss: 0.694400]\n",
      "9320 [D loss: 0.694400, acc: 14.45%] [G loss: 0.694400]\n",
      "9325 [D loss: 0.694400, acc: 12.50%] [G loss: 0.694400]\n",
      "9330 [D loss: 0.694400, acc: 13.28%] [G loss: 0.694400]\n",
      "9335 [D loss: 0.694400, acc: 12.70%] [G loss: 0.694400]\n",
      "9340 [D loss: 0.694400, acc: 12.50%] [G loss: 0.694400]\n",
      "9345 [D loss: 0.694400, acc: 11.91%] [G loss: 0.694400]\n",
      "9350 [D loss: 0.694400, acc: 13.48%] [G loss: 0.694400]\n",
      "Epoch: 9350, auc: 0.53049, ks: 0.0456963232703862\n",
      "9355 [D loss: 0.694400, acc: 13.28%] [G loss: 0.694400]\n",
      "9360 [D loss: 0.694400, acc: 10.16%] [G loss: 0.694400]\n",
      "9365 [D loss: 0.694400, acc: 13.09%] [G loss: 0.694400]\n",
      "9370 [D loss: 0.694400, acc: 11.13%] [G loss: 0.694400]\n",
      "9375 [D loss: 0.694400, acc: 10.74%] [G loss: 0.694400]\n",
      "9380 [D loss: 0.694400, acc: 12.70%] [G loss: 0.694400]\n",
      "9385 [D loss: 0.694400, acc: 12.89%] [G loss: 0.694400]\n",
      "9390 [D loss: 0.694400, acc: 10.94%] [G loss: 0.694400]\n",
      "9395 [D loss: 0.694400, acc: 11.33%] [G loss: 0.694400]\n",
      "9400 [D loss: 0.694400, acc: 13.28%] [G loss: 0.694400]\n",
      "Epoch: 9400, auc: 0.53041, ks: 0.045060164482003784\n",
      "9405 [D loss: 0.694400, acc: 13.09%] [G loss: 0.694400]\n",
      "9410 [D loss: 0.694400, acc: 11.72%] [G loss: 0.694400]\n",
      "9415 [D loss: 0.694400, acc: 11.52%] [G loss: 0.694400]\n",
      "9420 [D loss: 0.694400, acc: 11.33%] [G loss: 0.694400]\n",
      "9425 [D loss: 0.694400, acc: 11.91%] [G loss: 0.694400]\n",
      "9430 [D loss: 0.694400, acc: 10.94%] [G loss: 0.694400]\n",
      "9435 [D loss: 0.694400, acc: 16.21%] [G loss: 0.694400]\n",
      "9440 [D loss: 0.694400, acc: 11.33%] [G loss: 0.694400]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9445 [D loss: 0.694400, acc: 11.91%] [G loss: 0.694400]\n",
      "9450 [D loss: 0.694400, acc: 11.33%] [G loss: 0.694400]\n",
      "Epoch: 9450, auc: 0.53035, ks: 0.045773198121566105\n",
      "9455 [D loss: 0.694400, acc: 12.11%] [G loss: 0.694400]\n",
      "9460 [D loss: 0.694400, acc: 10.94%] [G loss: 0.694400]\n",
      "9465 [D loss: 0.694400, acc: 11.72%] [G loss: 0.694400]\n",
      "9470 [D loss: 0.694400, acc: 11.33%] [G loss: 0.694400]\n",
      "9475 [D loss: 0.694400, acc: 12.89%] [G loss: 0.694400]\n",
      "9480 [D loss: 0.694400, acc: 11.13%] [G loss: 0.694400]\n",
      "9485 [D loss: 0.694400, acc: 10.35%] [G loss: 0.694400]\n",
      "9490 [D loss: 0.694400, acc: 10.74%] [G loss: 0.694400]\n",
      "9495 [D loss: 0.694400, acc: 10.94%] [G loss: 0.694400]\n",
      "9500 [D loss: 0.694400, acc: 9.38%] [G loss: 0.694400]\n",
      "Epoch: 9500, auc: 0.53066, ks: 0.04580930460430244\n",
      "9505 [D loss: 0.694400, acc: 11.33%] [G loss: 0.694400]\n",
      "9510 [D loss: 0.694400, acc: 10.74%] [G loss: 0.694400]\n",
      "9515 [D loss: 0.694400, acc: 12.11%] [G loss: 0.694400]\n",
      "9520 [D loss: 0.694400, acc: 10.35%] [G loss: 0.694400]\n",
      "9525 [D loss: 0.694400, acc: 11.91%] [G loss: 0.694400]\n",
      "9530 [D loss: 0.694400, acc: 8.20%] [G loss: 0.694400]\n",
      "9535 [D loss: 0.694400, acc: 11.13%] [G loss: 0.694400]\n",
      "9540 [D loss: 0.694400, acc: 10.35%] [G loss: 0.694400]\n",
      "9545 [D loss: 0.694400, acc: 10.16%] [G loss: 0.694400]\n",
      "9550 [D loss: 0.694400, acc: 11.13%] [G loss: 0.694400]\n",
      "Epoch: 9550, auc: 0.53077, ks: 0.045964413726769404\n",
      "9555 [D loss: 0.694400, acc: 7.81%] [G loss: 0.694400]\n",
      "9560 [D loss: 0.694400, acc: 9.18%] [G loss: 0.694400]\n",
      "9565 [D loss: 0.694400, acc: 9.57%] [G loss: 0.694400]\n",
      "9570 [D loss: 0.694400, acc: 10.55%] [G loss: 0.694400]\n",
      "9575 [D loss: 0.694400, acc: 10.94%] [G loss: 0.694400]\n",
      "9580 [D loss: 0.694400, acc: 11.52%] [G loss: 0.694400]\n",
      "9585 [D loss: 0.694400, acc: 10.35%] [G loss: 0.694400]\n",
      "9590 [D loss: 0.694400, acc: 9.18%] [G loss: 0.694400]\n",
      "9595 [D loss: 0.694400, acc: 10.35%] [G loss: 0.694400]\n",
      "9600 [D loss: 0.694400, acc: 9.18%] [G loss: 0.694400]\n",
      "Epoch: 9600, auc: 0.53054, ks: 0.045421578061123236\n",
      "9605 [D loss: 0.694400, acc: 9.57%] [G loss: 0.694400]\n",
      "9610 [D loss: 0.694400, acc: 9.38%] [G loss: 0.694400]\n",
      "9615 [D loss: 0.694400, acc: 8.79%] [G loss: 0.694400]\n",
      "9620 [D loss: 0.694400, acc: 10.55%] [G loss: 0.694400]\n",
      "9625 [D loss: 0.694400, acc: 10.35%] [G loss: 0.694400]\n",
      "9630 [D loss: 0.694400, acc: 8.98%] [G loss: 0.694400]\n",
      "9635 [D loss: 0.694400, acc: 8.79%] [G loss: 0.694400]\n",
      "9640 [D loss: 0.694400, acc: 10.55%] [G loss: 0.694400]\n",
      "9645 [D loss: 0.694400, acc: 10.55%] [G loss: 0.694400]\n",
      "9650 [D loss: 0.694400, acc: 8.98%] [G loss: 0.694400]\n",
      "Epoch: 9650, auc: 0.53045, ks: 0.04546691578929796\n",
      "9655 [D loss: 0.694400, acc: 8.20%] [G loss: 0.694400]\n",
      "9660 [D loss: 0.694400, acc: 9.96%] [G loss: 0.694400]\n",
      "9665 [D loss: 0.694400, acc: 9.18%] [G loss: 0.694400]\n",
      "9670 [D loss: 0.694400, acc: 7.62%] [G loss: 0.694400]\n",
      "9675 [D loss: 0.694400, acc: 11.13%] [G loss: 0.694400]\n",
      "9680 [D loss: 0.694400, acc: 9.57%] [G loss: 0.694400]\n",
      "9685 [D loss: 0.694400, acc: 9.38%] [G loss: 0.694400]\n",
      "9690 [D loss: 0.694400, acc: 7.23%] [G loss: 0.694400]\n",
      "9695 [D loss: 0.694400, acc: 9.57%] [G loss: 0.694400]\n",
      "9700 [D loss: 0.694400, acc: 9.18%] [G loss: 0.694400]\n",
      "Epoch: 9700, auc: 0.53069, ks: 0.045039545424151006\n",
      "9705 [D loss: 0.694400, acc: 10.35%] [G loss: 0.694400]\n",
      "9710 [D loss: 0.694400, acc: 8.20%] [G loss: 0.694400]\n",
      "9715 [D loss: 0.694400, acc: 8.20%] [G loss: 0.694400]\n",
      "9720 [D loss: 0.694400, acc: 9.38%] [G loss: 0.694400]\n",
      "9725 [D loss: 0.694400, acc: 8.59%] [G loss: 0.694400]\n",
      "9730 [D loss: 0.694400, acc: 7.62%] [G loss: 0.694400]\n",
      "9735 [D loss: 0.694400, acc: 7.03%] [G loss: 0.694400]\n",
      "9740 [D loss: 0.694400, acc: 8.79%] [G loss: 0.694400]\n",
      "9745 [D loss: 0.694400, acc: 10.55%] [G loss: 0.694400]\n",
      "9750 [D loss: 0.694400, acc: 7.81%] [G loss: 0.694400]\n",
      "Epoch: 9750, auc: 0.53061, ks: 0.04549130706001314\n",
      "9755 [D loss: 0.694400, acc: 9.18%] [G loss: 0.694400]\n",
      "9760 [D loss: 0.694400, acc: 7.62%] [G loss: 0.694400]\n",
      "9765 [D loss: 0.694400, acc: 8.01%] [G loss: 0.694400]\n",
      "9770 [D loss: 0.694400, acc: 9.18%] [G loss: 0.694400]\n",
      "9775 [D loss: 0.694400, acc: 10.35%] [G loss: 0.694400]\n",
      "9780 [D loss: 0.694400, acc: 8.40%] [G loss: 0.694400]\n",
      "9785 [D loss: 0.694400, acc: 7.62%] [G loss: 0.694400]\n",
      "9790 [D loss: 0.694400, acc: 8.79%] [G loss: 0.694400]\n",
      "9795 [D loss: 0.694400, acc: 9.77%] [G loss: 0.694400]\n",
      "9800 [D loss: 0.694400, acc: 6.05%] [G loss: 0.694400]\n",
      "Epoch: 9800, auc: 0.53087, ks: 0.04634829688325903\n",
      "9805 [D loss: 0.694400, acc: 8.20%] [G loss: 0.694400]\n",
      "9810 [D loss: 0.694400, acc: 8.20%] [G loss: 0.694400]\n",
      "9815 [D loss: 0.694400, acc: 6.45%] [G loss: 0.694400]\n",
      "9820 [D loss: 0.694400, acc: 9.38%] [G loss: 0.694400]\n",
      "9825 [D loss: 0.694400, acc: 6.64%] [G loss: 0.694400]\n",
      "9830 [D loss: 0.694400, acc: 8.40%] [G loss: 0.694400]\n",
      "9835 [D loss: 0.694400, acc: 7.62%] [G loss: 0.694400]\n",
      "9840 [D loss: 0.694400, acc: 6.25%] [G loss: 0.694400]\n",
      "9845 [D loss: 0.694400, acc: 7.23%] [G loss: 0.694400]\n",
      "9850 [D loss: 0.694400, acc: 6.45%] [G loss: 0.694400]\n",
      "Epoch: 9850, auc: 0.53092, ks: 0.045229636482878366\n",
      "9855 [D loss: 0.694400, acc: 6.45%] [G loss: 0.694400]\n",
      "9860 [D loss: 0.694400, acc: 8.40%] [G loss: 0.694400]\n",
      "9865 [D loss: 0.694400, acc: 8.20%] [G loss: 0.694400]\n",
      "9870 [D loss: 0.694400, acc: 8.20%] [G loss: 0.694400]\n",
      "9875 [D loss: 0.694400, acc: 7.23%] [G loss: 0.694400]\n",
      "9880 [D loss: 0.694400, acc: 7.62%] [G loss: 0.694400]\n",
      "9885 [D loss: 0.694400, acc: 8.79%] [G loss: 0.694400]\n",
      "9890 [D loss: 0.694400, acc: 7.42%] [G loss: 0.694400]\n",
      "9895 [D loss: 0.694400, acc: 8.20%] [G loss: 0.694400]\n",
      "9900 [D loss: 0.694400, acc: 8.79%] [G loss: 0.694400]\n",
      "Epoch: 9900, auc: 0.53064, ks: 0.04534189184375337\n",
      "9905 [D loss: 0.694400, acc: 8.40%] [G loss: 0.694400]\n",
      "9910 [D loss: 0.694400, acc: 8.59%] [G loss: 0.694400]\n",
      "9915 [D loss: 0.694400, acc: 8.40%] [G loss: 0.694400]\n",
      "9920 [D loss: 0.694400, acc: 7.23%] [G loss: 0.694400]\n",
      "9925 [D loss: 0.694400, acc: 6.25%] [G loss: 0.694400]\n",
      "9930 [D loss: 0.694400, acc: 6.84%] [G loss: 0.694400]\n",
      "9935 [D loss: 0.694400, acc: 7.23%] [G loss: 0.694400]\n",
      "9940 [D loss: 0.694400, acc: 7.03%] [G loss: 0.694400]\n",
      "9945 [D loss: 0.694400, acc: 7.42%] [G loss: 0.694400]\n",
      "9950 [D loss: 0.694400, acc: 6.45%] [G loss: 0.694400]\n",
      "Epoch: 9950, auc: 0.53069, ks: 0.045801525304946966\n",
      "9955 [D loss: 0.694400, acc: 6.84%] [G loss: 0.694400]\n",
      "9960 [D loss: 0.694400, acc: 6.64%] [G loss: 0.694400]\n",
      "9965 [D loss: 0.694400, acc: 6.45%] [G loss: 0.694400]\n",
      "9970 [D loss: 0.694400, acc: 6.64%] [G loss: 0.694400]\n",
      "9975 [D loss: 0.694400, acc: 6.45%] [G loss: 0.694400]\n",
      "9980 [D loss: 0.694400, acc: 6.05%] [G loss: 0.694400]\n",
      "9985 [D loss: 0.694400, acc: 6.64%] [G loss: 0.694400]\n",
      "9990 [D loss: 0.694400, acc: 8.01%] [G loss: 0.694400]\n",
      "9995 [D loss: 0.694400, acc: 8.01%] [G loss: 0.694400]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    gan = GAN()\n",
    "    auc_list,ks_list=gan.train(X_train_arr,X_train_f1,X_train_f2,y_train,X_valid_arr,X_valid_f1,X_valid_f2,y_valid,\n",
    "             epochs=10000, \n",
    "             batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXhU5d3/8fc3OwkhQAj7kgARZFHEgKJgBRQBW3FtsbVSS0tta2urrUWfp9b6tLV00daf2mpdqmhVihtWKqi4yxZ2wmYIS0JYAglJCGSd+/fHTGJWmEA2Dp/XdeXK5Mx9Mt85mXzmnvvc5xxzziEiIt4V0toFiIhI81LQi4h4nIJeRMTjFPQiIh6noBcR8TgFvYiIxynoRUQ8TkEvUouZ7TSzy6r9PN3M8szsS2Y208y2mFmhme03s7fMLLY16xU5EQW9yHGY2QzgUeDKwKLfATc652KBs4F5rVWbSLAU9CINMLNZwJ+BK5xznwGjgKXOuTUAzrlc59yzzrnC1qxT5ERMp0AQqcnMdgKrgLHAJOfcusDyccBi4A+B76nOuZLWqlMkWOrRi9TvcmAZsKFygXPuY+BaYCTwFnDIzB40s9DWKVEkOAp6kfrdCpwFPGlmVrnQOfdf59xXgM7ANOBbwHdapUKRICnoRep3AJgIjAMeq32nc87nnHsPWAIMa+HaRBpFQS/SAOdcNjABmGxmD5nZtMBUy07mNxr4Ev4hHpE2K6y1CxBpy5xzmWY2AfgIGAH4gEeASGAv8Efn3AutWKLICWnWjYiIx2noRkTE4xT0IiIep6AXEfE4Bb2IiMe1uVk3Xbp0cYmJia1dhojIaWXVqlUHnXMJ9d3X5oI+MTGR1NTU1i5DROS0Yma7GrpPQzciIh6noBcR8TgFvYiIxynoRUQ8TkEvIuJxCnoREY9T0IuIeJyC/jTj8znmrcwkM/doa5ciIqcJBf1p5q/vfc5dr6znyoc/5p1N+6uWF5dV8MvXN7Is41CddbLyjnLoiK5hLXKmCirozWyymW01s3Qzm13P/ZFm9nLg/uVmlhhYnmhmx8xsbeDr701b/pnl9TV7+Ot7n3Pl8B70i4/hu8+l8ruFm9lxsIjb/rWauct28eA722qsc6CgmCsf/oSJD37I4rR9rVS5VKdrQEhLO+EpEAJXuH8UuBzIAlaa2QLn3KZqzWYCec65gWY2HZgDfC1w33bn3IgmrvuMcaSknM/SD7Jww15eX5tNSr9O/Pmr5wLw27c288RHGTzxUQYA5/aOY+XOXLIPH6Nnx3Y457jntY0Ul1XQP6E9s+au4oFrh3Pj6L6t+ZTOaPNXZfHbtzbxwc/HE9cuvLXLkTNEMOe6GQ2kO+cyAMzsJWAaUD3opwH3BW7PBx4xM2vCOs9IqTtz+dGLa9ibX0xEaAg/mjCQH01IJiLM/0Hs/64extXn9WJ3bhEJ7aPo1akd4//0Af9Zn82sSwawKG0/727ezz1TBzPjokS+N3cV97y2gQ5R4Vx5To9WfnZnHuccT36cQd7RMt7fcoCrz+vV2iXJGSKYoZteQGa1n7MCy+pt45wrB/KB+MB9SWa2xsw+NLNx9T2Amc0ys1QzS83JyWnUE2jLlm4/xF3z11Fa7mvUej6f49H30/naE8uICAth7szRrL9vEndOGlQV8pXO79eJa87rzdjkLiR1ieHc3nG8sTYbn8/x0DvbGJAQw8yx/YkMC+XvN53PiD4d+eUbGyksLmvKp3rae+qTHUx75BMOFBY322Ns2JPPln2FACzepGE0aTnBBH19PfPag4wNtdkL9HXOnQfcAfzLzDrUaejcE865FOdcSkJCvWfZPO0457hvQRrzUrP4+4fbg1pn16EiJv/lI1J++y5/XLSVKcO6858fjWVccgJR4aFB/Y7rzu9NWnYBM55Zwdb9hfx4YjKhIf4/T1R4KPd9ZSi5RaU8+fEODhQUk1dUetLP0Sv+8PYW/u8/m1iXlc+d89bh89UdQ39zXTbvbNqPc47fLdzMvW9srLddpXc27eemJ5dz57x1pB/wh/tLKzOJCg/hqnN78sHWHIrLKprtOYlUF8zQTRbQp9rPvYHsBtpkmVkYEAfkOv9epxIA59wqM9sOnAV49jzEe/OPsT4rn4jQELbuL6RXx3Y8siSdy4d04+wedd7janh+2S7SDxzh+vN7c0H/zlw9oheNHQG76YJ+bN5byIsrdtM/IYYvn9Ozxv3n9unI1OHdeeyDdB5e8jl9O0fz9u2X0C6i5huJc461mYcZ1iuO8NDTb3JWhc/x4ord/O2D7eQdLaVTdAQTz+7K9y8dQI+4dlXt5i7dyWMfbOfG0X05u0cs976RxpxFW7jrisGs2Z1Hx+gIDhQW8+OX1mDAtSN7M39VFgCdoiP46eVn1fvYv3lrEwXHylixM5cKn4/fXTucN9dmM3V4D64e0YsF67L55PODXDakW6Of25MfZ7A28zCPfH3kyW4eOcMEE/QrgWQzSwL2ANOBr9dqswCYASwFrgeWOOecmSXgD/wKM+sPJAMZTVZ9G/Twe+m8uGI3kWEhdO8Qxfzvj+HKhz/hmsc+5crhPVmfdZgrz+nBD8cP5K7569m8t4DkbrH875Vn8+rqPVx2djd+f905J/34ISHG764ZxuDusYzo07GqN1/dLyYPJreolIFd2/P8st385d1t3D317Kr7fT5/r/XJT3Zw+8TkesOsrfvFK+uZvyqLUYmdmDKsO7tzj/LyykwWpe3jmW+NZkjPDry3eT/3vbmJiYO78purhxFisCm7gMc/zODV1XvIKSwhxCA6IoykLjHERIQxf1UWlw5KoEv7SP763ucM7xVXJ6wXpe1j16Gj/O0bI/no8xzeWJvN+YmdKSwpZ/qovozo05G4duH8buFm+sZHc1a32Hqfw7ub9lPuc0we1r1qWV5RKQ+9s42i0gpun1hIn87R7C8opl98TLNuTzm9nTDonXPlZnYbsAgIBZ52zqWZ2f1AqnNuAfAUMNfM0oFc/G8GAJcA95tZOVAB3Oqcy22OJ9JWbD9whK6xkRSXVXDbhIH0iGvHWz8ey30L0nhrQzY9O7bjL+9+zrKMQyzLyGXswC4sStvHyh25HCoq5YaU3qdcg5kx46LEBu/vFx/DS7PGAP7e5z8+ziDvaClfTelDSmJn5izawpOf7CCuXTgvr8zkRxMGEnYa9ep3Hizi1dVZfOuiRH71lSFVn4q27CvglmdWMu3RT7hoQBc++jyHoT078JfpI6reEB+4djjDesXx8spMfjwxmV0Hi1i8aT+P3DiSrh0imbt0F7dcnEi7iFA2ZRdw1yvrmddlDNtzjpAYH0N0RCiPfZBOYnw0k4Z2Jy46nBdXZPLAws307xLDqMROmBlPfPN8fvivNUx75FMeu2kk4wd1BfzTYQtLytl+4Ai3Pr8KgH/eMppLzvIPaT7z6Q6KSisIMXh97R525x7jzXXZJHdtz12TB3P5SXxCEO+ztjanNyUlxZ3OV5hK+c27TBicwJzrzqkz7OKco7TCx1cfX8a6zMPMHJvEL788hDfW7uH2l9aSEBvJ0tkTWjRUC4rLuPf1jSzZcoCSch9/uP4c7pi3jutG9mLC4G7c+vwqnpqRwqikzrSPCCMkxFixI5f+CTF0aR/ZYnU2xj2vbWB+ahaf/GI8XTtE1bjvQGExj72/nZdXZnLF0G48cO05dYatgpV+oJArH/6Eknp2tv/5hnO57vzeVPgcF/3+PfYXlDB7ymBu/dKAL2opKOaWf65k675C/jJ9BBMGd+XyBz9iz+FjgH+6bEm5jz2Hj3HZ2d2o8Dne27yfcckJFJWWszbzMIXF5VwxtBu7Dh1ly75CLju7K+uz8vnelwYwc2wSAJ+lH+Tn89czoGt7br2kPxcN7HJSzzcYRSXlxES2uQvXnRHMbJVzLqXe+xT0TaeguIxz7ltc5x+6tgOFxSxO28/0UX2qQv3fqZl0jolg4tmt0yPLKSxh6sMfk1NYQsfocN6/81LaR4Ux5oElOOfIO1rKXZMHc815vbjo90u4ekSvqvn8J3KkpByfc3SIav554zmFJVw8ZwnXjezNA9cOb7BdeYWvSd5QF6XtY83uw1w6KIHM3KMcLa1gXHIX+ie0r2rzwMLNPPPZTv8bT2zNN57C4jJueWYlG/bkM2lod95cl83tE5MpKC7jh+MHcrSkgl+8sp6sw0cxjH7x0fzftGGs2pXHnf9eR8+4KN6781LM4N43NvLOpv1ER4RRWFzGJ7Mn0CEqnOv/9hkZB4sIDzWcg4/uGt/gzv1FafvYe/gY37o4qdHbYv6qLO55bQMLbruYwd2Pvz9Kmt7xgl5vvU0oI6cIgP5djj9e2jU2ipsu7Fdj2Q0pfRpo3TISYiP569dGcMs/V3L3lMF0iokA4Dvjknjy4x307hTNs5/tpLzCR4XPsThtH8Vlw4KaDXTLMys4VlbBm7eNbfTO5fqs2JHLs5/tZMu+An5+xeAaY9jzUjMpLffxnXHHD6qm+tR0xdDuXDHU//gX9o+vt81PLz+L6aP71gl5gNiocP520/l8+f99zJvrspk2omfNfSLt4cVZF9ZZr0tsJM8t28XtEwdWfSL5w/X+N94NWfl85ZFPePbTnYwZEE/qrjx+fdVQkru15+v/WM5LK3bXG+Tbc47w4xfXUFLuo1NMBGf36MDitH1k5BTx9Qv6kpLYmVW7chnYNbbOwV4VPscjSz6ntNzHb9/azHPfHl31t84pLOHBd7bxnXFJDKj2BgiQlp3PgIT2Qc8qk5OjHn0TenV1FnfMW8e7d3yJgV3bn3iFNqi4rKLef7q3N+7j1udXERkWQlR4KPnHynjim+czaWj3en7LFypDB+CF71zAxY0YNthz+Bhvb9zH2szD5BWV8pVze3BO745Me/RTYiPD6BQTQfqBI4xK7ESX9pHMnjKYr/9jOYldonnhO3XDsS1bszuPh9/7nAeuPYfucXXfEBpr5j9X8tn2Q3SOieBoaTmfzp5Au/BQvvbEMnYeLOLDn4+nXUQoa3bn8e9VWRQWl5N+4AjZh4+R2CWGzdkFlFb4h6RiIkKpcI6LB3ThvS0H6NWxHXdP9e/Q7xHXjjED4vloWw4/eGE145K78PHnB3nmW6MYP9i/3+HuVzfw4orddIwO5/fXnsOlg/zThSuHLBPjo/nyOT1JP3CE6aP7MKh7LL94ZQPfGZtUtW9CTkxDNy3kz4u38tgH29l8/+Q6Bzad7sorfIyd8z77Cop54Nrh/OHtLYxLTuDhG8/jaGk5S7cfYsLgrnV67HfNX8eb6/YSExnK8F5xPHPL6KAeL/9YGePmLKGguJzendoRHhrCjoNFdI6JIDTE+O/t44iNCuPBxdtYs/swm/YWEBZqHD5axmPfGMnU4Wf2kb9ZeUf58+JtbN5bwDcu6Ms3xyQCsHJnLl99fClXnduT/l3a89C722gXHkrH6HD25hfz1+kjGJ3UmZ+8tJYL+8dz85h+OODmp/zHZdxyUSJvp+0jK+9YjceLCA2hV6d2/Pf2cUx9+GMKi8tZcNvFlJT5mPjgh0we2p3N+wrIyCkiKjyEr6b04bXVe+gbH82x0gp2HCoirl04hcXlxMdEcKCwhNFJnZn3vTEtv/FOUxq6aSEZOUX06dTOcyEP/qGOb49N5ImPdnDVuf5pom+szWZ/QTFz3t7Cq6v38NPLzuL2y5Kr1tm4J5831mZz7cje9IiL4sF3trF1XyGDuseSV1RKx+hwzAyfzxFSaxrovJWZFBSX8+9bxzAqsTNlFT7unLeO/6zP5tlvj67aEVw5LXTlzlxuenI5CbGRmnkC9O4UzUNfq3uKqVGJnfnZpEH8cdFWAK4d2Yv7pw2jfWQYR0rKaR/YkfpyrYB95fsXcfBICX06R/Pjy5LZuCefxPgYdh4sInVXHll5R5k2ohdR4f4jsK959FO+8Y/lhIYY4aHGr64aQoeocJZmHOI/6/by/LJdxESE8febzqdnx3YcCxw89oMXVrMh6zDTRvTkjbXZZOYepU/n6OM+1+zDx/jToq3ERoXxiymDiY5QrNWmHn0TmvyXj+jZsR1Pf2tUa5fSLJxzlPsc4aEhpB8o5KpHPqVbhyh2HCyiZ1wU2fnF/PH6c5g6vAc/eXkt72zaT/vIMF7/4UXEx0RyyR/f5/x+nfjWRYl8+58ruXpEL8YP7sr/vLaBn1x2FrdcnMgba7M5q1sss+am0rNjuxo9Oucch4pKG5ztsz7rMM75DwqThlUetR0ZHsrsyYPrvMk2hfe3HuC+BWl0bBfOjIsSuXZkzWnDOw8WUe7zMbBrzWMInHOUlPvILSrl4jlLuH1iMj+5rP7jOJZnHOJfK3azKG0fPgdlFT76d4nhxVkX1rs/pKll5Byh3OcaPA6ipWnopgX4fI4hv3qbmy7ox/9+eUhrl9MiFm7Yyw9eWE1ifDQLfjSWWc+lsiwjl66xkRw8UsKdkwZx0wX9iIv277h78uMMfvPWZqIjQomOCOXgEf/pF2IiQiku9zF+UALvbj5Q9fv/9o2RTDnDh2DOZN94chmrdx0mIiyEQd1juercniTERnJen45szyni5qeX0z4yjMnDuvODSweyO/cotzyzki+f04PZUwfz+IcZfHdc/ybZ57FxTz5/XLSV6aP6MHlYd8yMaY9+ysHCEj6+a3yzvFk2loZuWsDOQ0UUl/lO252wJ2Pq8B48eXMK/RNi6BAVztyZF/Dwe5/z4ordPP7NlDpDKDePSeT5ZbvYX1DCS7MuZMOefLbtP8LMsUlc97fPeHfzAb73pf4A7D50VEMwZ7gfTUjm8Q+30z0uiqXbD/G/r28EICzEiAgLoV98DK98/6KqGUB9Okfz3UuSePT97azYmUtW3jF2HTrKkzP82Vfhc6zNzCMtu4DSch9De8YxZkA8B4+UsCzjEOOSE1i6/SAb9uRz+8SzqoZg84+V8f0XVpGVd4wPt+UwY0w/7pg0iA1Zh/E5WL4jlzED6p9x1ZBVu3J56pMdPPS1EUSGNf+MIwV9E/l0u//KThc0MMXOq6of/h8eGsKdkwZx56RB9baNCAvhhe9eSMGxMgZ2ja3xsf35mRewZV+hwl2qXNg/vmrKqs/nyMo7xqGiEt5ct5e1mXn8dfp5daZ5/uDSgbyyag8Hj5Rw9YievL42m8Vp+5g0tDt3zlvL62u/OE1XiMF/fjSOPyzawgdbczCDygGO0nIfk4d155lPd7J5bwF7Dxcz73tjmLt0F/NSsxjZrxOV57R7dXVWnaAvr/CxJvMww3vF1TuL7ff/3cLKnXlcPSLnhDPXmoKCvol8ln6QXh3bkRh//B1HZ7peHdvRq2O7Osv7dI4+4U43OXOFhBh946PpGx/NeX07NdguJjKMl2ZdSLnPR7/4GDbvLeTuVzewN7+Y19dmM3NsEt8d1x+HY+pfP+a7z6Wy5/Axvn1xEpHhIZzVrT2rduXxj4938NQnO+gUHcGAhPb8eGIyoxI7U17hWLAumz8t3kpkWAhTh/dg4Ya9/HraUMJDQ3j8w+1s3X+EFTsOsb/A/2bzl+nn1ahx1a48Vu7MA2DBumwmDe2Oc45fLUgjuVss36x1jE1TUNA3gQqf47Pth5g0pFuTHBAkIicvsdoBi4/dNJKvPb6UXy1IIzE+mrsmD6oaKrlj0iB++fpG+ifEMHvK4KqhminDepB9uJjucVHcPWUwsdWO6B6d1JmE2Egyc48xdmAXbhzdl9fW7OGFZbuJCAvhT4u30bdzNMN7xXFJcgT/XpXFoO4dSIyP5qKBXYhrF87jH26nY3Q4Ewd3460N2RwpKefF5bt5bukuvn9pw0fUnwoF/Ul6bulOMnKKuO+qoWzKLiD/WFmjDgYSkeY3IKE9z3/nAma/soHZUwbXGA+/cVQfMnOPcuXwHjWmREeFhzY4cy40xJg6rDvPLt3FmAHxjErsxITBXXno3W1EhoUwpn88//ruBZgZFT7HrtyjzHl7CwBDenTgmvN6sXjTfn562VlcPDCeV1ZncdOTy1mXdZgrh/fg5w0Me54qBf1JcM7xxEcZZOUdY8LgrqzPOgzARY3cISMizW9w9w68/sOL6ywPCw3hnmqn5w7WDSl9mL8qi8vO9n+C//VVQ7n8oQ/JO1rGPVPPrvpUHxpiPPft0azZfZgDhcXcOW8dv124mfGDEvjh+AGEmDG4eyz78ou5+cJ+3D317GabvaPplSdhx8Eixv/pA8ygW2wUh4pKOK9vJx3FJ3KGenvjPvblH/9kcG9v3MfbG/fy22uGV53hszJ/m2LIV9Mrm9hH2/zXtb1nytn8duFmRid15h8317t9ReQMUP3EesdrU7tdS+3TU9CfhI+25dAvPprvXtKfUUmdGdw9VmffE5E2y3snZWlmpeU+lmYcYlyyf8friD4dFfIi0qYp6BtpbebhwMUldPpUETk9KOgbadUu/4EOKf0aPmhDRKQtUdA30urdeSR1iSG+jV4vVUSkNgV9IzjnWL0rj5HHOQRbRKStUdA3wu7coxwqKuV8DduIyGlEQd8IlePzCnoROZ0o6INUVuFjyZYDxEaGkXwGnXNeRE5/OmAqCPvyi7nub5+x5/Axbji/d5u4moyISLAU9EF4e+Ne9hw+xt9vGsmkIc1/kQARkaZ0xgT9e5v38/B7n5N7tJRffXlojSsjnciKnbn06tiOycN0/VIROf2cEWP0OYUl/PTlteQfK6OwuJyXUzODXtc5x4odeYxO6tyMFYqINJ8zIuh//WYaxeU+nv7WKK4c3oPP0g9SUl4R1Lo7DhZx8EiJgl5ETltBBb2ZTTazrWaWbmaz67k/0sxeDty/3MwSa93f18yOmNnPmqbs4B0oKOY/6/cya1x/+ie0Z/ygrhSVVpAauGZjQ3w+x6pdeSzLyAVgVKKCXkROTyccozezUOBR4HIgC1hpZgucc5uqNZsJ5DnnBprZdGAO8LVq9z8E/Lfpyg7ertyjAIwK9MgvGhhPRGgIH2w9cNxL/72yOoufz19PWIgRHxPBgISYBtuKiLRlwfToRwPpzrkM51wp8BIwrVabacCzgdvzgYkWOKO+mV0NZABpTVNy42QGgr5Pp3YAREeEMTqpM0u2HOB4V9dauGEvXdpH0rtTO6YM766LfovIaSuYoO8FVN97mRVYVm8b51w5kA/Em1kM8Avg16de6snJyjsGQM+O7aqWXXlOD7bnFLFww7561ykoLuPT9ENcc15PPvj5eH5z9fAWqVVEpDkEE/T1dWVrd4UbavNr4CHn3JHjPoDZLDNLNbPUnJycIEoKXmbuUbp1iKxxcZAbzu/NsF4duO/NNAqKy+qs8/6WA5RW+LhiqObMi8jpL5igzwL6VPu5N5DdUBszCwPigFzgAuAPZrYT+Alwj5ndVvsBnHNPOOdSnHMpCQlNe0GPrLxj9O4UXWNZWGgID1xzDoeOlPDo++l11lmUto+E2EidpVJEPCGYoF8JJJtZkplFANOBBbXaLABmBG5fDyxxfuOcc4nOuUTgL8DvnHOPNFHtQcnMO1o1Pl/d8N5xTBneg38t301RSXnV8tyiUt7dfIApw7rrVAci4gknDPrAmPttwCJgMzDPOZdmZveb2VWBZk/hH5NPB+4A6kzBbA3lFT725hfX6dFXmjk2icLicuavyqpaNi81k9JyHzdd2K+lyhQRaVZBnQLBObcQWFhr2b3VbhcDN5zgd9x3EvWdkr35xVT4HH061+3RA4zs24nz+nbkz4u38sRHGVzQvzMrduRyQVJnzuoW28LViog0D08fGVs546ahHj3AnZcPIqlLDEN7dmDB2myy8o5x85jEFqpQRKT5efakZqXlPjLz/HPoe9czRl9pbHIXxiaPBWBDVj4fbD3AFUODP+GZiEhb58mgP3ikhIt/v4R2EaGEGPSIazjoqxveO47hveOauToRkZblyaDfk3eMknIfFT7HwK7tiQjz9AiViMhxeTLoC4v90yWfnJHCeX00F15Ezmye7OoWBo527RobRVx0eCtXIyLSujwa9P4efWyUJz+wiIg0iieDvvL8NR2i1JsXEfFk0Ff26NurRy8i4t2gj4kIJVTnqhER8WrQl9GhnYZtRETAo0FfUFymHbEiIgGeDPrC4nJitSNWRATwdNCrRy8iAp4N+jL16EVEAjwa9OrRi4hUUtCLiHic54K+uKyC0gqfjooVEQnwXNDrPDciIjV5MOj957lR0IuI+Hkw6P09eg3diIj4eS7oC6p69Ap6ERHwYNBrjF5EpCYPBr3G6EVEqvNg0Ff26DV0IyICHgz6gsqLjkSqRy8iAh4M+sLiMtpHhumiIyIiAR4Mep3+QESkOs8FfVFJuYZtRESq8VzQl1U4wkI997RERE5aUIloZpPNbKuZpZvZ7HrujzSzlwP3LzezxMDy0Wa2NvC1zsyuadry6/I5h3JeROQLJ4xEMwsFHgWmAEOAG81sSK1mM4E859xA4CFgTmD5RiDFOTcCmAw8bmbNOq5S7nOEhijpRUQqBZOIo4F051yGc64UeAmYVqvNNODZwO35wEQzM+fcUedceWB5FOCaoujj8fkcoZpwIyJSJZig7wVkVvs5K7Cs3jaBYM8H4gHM7AIzSwM2ALdWC/5mUeFzmlopIlJNMEFfX2rW7pk32MY5t9w5NxQYBdxtZlF1HsBslpmlmllqTk5OECU1rMI5QkxBLyJSKZigzwL6VPu5N5DdUJvAGHwckFu9gXNuM1AEDKv9AM65J5xzKc65lISEhOCrr0eFzxGmsRsRkSrBBP1KINnMkswsApgOLKjVZgEwI3D7emCJc84F1gkDMLN+wCBgZ5NU3oAKn3r0IiLVnXAGjHOu3MxuAxYBocDTzrk0M7sfSHXOLQCeAuaaWTr+nvz0wOpjgdlmVgb4gB845w42xxOp5J9eqaAXEakU1FRH59xCYGGtZfdWu10M3FDPenOBuadYY6NU+Byh6tGLiFTx3ITzCp8jRD16EZEqngz6MAW9iEgV7wW9U49eRKQ6zwW9T2P0IiI1eC7oKzTrRkSkBu8FfYWCXkSkOu8FvdPQjYhIdd4Leh/aGSsiUo3ngl4XHhERqclzkeifR++5pyUictI8l4g6qZmISE2eDHoN3YiIfMak8jkAAAyrSURBVMFzkagjY0VEavJc0OvIWBGRmjwX9OU6qZmISA2eCnqfz38pWw3diIh8wVNBX+H8Qa+hGxGRL3gr6NWjFxGpw1NB7wv06DVGLyLyBU8FfXmgR6+zV4qIfMFTQV+1M1Zj9CIiVTwV9BXq0YuI1OGtoHfaGSsiUpu3gt6nnbEiIrV5Mug1j15E5AueCnqfz/9dQzciIl/wVNBXHRnrqWclInJqPBWJX8y68dTTEhE5JZ5KRI3Ri4jU5c2g99SzEhE5NUFFoplNNrOtZpZuZrPruT/SzF4O3L/czBIDyy83s1VmtiHwfULTll9T5bludGSsiMgXThj0ZhYKPApMAYYAN5rZkFrNZgJ5zrmBwEPAnMDyg8BXnHPDgRnA3KYqvD46MlZEpK5gevSjgXTnXIZzrhR4CZhWq8004NnA7fnARDMz59wa51x2YHkaEGVmkU1ReH10UjMRkbqCCfpeQGa1n7MCy+pt45wrB/KB+FptrgPWOOdKaj+Amc0ys1QzS83JyQm29jp8TkEvIlJbMEFfX2q6xrQxs6H4h3O+V98DOOeecM6lOOdSEhISgiipfpp1IyJSVzBBnwX0qfZzbyC7oTZmFgbEAbmBn3sDrwE3O+e2n2rBx6NrxoqI1BVM0K8Eks0sycwigOnAglptFuDf2QpwPbDEOefMrCPwFnC3c+7Tpiq6IeU6qZmISB0nDPrAmPttwCJgMzDPOZdmZveb2VWBZk8B8WaWDtwBVE7BvA0YCPzSzNYGvro2+bMI0GmKRUTqCgumkXNuIbCw1rJ7q90uBm6oZ73fAL85xRqD5tMYvYhIHZ46hlTz6EVE6vJU0Gt6pYhIXZ4Keh0wJSJSl6eCvnLoRue6ERH5gqeCXkM3IiJ1eSroKwKXEtSsGxGRL3gs6P1JHxqqoBcRqeSxoPd/V49eROQL3gr6qiNjW7kQEZE2xFORqCNjRUTq8lTQV1Sd1MxTT0tE5JR4KhGr5tF76lmJiJwaT0VihebRi4jU4a2g15GxIiJ1eCrofTrXjYhIHZ4K+nLNuhERqcNTQe9zDjNdYUpEpDpPBX2Fz6k3LyJSi7eC3jn15kVEavFU0Pt8jjAFvYhIDZ4K+nIN3YiI1OGpoPf5NHQjIlKbp4K+wjnNoRcRqcVbQe/TUbEiIrV5LOh92hkrIlKLx4Jepz8QEanNU0Hvc06nKBYRqcVTsagjY0VE6vJW0GvWjYhIHd4K+goFvYhIbUEFvZlNNrOtZpZuZrPruT/SzF4O3L/czBIDy+PN7H0zO2JmjzRt6XVVOKfplSIitZww6M0sFHgUmAIMAW40syG1ms0E8pxzA4GHgDmB5cXAL4GfNVnFx+HzqUcvIlJbMD360UC6cy7DOVcKvARMq9VmGvBs4PZ8YKKZmXOuyDn3Cf7Ab3YVTic1ExGpLZig7wVkVvs5K7Cs3jbOuXIgH4gPtggzm2VmqWaWmpOTE+xqdVToXDciInUEE/T1Jac7iTYNcs494ZxLcc6lJCQkBLtaHZpeKSJSVzBBnwX0qfZzbyC7oTZmFgbEAblNUWBjqEcvIlJXMEG/Ekg2syQziwCmAwtqtVkAzAjcvh5Y4pwLukffVHxOPXoRkdrCTtTAOVduZrcBi4BQ4GnnXJqZ3Q+kOucWAE8Bc80sHX9Pfnrl+ma2E+gARJjZ1cAk59ympn8q/guPRIUr6EVEqjth0AM45xYCC2stu7fa7WLghgbWTTyF+hrF59M8ehGR2rx1ZKxOgSAiUoe3gl4XHhERqcNTQe/z6YApEZHaPBX05T6fhm5ERGrxVND7HJpHLyJSi6eC3n9kbGtXISLStngu6NWjFxGpyXNBr52xIiI1eSvoNY9eRKQOTwW9jowVEanLU0GvHr2ISF3eCnpdSlBEpA7vBb2GbkREavBe0KtHLyJSg6eC3uc0j15EpDZPBb2GbkRE6vJM0Dvn8Dk0dCMiUotngr7C579ErYJeRKQm7wS9U9CLiNTHM0Hv8/m/68hYEZGaPBP0lT16ndRMRKQm7wR9hT/oNb1SRKQm7wR95Ri9cl5EpAbvBL1m3YiI1MszQe+rmnXjmackItIkPJOK5VU9+lYuRESkjfFMLPoCQa/plSIiNXkm6DVGLyJSP+8EvY6MFRGpl2eC3qcevYhIvYIKejObbGZbzSzdzGbXc3+kmb0cuH+5mSVWu+/uwPKtZnZF05VeU9XOWI3Ri4jUcMKgN7NQ4FFgCjAEuNHMhtRqNhPIc84NBB4C5gTWHQJMB4YCk4HHAr+vyVWO0evIWBGRmoLp0Y8G0p1zGc65UuAlYFqtNtOAZwO35wMTzcwCy19yzpU453YA6YHf1+Sq5tGrRy8iUkMwQd8LyKz2c1ZgWb1tnHPlQD4QH+S6mNksM0s1s9ScnJzgq6+mQ1Q4Vw7vQfe4qJNaX0TEq4IJ+vq6yC7INsGsi3PuCedcinMuJSEhIYiS6krsEsOj3xjJsF5xJ7W+iIhXBRP0WUCfaj/3BrIbamNmYUAckBvkuiIi0oyCCfqVQLKZJZlZBP6dqwtqtVkAzAjcvh5Y4pxzgeXTA7NykoBkYEXTlC4iIsEIO1ED51y5md0GLAJCgaedc2lmdj+Q6pxbADwFzDWzdPw9+emBddPMbB6wCSgHfuicq2im5yIiIvUw5+oMmbeqlJQUl5qa2tpliIicVsxslXMupb77PHNkrIiI1E9BLyLicQp6ERGPU9CLiHhcm9sZa2Y5wK5T+BVdgINNVE5TUl2No7oar63Wproa52Tr6uecq/eI0zYX9KfKzFIb2vPcmlRX46iuxmurtamuxmmOujR0IyLicQp6ERGP82LQP9HaBTRAdTWO6mq8tlqb6mqcJq/Lc2P0IiJSkxd79CIiUo2CXkTE4zwT9Ce6gHkL1tHHzN43s81mlmZmtweW32dme8xsbeBraivVt9PMNgRqSA0s62xm75jZ54HvnVq4pkHVtstaMysws5+0xjYzs6fN7ICZbay2rN7tY34PB15z681sZAvX9Ucz2xJ47NfMrGNgeaKZHau23f7eXHUdp7YG/3Zmdndgm201sytauK6Xq9W008zWBpa32DY7TkY03+vMOXfaf+E/ffJ2oD8QAawDhrRSLT2AkYHbscA2/BdVvw/4WRvYVjuBLrWW/QGYHbg9G5jTyn/LfUC/1thmwCXASGDjibYPMBX4L/4rqV0ILG/huiYBYYHbc6rVlVi9XStts3r/doH/hXVAJJAU+L8Nbam6at3/Z+Delt5mx8mIZnudeaVHH8wFzFuEc26vc2514HYhsJl6rpPbxlS/uPuzwNWtWMtEYLtz7lSOjj5pzrmP8F9TobqGts804DnntwzoaGY9Wqou59xi579GM8Ay/Fdwa3ENbLOGTANecs6VOOd2AOn4/39btC4zM+CrwIvN8djHc5yMaLbXmVeCPqiLkLc0M0sEzgOWBxbdFvjo9XRLD49U44DFZrbKzGYFlnVzzu0F/4sQ6NpKtYH/ojXV//nawjZraPu0pdfdt/H3+iolmdkaM/vQzMa1Uk31/e3ayjYbB+x3zn1ebVmLb7NaGdFsrzOvBH1QFyFvSWbWHngF+IlzrgD4GzAAGAHsxf+xsTVc7JwbCUwBfmhml7RSHXWY/1KVVwH/DixqK9usIW3idWdm/4P/Cm4vBBbtBfo6584D7gD+ZWYdWrishv52bWKbATdSs0PR4tusnoxosGk9yxq1zbwS9G3qIuRmFo7/D/iCc+5VAOfcfudchXPOB/yDZvq4eiLOuezA9wPAa4E69ld+FAx8P9AateF/81ntnNsfqLFNbDMa3j6t/rozsxnAl4FvuMCAbmBY5FDg9ir84+BntWRdx/nbtYVtFgZcC7xcuaylt1l9GUEzvs68EvTBXMC8RQTG/p4CNjvnHqy2vPqY2jXAxtrrtkBtMWYWW3kb/868jdS8uPsM4I2Wri2gRi+rLWyzgIa2zwLg5sCsiAuB/MqP3i3BzCYDvwCucs4drbY8wcxCA7f7A8lARkvVFXjchv52C4DpZhZpZkmB2la0ZG3AZcAW51xW5YKW3GYNZQTN+Tprib3MLfGFf8/0NvzvxP/TinWMxf+xaj2wNvA1FZgLbAgsXwD0aIXa+uOf8bAOSKvcTkA88B7weeB751aoLRo4BMRVW9bi2wz/G81eoAx/T2pmQ9sH/0fqRwOvuQ1ASgvXlY5/7Lbydfb3QNvrAn/fdcBq4CutsM0a/NsB/xPYZluBKS1ZV2D5P4Fba7VtsW12nIxotteZToEgIuJxXhm6ERGRBijoRUQ8TkEvIuJxCnoREY9T0IuIeJyCXkTE4xT0IiIe9/8BiYc7gNRONh4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ks_list)\n",
    "plt.title(\"KS\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(auc_list)\n",
    "plt.title(\"AUC\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
